{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load ling530-final.py\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import math\n",
    "import random \n",
    "import shutil\n",
    "import datetime\n",
    "import logging\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torch.nn.utils.rnn import pad_packed_sequence, pack_padded_sequence\n",
    "from allennlp.modules.elmo import Elmo, batch_to_ids\n",
    "\n",
    "# logging configurations\n",
    "LOG_FORMAT = \"%(asctime)s %(message)s\"\n",
    "logging.basicConfig(level=logging.INFO, format=LOG_FORMAT, datefmt=\"%H:%M:%S\")\n",
    "\n",
    "# seeding for reproducibility\n",
    "random.seed(1)\n",
    "np.random.seed(2)\n",
    "torch.manual_seed(3)\n",
    "torch.cuda.manual_seed(4)\n",
    "\n",
    "# define directory structure needed for data processing\n",
    "TMP_DIR = os.path.join(\"..\", \"data\", \"tmp\")\n",
    "DATA_DIR = os.path.join(\"..\", \"data\", \"gigawordunsplit\")\n",
    "TRAIN_DIR = os.path.join(\"..\", \"data\", \"gigaword\",\"train\")\n",
    "DEV_DIR = os.path.join(\"..\", \"data\", \"gigaword\",\"dev\")\n",
    "CHECKPOINT_FNAME = \"gigaword.ckpt\"\n",
    "GOLD_DIR = os.path.join(TMP_DIR, \"gold\")\n",
    "SYSTEM_DIR = os.path.join(TMP_DIR, \"system\")\n",
    "TRUE_HEADLINE_FNAME = 'gold.A.0.txt'\n",
    "PRED_HEADLINE_FNAME = 'system.0.txt'\n",
    "\n",
    "for d in [DATA_DIR, TRAIN_DIR, DEV_DIR, TMP_DIR, GOLD_DIR, SYSTEM_DIR]:\n",
    "    if not os.path.exists(d):\n",
    "        os.makedirs(d)\n",
    "\n",
    "\n",
    "PAD_token = 0\n",
    "SOS_token = 1\n",
    "EOS_token = 2\n",
    "\n",
    "UNKNOWN_TOKEN = 'unk' \n",
    "\n",
    "MIN_LENGTH = 3\n",
    "MAX_LENGTH = 35\n",
    "MAX_HEADLINE_LENGTH = 30\n",
    "MAX_TEXT_LENGTH = 50\n",
    "MIN_TEXT_LENGTH = 5\n",
    "MIN_FREQUENCY   = 4 \n",
    "MIN_KNOWN_COUNT = 3\n",
    "\n",
    "EMBEDDING_DIM = 1024\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "def write_headlines_to_file(fpath, headlines):\n",
    "    \n",
    "    logging.info(\"Writing %d headlines to file\", len(headlines))\n",
    "    with open(fpath, 'w+') as f:\n",
    "        for h in headlines:\n",
    "            f.write(' '.join(h) + '\\n')\n",
    "\n",
    "# # Preprocess\n",
    "\n",
    "# Split data into 80% training and 20% dev.\n",
    "\n",
    "# In[ ]:\n",
    "TMP = \"../data/tmp\"\n",
    "pkl_names = ['train_data', 'dev_data', 'word2index', 'index2word']\n",
    "pickles = []\n",
    "if os.path.exists('../data/tmp/train_data.pkl'):\n",
    "    for i, name in enumerate(pkl_names):\n",
    "        with open(os.path.join(TMP, name+'.pkl'), 'rb') as handle:\n",
    "            pickles.append(pickle.load(handle))\n",
    "    train_data = pickles[0]\n",
    "    dev_data = pickles[1]\n",
    "    WORD_2_INDEX = pickles[2]\n",
    "    INDEX_2_WORD = pickles[3]\n",
    "\n",
    "    \n",
    "else:\n",
    "    logging.info(\"Splitting data into train and dev...\")\n",
    "    fnames = sorted(os.listdir(DATA_DIR))\n",
    "    random.shuffle(fnames)\n",
    "\n",
    "    train_end = int(len(fnames)-1000)\n",
    "\n",
    "    for i, fname in enumerate(fnames):\n",
    "        src = os.path.join(DATA_DIR, fname)\n",
    "        if i < train_end:\n",
    "            dst = os.path.join(TRAIN_DIR, fname)\n",
    "        else:\n",
    "            dst = os.path.join(DEV_DIR, fname)\n",
    "        shutil.copyfile(src, dst)  \n",
    "\n",
    "    # Count the frequency of each word appears in the dataset\n",
    "\n",
    "    # In[ ]:\n",
    "\n",
    "\n",
    "    def update_freq_dict(freq_dict, tokens):\n",
    "        for t in tokens:\n",
    "            if t not in freq_dict:\n",
    "                freq_dict[t] = 0\n",
    "            freq_dict[t] += 1\n",
    "\n",
    "    def build_freq_dict(data_dir):\n",
    "        freq_dict = dict()\n",
    "        for fname in os.listdir(data_dir):\n",
    "            fpath = os.path.join(data_dir, fname)\n",
    "            with open(fpath) as f:\n",
    "                for line in f:\n",
    "                    obj = json.loads(line)\n",
    "                    headline = [t for t in obj['Headline'].split()]\n",
    "                    text = [t for t in obj['Text'].split()]\n",
    "                    update_freq_dict(freq_dict, headline)\n",
    "                    update_freq_dict(freq_dict, text)\n",
    "        return freq_dict\n",
    "\n",
    "    logging.info(\"Building frequency dict on TRAIN data...\")\n",
    "    freq_dict = build_freq_dict(TRAIN_DIR)\n",
    "    logging.info(\"Number of unique tokens: %d\", len(freq_dict))\n",
    "\n",
    "\n",
    "    # Convert words with frequency less than or equal to 2 to unk.  Ignore the article if it's headline has known word less than 3.\n",
    "\n",
    "    # In[ ]:\n",
    "\n",
    "\n",
    "    vocab_freq_dict = {}\n",
    "\n",
    "    WORD_2_INDEX = {\"PAD\": 0, \"SOS\": 1, \"EOS\": 2}#, \"unk\": 3}\n",
    "    INDEX_2_WORD = {0: \"PAD\", 1: \"SOS\", 2: \"EOS\"}#, 3:\"unk\"}\n",
    "\n",
    "    def remove_low_freq_words(freq_dict, tokens):\n",
    "        filtered_tokens = []\n",
    "        known_count = 0\n",
    "        for t in tokens:\n",
    "            if freq_dict[t] > MIN_FREQUENCY:\n",
    "                filtered_tokens.append(t)\n",
    "                known_count += 1\n",
    "            else:\n",
    "                filtered_tokens.append(UNKNOWN_TOKEN)\n",
    "        return filtered_tokens, known_count\n",
    "\n",
    "\n",
    "    def update_word_index(word2index, index2word, tokens):\n",
    "        for t in tokens:\n",
    "            if t not in word2index:\n",
    "                next_index = len(word2index)\n",
    "                word2index[t] = next_index\n",
    "                index2word[next_index] = t\n",
    "\n",
    "\n",
    "    def read_data(data_dir):\n",
    "        ignore_count = [0,0,0]\n",
    "        data = []\n",
    "        unk_count = 0\n",
    "        for fname in os.listdir(data_dir):\n",
    "            fpath = os.path.join(data_dir, fname)\n",
    "            with open(fpath) as f:\n",
    "                for line in f:\n",
    "                    obj = json.loads(line)\n",
    "                    headline = [t for t in obj['Headline'].split()]\n",
    "                    text = [t for t in obj['Text'].split()][:MAX_TEXT_LENGTH]\n",
    "                    if data_dir == TRAIN_DIR:\n",
    "                        if len(headline) > MAX_HEADLINE_LENGTH:\n",
    "                            ignore_count[1] += 1\n",
    "                            continue\n",
    "                        if len(text) < MIN_TEXT_LENGTH:\n",
    "                            ignore_count[2] +=1\n",
    "                            continue\n",
    "                        headline, known_count = remove_low_freq_words(freq_dict, headline)\n",
    "                        if known_count < MIN_KNOWN_COUNT:\n",
    "                            ignore_count[0] += 1\n",
    "                            continue\n",
    "                    \n",
    "                        # TODO: ignore if too short or too long?\n",
    "                        text, _ = remove_low_freq_words(freq_dict, text) \n",
    "                        for token in (headline + text):\n",
    "                            if token == 'unk':\n",
    "                                unk_count += 1\n",
    "                            elif token not in vocab_freq_dict.keys():\n",
    "                                vocab_freq_dict[token] = freq_dict[token]\n",
    "\n",
    "                    data.append((headline, text))\n",
    "\n",
    "        # Now ready to build word indexes\n",
    "        vocab_freq_dict['unk'] = unk_count\n",
    "        sorted_words = sorted(vocab_freq_dict, key=vocab_freq_dict.get, reverse=True)\n",
    "        update_word_index(WORD_2_INDEX, INDEX_2_WORD, sorted_words)\n",
    "\n",
    "        return data, ignore_count\n",
    "        \n",
    "\n",
    "    logging.info(\"Load TRAIN data and remove low frequency tokens...\")\n",
    "    train_data, ignore_count = read_data(TRAIN_DIR)\n",
    "    assert len(WORD_2_INDEX) == len(INDEX_2_WORD)\n",
    "    VOCAB_SIZE = len(WORD_2_INDEX)\n",
    "    logging.info(\"Removed %d articles due to not enough known words in headline\", ignore_count[0])\n",
    "    logging.info(\"Removed %d articles due to headline length greater than MAX_HEADLINE_LENGTH\", ignore_count[1])\n",
    "    logging.info(\"Removed %d articles due to text length less than MIN_TEXT_LENGTH\", ignore_count[2])\n",
    "    logging.info(\"Number of unique tokens after removing low frequency ones: %d\", VOCAB_SIZE)\n",
    "\n",
    "    logging.info(\"Load DEV data and remove low frequency tokens...\")\n",
    "    dev_data, _ = read_data(DEV_DIR)\n",
    "\n",
    "\n",
    "    for i, item in enumerate([train_data, dev_data, WORD_2_INDEX, INDEX_2_WORD]):\n",
    "        with open(os.path.join(TMP, pkl_names[i]+\".pkl\"), 'wb') as handle:\n",
    "            pickle.dump(item, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "dev_text = [text for (_, text) in dev_data]\n",
    "dev_true_headline = [headline for (headline,_) in dev_data]\n",
    "#write_headlines_to_file(os.path.join(GOLD_DIR,TRUE_HEADLINE_FNAME), dev_true_headline)\n",
    "\n",
    "assert len(WORD_2_INDEX) == len(INDEX_2_WORD)\n",
    "VOCAB_SIZE = len(WORD_2_INDEX)\n",
    "\n",
    "\n",
    "'''\n",
    "class GloVe():\n",
    "    def __init__(self, path, dim):\n",
    "        self.dim = dim\n",
    "        self.word_embedding_dict = {}\n",
    "        with open(path) as f:\n",
    "            for line in f:\n",
    "                values = line.split()\n",
    "                embedding = values[-dim:]\n",
    "                word = ''.join(values[:-dim])\n",
    "                self.word_embedding_dict[word] = np.asarray(embedding, dtype=np.float32)\n",
    "    \n",
    "    def get_word_vector(self, word):\n",
    "        if word not in self.word_embedding_dict.keys():\n",
    "            embedding = np.random.uniform(low=-1, high=1, size=self.dim).astype(np.float32)\n",
    "            self.word_embedding_dict[word] = embedding\n",
    "            return embedding\n",
    "        else:\n",
    "            return self.word_embedding_dict[word]\n",
    "glvmodel = GloVe(os.path.join('..', 'models', 'glove', 'glove.6B.300d.txt'), dim=300)\n",
    "'''\n",
    "\n",
    "# ## Gather word embeddings for tokens in the training data\n",
    "# - Since the RNN needs machine-readable inputs (hence numbers instead of strings), we need to convert all labels to indices, and all words to embeddings with mappings to indices.\n",
    "# - For each token, we query the GloVe model for an embedding.\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "pretrained_embeddings = []\n",
    "'''\n",
    "for i in range(VOCAB_SIZE):\n",
    "    pretrained_embeddings.append(glvmodel.get_word_vector(INDEX_2_WORD[i]))\n",
    "\n",
    "'''\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "# Return a list of indexes, one for each word in the sentence, plus EOS\n",
    "def indexes_from_sentence(tokens,isHeadline):\n",
    "    default_idx = WORD_2_INDEX[UNKNOWN_TOKEN]\n",
    "    idxs = [WORD_2_INDEX.get(word, default_idx) for word in tokens]\n",
    "    if isHeadline:\n",
    "        idxs = idxs + [EOS_token]\n",
    "    return idxs\n",
    "\n",
    "# Pad a sentence with the PAD symbol\n",
    "def pad_seq(seq, max_length):\n",
    "    seq += [PAD_token for i in range(max_length - len(seq))]\n",
    "    return seq\n",
    "\n",
    "\n",
    "def sequence_mask(sequence_length, max_len=None):\n",
    "    if max_len is None:\n",
    "        max_len = sequence_length.data.max()\n",
    "    batch_size = sequence_length.size(0)\n",
    "    seq_range = torch.arange(0, max_len).long()\n",
    "    seq_range_expand = seq_range.unsqueeze(0).expand(batch_size, max_len)\n",
    "    seq_range_expand = Variable(seq_range_expand)\n",
    "    seq_range_expand = seq_range_expand.to(device)\n",
    "    seq_length_expand = (sequence_length.unsqueeze(1)\n",
    "                         .expand_as(seq_range_expand))\n",
    "    return seq_range_expand < seq_length_expand\n",
    "\n",
    "\n",
    "def masked_adasoft(logits, target, lengths):\n",
    "    loss = 0\n",
    "    for i in range(logits.size(0)):\n",
    "        mask = (np.array(lengths) > i).astype(int)\n",
    "        logits_i = logits[i] * torch.tensor(mask, dtype=torch.float).unsqueeze(1).to(device)\n",
    "        targets_i = target[i] * torch.tensor(mask, dtype=torch.long).to(device)\n",
    "        asm_output = crit(logits_i, targets_i)\n",
    "        loss += asm_output.loss\n",
    "\n",
    "    loss /= logits.size(0)\n",
    "    return loss\n",
    "\n",
    "\n",
    "def masked_cross_entropy(logits, target, length):\n",
    "    length = Variable(torch.LongTensor(length)).to(device)\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        logits: A Variable containing a FloatTensor of size\n",
    "            (batch, max_len, num_classes) which contains the\n",
    "            unnormalized probability for each class.\n",
    "        target: A Variable containing a LongTensor of size\n",
    "            (batch, max_len) which contains the index of the true\n",
    "            class for each corresponding step.\n",
    "        length: A Variable containing a LongTensor of size (batch,)\n",
    "            which contains the length of each data in a batch.\n",
    "\n",
    "    Returns:\n",
    "        loss: An average loss value masked by the length.\n",
    "    \"\"\"\n",
    "\n",
    "    # logits_flat: (batch * max_len, num_classes)\n",
    "    logits_flat = logits.view(-1, logits.size(-1))\n",
    "    # log_probs_flat: (batch * max_len, num_classes)\n",
    "    log_probs_flat = F.log_softmax(logits_flat, dim=1)\n",
    "    # target_flat: (batch * max_len, 1)\n",
    "    target_flat = target.view(-1, 1)\n",
    "    # losses_flat: (batch * max_len, 1)\n",
    "    losses_flat = -torch.gather(log_probs_flat, dim=1, index=target_flat)\n",
    "    # losses: (batch, max_len)\n",
    "    losses = losses_flat.view(*target.size())\n",
    "    # mask: (batch, max_len)\n",
    "    mask = sequence_mask(sequence_length=length, max_len=target.size(1))\n",
    "    losses = losses * mask.float()\n",
    "    loss = losses.sum() / length.float().sum()\n",
    "    return loss\n",
    "\n",
    "# # copy from model.py\n",
    "\n",
    "# In[ ]:\n",
    "def param_init(params):\n",
    "    for name, param in params:\n",
    "            if 'bias' in name:\n",
    "                 nn.init.constant_(param, 0.0)\n",
    "            elif 'weight' in name:\n",
    "                nn.init.xavier_normal(param)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ## copy from eval.py\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "def save_checkpoint(encoder, decoder, encoder_optimizer, decoder_optimizer,  name=\"gigaword_model.pt\"):\n",
    "    path = \"../models/\" + name\n",
    "    torch.save({\n",
    "                'encoder_model_state_dict': encoder.state_dict(),\n",
    "                'decoder_model_state_dict': decoder.state_dict(),\n",
    "                'encoder_optimizer_state_dict':encoder_optimizer.state_dict(),\n",
    "                'decoder_optimizer_state_dict':decoder_optimizer.state_dict(),\n",
    "                'timestamp': str(datetime.datetime.now()),\n",
    "                }, path)\n",
    "\n",
    "def load_checkpoint(encoder, decoder, encoder_optimizer, decoder_optimizer,  name=\"gigaword_model.pt\"):\n",
    "    path = \"../models/\" + name\n",
    "    if os.path.isfile(path):\n",
    "        logging.info(\"Loading checkpoint\")\n",
    "        checkpoint = torch.load(path)\n",
    "        encoder.load_state_dict(checkpoint['encoder_model_state_dict'])\n",
    "        decoder.load_state_dict(checkpoint['decoder_model_state_dict'])\n",
    "        encoder_optimizer.load_state_dict(checkpoint['encoder_optimizer_state_dict'])\n",
    "        decoder_optimizer.load_state_dict(checkpoint['decoder_optimizer_state_dict'])\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    \n",
    "    \"\"\" \n",
    "    Scalars: \n",
    "    input_size: vocabulary size\n",
    "    hidden_size: the hidden dimension\n",
    "    n_layers: number of hidden layers in GRU\n",
    "    \n",
    "    \"\"\" \n",
    "    def __init__(self, input_size, hidden_size, embed_size,pretrained_embeddings, n_layers=1, dropout=0.1):\n",
    "        \n",
    "        super(EncoderRNN, self).__init__()\n",
    "        \n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.n_layers = n_layers\n",
    "        self.dropout = dropout\n",
    "        self.embed_size = embed_size\n",
    "        \n",
    "        # glove_embeddings = torch.tensor(pretrained_embeddings)\n",
    "        # self.embedding = nn.Embedding(input_size, embed_size).from_pretrained(glove_embeddings, freeze=True)\n",
    "        \n",
    "        self.gru = nn.GRU(embed_size, hidden_size, n_layers, dropout=self.dropout, bidirectional=True)\n",
    "        param_init(self.gru.named_parameters())\n",
    "        \n",
    "    def forward(self, input_seqs, input_lengths, hidden=None):\n",
    "        embedded = input_seqs #self.embedding(input_seqs)\n",
    "\n",
    "        # try:\n",
    "\n",
    "        # except Exception as e:\n",
    "        #     print(e)\n",
    "        #     print(input_seqs)\n",
    "        #     print(input_lengths)\n",
    "        packed = torch.nn.utils.rnn.pack_padded_sequence(embedded, input_lengths)\n",
    "        outputs, hidden = self.gru(packed, hidden)\n",
    "        \n",
    "        # unpack (back to padded)\n",
    "        outputs, output_lengths = torch.nn.utils.rnn.pad_packed_sequence(outputs) \n",
    "        \n",
    "        # Sum bidirectional outputs\n",
    "        #outputs = outputs[:, :, :self.hidden_size] + outputs[:, : ,self.hidden_size:] \n",
    "        \n",
    "        return outputs, hidden\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attn(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super(Attn, self).__init__()\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "\n",
    "    def forward(self, hidden, encoder_outputs):\n",
    "        attn_energies = torch.bmm(hidden.transpose(0,1), encoder_outputs.permute(1,2,0)).squeeze(1)\n",
    "        # Normalize energies to weights in range 0 to 1, resize to 1 x B x S\n",
    "        return F.softmax(attn_energies, dim=1).unsqueeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size, embed_size, pretrained_embeddings, n_layers=1, dropout=0.1):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "\n",
    "        # Keep for reference\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "        self.dropout = dropout\n",
    "        self.embed_size = embed_size\n",
    "\n",
    "        # Define layers\n",
    "\n",
    "        # glove_embeddings = torch.tensor(pretrained_embeddings)\n",
    "        # self.embedding = nn.Embedding(output_size, hidden_size).                from_pretrained(glove_embeddings, freeze=True)\n",
    "\n",
    "        self.embedding_dropout = nn.Dropout(dropout)\n",
    "        self.gru = nn.GRU(embed_size, hidden_size, n_layers, dropout=dropout)\n",
    "        self.concat = nn.Linear(hidden_size * 2, hidden_size)\n",
    "        self.out = nn.Linear(hidden_size, 2048)\n",
    "        \n",
    "        # Choose attention model\n",
    "        self.attn = Attn(hidden_size)\n",
    "        param_init(self.gru.named_parameters())\n",
    "        param_init(self.concat.named_parameters())\n",
    "        param_init(self.out.named_parameters())\n",
    "\n",
    "    def forward(self, input_seq, last_hidden, encoder_outputs):\n",
    "        # Note: we run this one step at a time\n",
    "\n",
    "        # Get the embedding of the current input word (last output word)\n",
    "        batch_size = input_seq.size(0)\n",
    "        embedded = input_seq #self.embedding(input_seq)\n",
    "        embedded = self.embedding_dropout(embedded)\n",
    "        embedded = embedded.view(1, batch_size, self.embed_size) # S=1 x B x N\n",
    "\n",
    "        # Get current hidden state from input word and last hidden state\n",
    "        rnn_output, hidden = self.gru(embedded, last_hidden)\n",
    "\n",
    "        # Calculate attention from current RNN state and all encoder outputs;\n",
    "        # apply to encoder outputs to get weighted average\n",
    "        attn_weights = self.attn(rnn_output, encoder_outputs)\n",
    "        context = attn_weights.bmm(encoder_outputs.transpose(0, 1)) # B x S=1 x N\n",
    "\n",
    "        # Attentional vector using the RNN hidden state and context vector\n",
    "        # concatenated together (Luong eq. 5)\n",
    "        rnn_output = rnn_output.squeeze(0) # S=1 x B x N -> B x N\n",
    "        context = context.squeeze(1)       # B x S=1 x N -> B x N\n",
    "        concat_input = torch.cat((rnn_output, context), 1)\n",
    "        concat_output = torch.tanh(self.concat(concat_input))\n",
    "\n",
    "        # Finally predict next token (Luong eq. 6, without softmax)\n",
    "        output = self.out(concat_output)\n",
    "\n",
    "        # Return final output, hidden state, and attention weights (for visualization)\n",
    "        return output, hidden, attn_weights\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(input_seq, encoder, decoder, max_length=MAX_LENGTH):\n",
    "    with torch.no_grad(): \n",
    "      \n",
    "        char_ids = batch_to_ids([input_seq])\n",
    "        input_embeds = elmo(char_ids)[\"elmo_representations\"][0]\n",
    "    \n",
    "        input_lengths = [len(input_seq)]\n",
    "        print(input_lengths)\n",
    "        input_embeds = Variable(input_embeds).transpose(0, 1).to(device)\n",
    "        \n",
    "        # Set to not-training mode to disable dropout\n",
    "        encoder.train(False)\n",
    "        decoder.train(False)\n",
    "        \n",
    "        # Run through encoder\n",
    "        encoder_outputs, encoder_hidden = encoder(input_embeds, input_lengths, None)\n",
    "\n",
    "        # Create starting vectors for decoder\n",
    "        char_ids = batch_to_ids([['<S>']])\n",
    "        input_embeds = elmo(char_ids)[\"elmo_representations\"][0]\n",
    "      \n",
    "        decoder_input = Variable(input_embeds).transpose(0, 1).to(device) # SOS\n",
    "        decoder_hidden = torch.cat((encoder_hidden[0], encoder_hidden[1]),1)\n",
    "        for i in range(1, encoder.n_layers):\n",
    "            decoder_hidden = torch.stack((decoder_hidden,torch.cat((encoder_hidden[i*2],encoder_hidden[i*2+1]),1)))\n",
    "        decoder_hidden = decoder_hidden.to(device)\n",
    "        print(decoder_hidden.size())\n",
    "        print(decoder_input.size())\n",
    "\n",
    "        # Store output words and attention states\n",
    "        decoded_words = []\n",
    "        decoder_attentions = torch.zeros(max_length + 1, max_length + 1).to(device)\n",
    "        \n",
    "        # Run through decoder\n",
    "        for di in range(max_length):\n",
    "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "                decoder_input, decoder_hidden, encoder_outputs\n",
    "            )\n",
    "            #decoder_attentions[di,:decoder_attention.size(2)] += decoder_attention.squeeze(0).squeeze(0).data\n",
    "            #decoder_attentions[di,:decoder_attention.size(2)] += decoder_attention.squeeze(0).squeeze(0).to(config.device).data\n",
    "            \n",
    "            # Choose top word from output\n",
    "            ni = crit.predict(decoder_output)\n",
    "            # topv, topi = decoder_output.data.topk(1)\n",
    "            # ni = topi[0][0]\n",
    "            if ni == EOS_token:\n",
    "                decoded_words.append('</S>')\n",
    "                break\n",
    "            else:\n",
    "                decoded_words.append(INDEX_2_WORD[int(ni)])\n",
    "                \n",
    "            # Next input is chosen word\n",
    "            \n",
    "            char_ids = batch_to_ids([[INDEX_2_WORD[int(ni)]]])\n",
    "            input_embeds = elmo(char_ids)[\"elmo_representations\"][0]\n",
    "            decoder_input = Variable(input_embeds).transpose(0, 1).to(device)\n",
    "\n",
    "        # Set back to training mode\n",
    "        encoder.train(True)\n",
    "        decoder.train(True)\n",
    "        \n",
    "        return decoded_words#, decoder_attentions[:di+1, :len(encoder_outputs)]\n",
    "\n",
    "def evaluate_randomly(encoder, decoder, pairs):\n",
    "    article = random.choice(pairs)\n",
    "    headline = article[0]\n",
    "    text = article[1]\n",
    "    print('>', ' '.join(text))\n",
    "    if headline is not None:\n",
    "        print('=', ' '.join(headline))\n",
    "\n",
    "    #output_words, attentions = evaluate(headline, encoder, decoder)\n",
    "    output_words = evaluate(text, encoder, decoder)\n",
    "    output_sentence = ' '.join(output_words)\n",
    "    \n",
    "    print('<', output_sentence)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_batch(input_batches, input_lengths, input_embeds, target_batches, target_lengths, target_embeds, batch_size, encoder, decoder, encoder_optimizer, decoder_optimizer, clip):\n",
    "    \n",
    "    # Zero gradients of both optimizers\n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "    loss = 0 # Added onto for each word\n",
    "\n",
    "    input_batches = input_batches.to(device)\n",
    "\n",
    "    # Run words through encoder\n",
    "    encoder_outputs, encoder_hidden = encoder(input_embeds, input_lengths, None)\n",
    "    \n",
    "    # Prepare input and output variables\n",
    "    decoder_input = Variable(SOS_emb.repeat(batch_size,1)).to(device)\n",
    "    decoder_hidden = torch.cat((encoder_hidden[0], encoder_hidden[1]),1)\n",
    "    for i in range(1, encoder.n_layers):\n",
    "        decoder_hidden = torch.stack((decoder_hidden,torch.cat((encoder_hidden[i*2],encoder_hidden[i*2+1]),1)))\n",
    "    decoder_hidden = decoder_hidden.to(device)\n",
    "\n",
    "    max_target_length = max(target_lengths)\n",
    "    all_decoder_outputs = Variable(torch.zeros(max_target_length, batch_size, 2048)).to(device)\n",
    "    print(decoder_hidden.size())\n",
    "\n",
    "    # Run through decoder one time step at a time\n",
    "    for t in range(max_target_length):\n",
    "        decoder_output, decoder_hidden, decoder_attn = decoder(\n",
    "            decoder_input, decoder_hidden, encoder_outputs\n",
    "        )\n",
    "\n",
    "        all_decoder_outputs[t] = decoder_output\n",
    "        decoder_input = target_embeds[t] # Next input is current target\n",
    "\n",
    "    # Loss calculation and backpropagation\n",
    "    loss = masked_adasoft(all_decoder_outputs, target_batches, target_lengths)\n",
    "    # loss = masked_cross_entropy(\n",
    "    #     all_decoder_outputs.transpose(0, 1).contiguous(), # -> batch x seq\n",
    "    #     target_batches.transpose(0, 1).contiguous(), # -> batch x seq\n",
    "    #     target_lengths\n",
    "    # )\n",
    "    loss.backward()\n",
    "    \n",
    "    # Clip gradient norms\n",
    "    ec = torch.nn.utils.clip_grad_norm_(encoder.parameters(), clip)\n",
    "    dc = torch.nn.utils.clip_grad_norm_(decoder.parameters(), clip)\n",
    "\n",
    "    # Update parameters with optimizers\n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "    \n",
    "    #return loss.data[0], ec, dc\n",
    "    return loss.item(), ec, dc\n",
    "\n",
    "\n",
    "def train(pairs, encoder, decoder, encoder_optimizer, decoder_optimizer, n_epochs, batch_size, clip):\n",
    "\n",
    "    logging.info(\"Start training\")\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        logging.info(\"Starting epoch: %d\", epoch)\n",
    "        running_loss = 0\n",
    "        \n",
    "        # Get training data for this cycle\n",
    "        for batch_ind, batch_data in enumerate(random_batch(batch_size, pairs)):\n",
    "            input_seqs, input_lengths, target_seqs, target_lengths, input_embeds, target_embeds = batch_data\n",
    "\n",
    "            # Run the train function\n",
    "            loss, ec, dc = train_batch(\n",
    "                input_seqs, input_lengths, input_embeds, target_seqs, target_lengths, target_embeds, batch_size,\n",
    "                encoder, decoder,\n",
    "                encoder_optimizer, decoder_optimizer, clip\n",
    "            )\n",
    "            # Keep track of loss\n",
    "            running_loss += loss\n",
    "        \n",
    "\n",
    "            if batch_ind % 25 == 0:\n",
    "                avg_running_loss = running_loss / 25\n",
    "                running_loss = 0\n",
    "                logging.info(\"Iteration: %d running loss: %f\", batch_ind, avg_running_loss)\n",
    "            \n",
    "            if batch_ind % 50 == 0:\n",
    "                logging.info(\"Iteration: %d, evaluating\", batch_ind)\n",
    "                evaluate_randomly(encoder, decoder, pairs)\n",
    "\n",
    "            if batch_ind % 1000 == 0:\n",
    "                logging.info(\"Iteration: %d model saved\",batch_ind)\n",
    "                save_checkpoint(encoder, decoder, encoder_optimizer, decoder_optimizer, name=CHECKPOINT_FNAME)\n",
    "            break\n",
    "\n",
    "\n",
    "\n",
    "def random_batch(batch_size, data):\n",
    "    random.shuffle(data)\n",
    "    end_index = len(data) - len(data) % batch_size\n",
    "    input_seqs = []\n",
    "    target_seqs = []\n",
    "    # Choose random pairs\n",
    "    for i in range(0, end_index, batch_size):\n",
    "        pairs = data[i:i+batch_size]\n",
    "        input_seqs = [indexes_from_sentence( pair[1], isHeadline=False) for pair in pairs]\n",
    "\n",
    "        target_seqs = [indexes_from_sentence(pair[0], isHeadline=True) for pair in pairs]\n",
    "\n",
    "        input_sents = [pair[1] for pair in pairs]\n",
    "        print(input_sents)\n",
    "        char_ids = batch_to_ids(input_sents)\n",
    "        input_embeds = elmo(char_ids)[\"elmo_representations\"][0]\n",
    "      \n",
    "\n",
    "        target_sents = [pair[0] +['</S>']for pair in pairs]\n",
    "    \n",
    "        char_ids = batch_to_ids(target_sents)\n",
    "        target_embeds = elmo(char_ids)[\"elmo_representations\"][0]\n",
    "\n",
    "        seq_pairs = sorted(zip(input_seqs, target_seqs, input_embeds, target_embeds), key=lambda p: len(p[0]), reverse=True)\n",
    "        input_seqs, target_seqs, input_embeds, target_embeds = zip(*seq_pairs)\n",
    "\n",
    "        input_lengths = [len(s) for s in input_seqs]\n",
    "       \n",
    "        input_padded = [pad_seq(s, max(input_lengths)) for s in input_seqs]\n",
    "        \n",
    "        target_lengths = [len(s) for s in target_seqs]\n",
    "        target_padded = [pad_seq(s, max(target_lengths)) for s in target_seqs]\n",
    "\n",
    "        input_var = Variable(torch.LongTensor(input_padded)).transpose(0, 1)\n",
    "        target_var = Variable(torch.LongTensor(target_padded)).transpose(0, 1)\n",
    "        \n",
    "        input_var = input_var.to(device)\n",
    "        target_var = target_var.to(device)\n",
    "\n",
    "        input_embeds = torch.stack(list(input_embeds)).squeeze(0).transpose(0,1).to(device)\n",
    "        target_embeds = torch.stack(list(target_embeds)).squeeze(0).transpose(0,1).to(device)\n",
    "        \n",
    "        print(target_embeds.size())\n",
    "        print(target_lengths)\n",
    "        yield input_var, input_lengths, target_var, target_lengths, input_embeds, target_embeds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11:04:25 Initializing ELMo\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:348: UserWarning: nn.init.xavier_normal is now deprecated in favor of nn.init.xavier_normal_.\n",
      "11:04:30 Start training\n",
      "11:04:30 Starting epoch: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['top', 'seed', 'arantxa', 'sanchez', 'vicario', 'won', 'a', 'three-set', 'semifinal', 'to', 'book', 'her', 'berth', 'in', 'the', 'finals', 'of', 'the', 'princess', 'cup', 'tennis', 'tournament', 'here', 'on', 'saturday'], ['lebanon', 'would', 'not', 'attend', 'the', 'proposed', 'multilateral', 'talks', 'on', 'middle', 'east', 'peace', 'process', 'in', 'moscow', 'before', 'it', 'sees', 'progress', 'in', 'its', 'own', 'negotiations', 'with', 'israel', 'a', 'foreign', 'ministry', 'official', 'said', 'on', 'tuesday'], ['the', 'european', 'union', 'eu', 'on', 'friday', 'voiced', 'its', 'full', 'support', 'for', 'u.n', 'secretary', 'general', 'ban', 'ki-moon', 's', 'bid', 'to', 'speed', 'up', 'cyprus', 'reunification', 'process', 'and', 'bring', 'it', 'to', 'a', 'successful', 'end'], ['a', 'majority', 'of', 'new', 'yorkers', 'oppose', 'invading', 'iraq', 'without', 'international', 'support', 'with', 'seven', 'out', 'of', '10', 'fearing', 'a', 'terrorist', 'retaliation', 'if', 'war', 'breaks', 'out'], ['yugoslavia', 'and', 'bulgaria', 'saturday', 'congratulated', 'yevgeny', 'primakov', 'for', 'his', 'appointment', 'as', 'russia', 's', 'new', 'prime', 'minister', 'amid', 'the', 'political', 'and', 'economic', 'crises', 'in', 'russia'], ['athens', 'may', '20', 'xinhua', 'greece', 'was', 'hit', 'thursday', 'by', 'its', 'fourth', 'general', 'strike', 'of', 'the', 'year', 'against', 'harsh', 'austerity', 'measures', 'aimed', 'at', 'easing', 'the', 'cash-strapped', 'company', 's', 'financial', 'problems'], ['ratooning', 'paddy', 'growing', 'methods', 'that', 'were', 'once', 'encouraged', 'by', 'the', 'government', 'are', 'gradually', 'fading', 'away', 'as', 'china', 'manages', 'to', 'produce', 'sufficient', 'food', 'for', 'its', 'more', 'than', '1.2', 'billion', 'people'], ['the', 'african', 'union', 'au', 'on', 'tuesday', 'evening', 'urged', 'the', 'malagasy', 'parties', 'to', 'take', 'all', 'necessary', 'measures', 'to', 'ensure', 'the', 'safety', 'of', 'marc', 'ravalomanana', 'who', 'resigned', 'from', 'presidency', 'under', 'the', 'pressure', 'from', 'the', 'opposition', 'and', 'the', 'armed', 'forces'], ['mauritian', 'president', 'anerood', 'jugnauth', 'expressed', 'his', 'hope', 'here', 'on', 'wednesday', 'that', 'relationship', 'between', 'the', 'indian', 'ocean', 'island', 'country', 'and', 'china', 'be', 'consolidated', 'and', 'expanded', 'further', 'both', 'politically', 'and', 'economically'], ['the', 'nigerian', 'government', 'is', 'strengthening', 'its', 'anti-graft', 'campaign', 'to', 'meet', 'the', 'requirements', 'for', 'achieving', 'debt', 'forgiveness', 'from', 'creditor', 'nations', 'a', 'government', 'official', 'said', 'here', 'wednesday'], ['the', 'best', 'way', 'to', 'cope', 'with', 'africa', 's', 'water', 'pollution', 'problems', 'is', 'through', 'prevention', 'says', 'a', 'senior', 'expert', 'with', 'the', 'united', 'nations', 'environment', 'program'], ['russia', 'on', 'thursday', 'praised', 'israel', 's', 'decision', 'to', 'withdraw', 'from', 'the', 'gaza', 'strip', 'as', 'a', 'constructive', 'step', 'in', 'settling', 'the', 'middle', 'east', 'conflict', 'calling', 'for', 'more', 'efforts', 'in', 'the', 'direction'], ['india', 'thursday', 'told', 'britain', 'there', 'could', 'be', 'no', 'de-escalation', 'of', 'tension', 'on', 'the', 'border', 'if', 'pakistan', 'did', 'not', 'stick', 'to', 'its', 'promise', 'to', 'stop', 'cross', 'border', 'terrorism'], ['afghan', 'planning', 'minister', 'haji', 'mohammad', 'unk', 'on', 'friday', 'here', 'announced', 'his', 'nomination', 'for', 'the', 'coming', 'presidential', 'election', 'scheduled', 'for', 'june'], ['more', 'than', '40', 'fishing', 'boat', 'owners', 'lodged', 'a', 'complaint', 'with', 'the', 'legislative', 'yuan', 'friday', 'over', 'requirements', 'that', 'they', 'enroll', 'foreign', 'workers', 'in', 'labor', 'and', 'health', 'insurance', 'programs'], ['the', 'united', 'states', 'and', 'british', 'warplanes', 'bombed', 'southern', 'iraq', 'sunday', 'and', 'injured', 'seven', 'civilians', 'an', 'iraqi', 'military', 'spokesman', 'said', 'sunday', 'evening'], ['a', 'suicide', 'car', 'bomber', 'on', 'thursday', 'blew', 'up', 'his', 'car', 'in', 'a', 'funeral', 'in', 'kirkuk', 'about', '250', 'km', 'north', 'of', 'baghdad', 'killing', '15', 'people', 'and', 'wounding', 'over', '50', 'others', 'iraq', 's', 'state-run', 'television', 'reported'], ['swiss', 'president', 'pascal', 'couchepin', 'arrived', 'in', 'beijing', 'thursday', 'morning', 'for', 'the', 'olympic', 'games'], ['before', 'they', 'scored', 'a', '7-0', 'win', 'in', 'their', 'last', 'match', 'over', 'mudanjiang', 'this', 'morning', 'in', 'the', 'three-team', 'women', 's', 'ice', 'hockey', 'tournament', 'at', 'the', \"china's\", '8th', 'national', 'winter', 'games', 'to', 'be', 'ended', 'later', 'tuesday', 'harbin', 'had', 'made', 'sure', 'to', 'defend', 'the', 'title'], ['a', 'high-grade', 'asphalt', 'mixing', 'plant', 'imported', 'from', 'britain', 'has', 'been', 'transformed', 'to', 'meet', 'local', 'road-building', 'requirements'], ['mozambican', 'prime', 'minister', 'pascoal', 'manuel', 'mocumbi', 'has', 'met', 'south', 'african', 'education', 'minister', 'sibusiso', 'bengu', 'to', 'discuss', 'educational', 'cooperation', 'mozambican', 'information', 'agency', 'reported', 'today'], ['african', 'ministers', 'responsible', 'for', 'energy', 'from', 'nine', 'countries', 'bordering', 'the', 'nile', 'will', 'meet', 'in', 'tanzania', 'this', 'week', 'to', 'review', 'how', 'to', 'economically', 'benefit', 'from', 'the', 'waters', 'that', 'has', 'been', 'a', 'source', 'of', 'conflict', 'in', 'the', 'region'], ['the', 'u.s', 'defense', 'secretary', 'robert', 'gates', 'who', 'is', 'now', 'on', 'his', 'middle', 'east', 'tour', 'said', 'here', 'tuesday', 'that', 'the', 'resignations', 'of', 'six', 'iraqi', 'cabinet', 'ministers', 'loyal', 'to', 'shiite', 'cleric', 'muqtada', 'al-sadr', 'might', 'have', 'positive', 'impact', 'on', 'the', 'reconciliation', 'process', 'in', 'iraq'], ['the', 'british', 'government', 'intends', 'to', 'increase', 'development', 'assistance', 'to', 'uganda', 'for', 'poverty', 'reduction', 'a', 'visiting', 'british', 'official', 'said', 'here', 'thursday'], ['ugandan', 'rebels', 'of', 'the', 'lord', 's', 'resistance', 'army', 'lra', 'abducted', 'more', 'than', '300', 'people', 'from', 'unk', 'village', 'eight', 'km', 'outside', 'of', 'uganda', 's', 'northern', 'town', 'of', 'gulu', 'over', 'the', 'weekend'], ['an', 'earthquake', 'measuring', '7.6', 'on', 'the', 'richter', 'scale', 'jolted', 'the', 'sea', 'area', 'west', 'to', 'sakhalin', 'russia', 's', 'largest', 'island', 'in', 'the', 'east', 'part', 'at', 'unk', 'beijing', 'time', 'today'], ['the', 'second', 'session', 'of', 'the', '11th', 'national', 'committee', 'of', 'the', 'chinese', \"people's\", 'political', 'consultative', 'conference', 'cppcc', 'concluded', 'here', 'thursday', 'morning', 'pledging', 'efforts', 'to', 'help', 'maintain', 'steady', 'and', 'relatively', 'fast', 'economic', 'development', 'in', '2009'], ['pakistani', 'president', 'general', 'pervez', 'musharraf', 'friday', 'underlined', 'vast', 'opportunities', 'existed', 'for', 'collaboration', 'between', 'pakistan', 'and', 'russia', 'in', 'diverse', 'sectors', 'such', 'as', 'oil', 'and', 'gas', 'railways', 'water', 'and', 'power', 'promotion', 'of', 'trade', 'and', 'defence', 'sales'], ['the', 'u.n', 'security', 'council', 'sanctions', 'committee', 'for', 'afghanistan', 'on', 'wednesday', 'made', 'public', 'its', 'list', 'of', 'terror', 'organizations', 'placed', 'under', 'sanctions'], ['new', 'zealand', 'authorities', 'have', 'seized', 'millions', 'of', 'dollars', 'worth', 'of', 'methamphetamine', 'being', 'smuggled', 'into', 'the', 'country', 'the', 'new', 'zealand', 'herald', 'reported', 'on', 'friday'], ['the', 'us-iraqi', 'massive', 'assault', 'on', 'iraq', 's', 'once', 'rebel-held', 'city', 'of', 'fallujah', 'ended', 'on', 'saturday', 'a', 'spokesman', 'of', 'the', 'interim', 'iraqi', 'government', 'said'], ['the', 'united', 'nations', 'is', 'expected', 'to', 'hold', 'a', 'disarmament', 'conference', 'in', 'akita', 'japan', 'on', 'august', '22-25', 'to', 'discuss', 'how', 'the', 'world', 'body', 'can', 'promote', 'disarmament', 'in', 'the', 'new', 'century', 'u.n', 'officials', 'said', 'here', 'wednesday']]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([15, 32, 1024])\n",
      "[8, 15, 11, 12, 8, 10, 9, 15, 9, 11, 9, 6, 8, 7, 6, 4, 8, 8, 8, 9, 8, 7, 11, 8, 8, 12, 7, 10, 7, 11, 9, 8]\n",
      "torch.Size([2, 32, 400])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11:04:33 Iteration: 0 running loss: 0.343421\n",
      "11:04:33 Iteration: 0, evaluating\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> the us-led nato should investigate the case of the attack on china's embassy in yugoslavia and make public the result of the investigation at an early date so as to meet the solemn demand of the chinese people and government\n",
      "= nato urged to promptly investigate attack on china s embassy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11:04:33 Iteration: 0 model saved\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[40]\n",
      "torch.Size([2, 1, 400])\n",
      "torch.Size([1, 1, 1024])\n",
      "< </S>\n"
     ]
    }
   ],
   "source": [
    "hidden_size = 200\n",
    "n_layers = 2\n",
    "dropout = 0.5\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "# Configure training/optimization\n",
    "clip = 50.0\n",
    "learning_rate = 1e-3\n",
    "decoder_learning_ratio = 5.0\n",
    "n_epochs = 1\n",
    "weight_decay = 1e-4\n",
    "\n",
    "options_file = \"https://s3-us-west-2.amazonaws.com/allennlp/models/elmo/2x4096_512_2048cnn_2xhighway/elmo_2x4096_512_2048cnn_2xhighway_options.json\"\n",
    "weight_file = \"https://s3-us-west-2.amazonaws.com/allennlp/models/elmo/2x4096_512_2048cnn_2xhighway/elmo_2x4096_512_2048cnn_2xhighway_weights.hdf5\"\n",
    "\n",
    "elmo = Elmo(options_file, weight_file, 1, dropout=0)\n",
    "\n",
    "SOS_emb = elmo(batch_to_ids([['<S>']]))[\"elmo_representations\"][0].view(EMBEDDING_DIM)\n",
    "EOS_emb = elmo(batch_to_ids([['</S>']]))[\"elmo_representations\"][0].view(EMBEDDING_DIM)\n",
    "# Initialize models\n",
    "encoder = EncoderRNN(VOCAB_SIZE, hidden_size, EMBEDDING_DIM, pretrained_embeddings, n_layers, dropout=dropout).to(device)\n",
    "decoder = DecoderRNN(2*hidden_size, VOCAB_SIZE, EMBEDDING_DIM, pretrained_embeddings, n_layers, dropout=dropout).to(device)\n",
    "\n",
    "\n",
    "encoder_optimizer = torch.optim.Adam(encoder.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "decoder_optimizer = torch.optim.Adam(decoder.parameters(), lr=learning_rate * decoder_learning_ratio, weight_decay=weight_decay)\n",
    "\n",
    "load_checkpoint(encoder, decoder, encoder_optimizer, decoder_optimizer, CHECKPOINT_FNAME)\n",
    "\n",
    "crit = nn.AdaptiveLogSoftmaxWithLoss(2048, VOCAB_SIZE, [1000, 20000]).to(device)\n",
    "\n",
    "train(train_data, encoder, decoder, encoder_optimizer, decoder_optimizer,  n_epochs, batch_size, clip)\n",
    "\n",
    "#test(dev_text, encoder, decoder)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.3980,  0.3580,  0.2370,  ..., -0.2964,  0.3514, -0.0293],\n",
       "       grad_fn=<SelectBackward>)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_sents = [['the', 'council', 'of', 'agriculture', 'coa', 'has', 'stepped', 'up', 'efforts', 'to', 'keep', 'avian', 'flu', 'at', 'bay', 'including', 'monitoring', 'excrement', 'dropped', 'by', 'migratory', 'birds', 'visiting', 'taiwan', 'coa', 'minister', 'lee', 'chin-lung', 'said', 'wednesday']]\n",
    "char_ids = batch_to_ids(input_sents)\n",
    "input_embeds = elmo(char_ids)[\"elmo_representations\"][0]\n",
    "input_embeds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.1580, -0.0314, -0.1077,  0.0044,  0.0842,  0.1824, -0.0136,\n",
       "          -0.1287],\n",
       "         [ 0.3282,  0.1845, -0.0500, -0.0714, -0.3130, -0.4401,  0.0860,\n",
       "           0.2998],\n",
       "         [-0.0480,  0.2332,  0.1139, -0.1090,  0.2138,  0.4215,  0.2184,\n",
       "          -0.4970]],\n",
       "\n",
       "        [[ 0.0219, -0.1135, -0.0528,  0.0904, -0.0311, -0.0341, -0.0548,\n",
       "          -0.0990],\n",
       "         [-0.1595, -0.0663,  0.2609,  0.1326,  0.4124,  0.1383,  0.1380,\n",
       "           0.1593],\n",
       "         [ 0.0723, -0.1091,  0.0034, -0.1783, -0.0401, -0.0890, -0.0665,\n",
       "          -0.1622]]])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "tmp = torch.cat((a[0],a[1]),1)\n",
    "\n",
    "for i in range(1,2):\n",
    "    tmp = torch.stack((tmp,torch.cat((a[i*2],a[i*2+1]),1)))\n",
    "tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.1580, -0.0314, -0.1077,  0.0044],\n",
       "         [ 0.3282,  0.1845, -0.0500, -0.0714],\n",
       "         [-0.0480,  0.2332,  0.1139, -0.1090]],\n",
       "\n",
       "        [[ 0.0842,  0.1824, -0.0136, -0.1287],\n",
       "         [-0.3130, -0.4401,  0.0860,  0.2998],\n",
       "         [ 0.2138,  0.4215,  0.2184, -0.4970]],\n",
       "\n",
       "        [[ 0.0219, -0.1135, -0.0528,  0.0904],\n",
       "         [-0.1595, -0.0663,  0.2609,  0.1326],\n",
       "         [ 0.0723, -0.1091,  0.0034, -0.1783]],\n",
       "\n",
       "        [[-0.0311, -0.0341, -0.0548, -0.0990],\n",
       "         [ 0.4124,  0.1383,  0.1380,  0.1593],\n",
       "         [-0.0401, -0.0890, -0.0665, -0.1622]]])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
