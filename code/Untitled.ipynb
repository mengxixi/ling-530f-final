{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23:56:35 Splitting data into train and dev...\n",
      "23:56:35 Building frequency dict on TRAIN data...\n",
      "23:56:47 Number of unique tokens: 312609\n",
      "23:56:47 Load TRAIN data and remove low frequency tokens...\n",
      "23:57:13 Removed 415 articles due to not enough known words in headline\n",
      "23:57:13 Removed 34 articles due to headline length greater than MAX_HEADLINE_LENGTH\n",
      "23:57:13 Removed 12198 articles due to text length less than MIN_TEXT_LENGTH\n",
      "23:57:13 Number of unique tokens after removing low frequency ones: 78826\n",
      "23:57:13 Load DEV data and remove low frequency tokens...\n"
     ]
    }
   ],
   "source": [
    "# %load ling530-final.py\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import math\n",
    "import random \n",
    "import shutil\n",
    "import datetime\n",
    "import logging\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torch.nn.utils.rnn import pad_packed_sequence, pack_padded_sequence\n",
    "from allennlp.modules.elmo import Elmo, batch_to_ids\n",
    "\n",
    "# logging configurations\n",
    "LOG_FORMAT = \"%(asctime)s %(message)s\"\n",
    "logging.basicConfig(level=logging.INFO, format=LOG_FORMAT, datefmt=\"%H:%M:%S\")\n",
    "\n",
    "# seeding for reproducibility\n",
    "random.seed(1)\n",
    "np.random.seed(2)\n",
    "torch.manual_seed(3)\n",
    "torch.cuda.manual_seed(4)\n",
    "\n",
    "# define directory structure needed for data processing\n",
    "TMP_DIR = os.path.join(\"..\", \"data\", \"tmp\")\n",
    "DATA_DIR = os.path.join(\"..\", \"data\", \"gigawordunsplit\")\n",
    "TRAIN_DIR = os.path.join(\"..\", \"data\", \"gigaword\",\"train\")\n",
    "DEV_DIR = os.path.join(\"..\", \"data\", \"gigaword\",\"dev\")\n",
    "CHECKPOINT_FNAME = \"gigaword.ckpt\"\n",
    "GOLD_DIR = os.path.join(TMP_DIR, \"gold\")\n",
    "SYSTEM_DIR = os.path.join(TMP_DIR, \"system\")\n",
    "TRUE_HEADLINE_FNAME = 'gold.A.0.txt'\n",
    "PRED_HEADLINE_FNAME = 'system.0.txt'\n",
    "\n",
    "for d in [DATA_DIR, TRAIN_DIR, DEV_DIR, TMP_DIR, GOLD_DIR, SYSTEM_DIR]:\n",
    "    if not os.path.exists(d):\n",
    "        os.makedirs(d)\n",
    "\n",
    "\n",
    "PAD_token = 0\n",
    "SOS_token = 1\n",
    "EOS_token = 2\n",
    "\n",
    "UNKNOWN_TOKEN = 'unk' \n",
    "\n",
    "MIN_LENGTH = 3\n",
    "MAX_LENGTH = 35\n",
    "MAX_HEADLINE_LENGTH = 30\n",
    "MAX_TEXT_LENGTH = 50\n",
    "MIN_TEXT_LENGTH = 5\n",
    "MIN_FREQUENCY   = 4 \n",
    "MIN_KNOWN_COUNT = 3\n",
    "\n",
    "EMBEDDING_DIM = 1024\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "def write_headlines_to_file(fpath, headlines):\n",
    "    \n",
    "    logging.info(\"Writing %d headlines to file\", len(headlines))\n",
    "    with open(fpath, 'w+') as f:\n",
    "        for h in headlines:\n",
    "            f.write(' '.join(h) + '\\n')\n",
    "\n",
    "# # Preprocess\n",
    "\n",
    "# Split data into 80% training and 20% dev.\n",
    "\n",
    "# In[ ]:\n",
    "TMP = \"../data/tmp\"\n",
    "pkl_names = ['train_data', 'dev_data', 'word2index', 'index2word']\n",
    "pickles = []\n",
    "if os.path.exists('../data/tmp/train_data.pkl'):\n",
    "    for i, name in enumerate(pkl_names):\n",
    "        with open(os.path.join(TMP, name+'.pkl'), 'rb') as handle:\n",
    "            pickles.append(pickle.load(handle))\n",
    "    train_data = pickles[0]\n",
    "    dev_data = pickles[1]\n",
    "    WORD_2_INDEX = pickles[2]\n",
    "    INDEX_2_WORD = pickles[3]\n",
    "\n",
    "    \n",
    "else:\n",
    "    logging.info(\"Splitting data into train and dev...\")\n",
    "    fnames = sorted(os.listdir(DATA_DIR))\n",
    "    random.shuffle(fnames)\n",
    "\n",
    "    train_end = int(len(fnames)-1000)\n",
    "\n",
    "    for i, fname in enumerate(fnames):\n",
    "        src = os.path.join(DATA_DIR, fname)\n",
    "        if i < train_end:\n",
    "            dst = os.path.join(TRAIN_DIR, fname)\n",
    "        else:\n",
    "            dst = os.path.join(DEV_DIR, fname)\n",
    "        shutil.copyfile(src, dst)  \n",
    "\n",
    "    # Count the frequency of each word appears in the dataset\n",
    "\n",
    "    # In[ ]:\n",
    "\n",
    "\n",
    "    def update_freq_dict(freq_dict, tokens):\n",
    "        for t in tokens:\n",
    "            if t not in freq_dict:\n",
    "                freq_dict[t] = 0\n",
    "            freq_dict[t] += 1\n",
    "\n",
    "    def build_freq_dict(data_dir):\n",
    "        freq_dict = dict()\n",
    "        for fname in os.listdir(data_dir):\n",
    "            fpath = os.path.join(data_dir, fname)\n",
    "            with open(fpath) as f:\n",
    "                for line in f:\n",
    "                    obj = json.loads(line)\n",
    "                    headline = [t for t in obj['Headline'].split()]\n",
    "                    text = [t for t in obj['Text'].split()]\n",
    "                    update_freq_dict(freq_dict, headline)\n",
    "                    update_freq_dict(freq_dict, text)\n",
    "        return freq_dict\n",
    "\n",
    "    logging.info(\"Building frequency dict on TRAIN data...\")\n",
    "    freq_dict = build_freq_dict(TRAIN_DIR)\n",
    "    logging.info(\"Number of unique tokens: %d\", len(freq_dict))\n",
    "\n",
    "\n",
    "    # Convert words with frequency less than or equal to 2 to unk.  Ignore the article if it's headline has known word less than 3.\n",
    "\n",
    "    # In[ ]:\n",
    "\n",
    "\n",
    "    vocab_freq_dict = {}\n",
    "\n",
    "    WORD_2_INDEX = {\"PAD\": 0, \"SOS\": 1, \"EOS\": 2}#, \"unk\": 3}\n",
    "    INDEX_2_WORD = {0: \"PAD\", 1: \"SOS\", 2: \"EOS\"}#, 3:\"unk\"}\n",
    "\n",
    "    def remove_low_freq_words(freq_dict, tokens):\n",
    "        filtered_tokens = []\n",
    "        known_count = 0\n",
    "        for t in tokens:\n",
    "            if freq_dict[t] > MIN_FREQUENCY:\n",
    "                filtered_tokens.append(t)\n",
    "                known_count += 1\n",
    "            else:\n",
    "                filtered_tokens.append(UNKNOWN_TOKEN)\n",
    "        return filtered_tokens, known_count\n",
    "\n",
    "\n",
    "    def update_word_index(word2index, index2word, tokens):\n",
    "        for t in tokens:\n",
    "            if t not in word2index:\n",
    "                next_index = len(word2index)\n",
    "                word2index[t] = next_index\n",
    "                index2word[next_index] = t\n",
    "\n",
    "\n",
    "    def read_data(data_dir):\n",
    "        ignore_count = [0,0,0]\n",
    "        data = []\n",
    "        unk_count = 0\n",
    "        for fname in os.listdir(data_dir):\n",
    "            fpath = os.path.join(data_dir, fname)\n",
    "            with open(fpath) as f:\n",
    "                for line in f:\n",
    "                    obj = json.loads(line)\n",
    "                    headline = [t for t in obj['Headline'].split()]\n",
    "                    text = [t for t in obj['Text'].split()][:MAX_TEXT_LENGTH]\n",
    "                    if data_dir == TRAIN_DIR:\n",
    "                        if len(headline) > MAX_HEADLINE_LENGTH:\n",
    "                            ignore_count[1] += 1\n",
    "                            continue\n",
    "                        if len(text) < MIN_TEXT_LENGTH:\n",
    "                            ignore_count[2] +=1\n",
    "                            continue\n",
    "                        headline, known_count = remove_low_freq_words(freq_dict, headline)\n",
    "                        if known_count < MIN_KNOWN_COUNT:\n",
    "                            ignore_count[0] += 1\n",
    "                            continue\n",
    "                    \n",
    "                        # TODO: ignore if too short or too long?\n",
    "                        text, _ = remove_low_freq_words(freq_dict, text) \n",
    "                        for token in (headline + text):\n",
    "                            if token == 'unk':\n",
    "                                unk_count += 1\n",
    "                            elif token not in vocab_freq_dict.keys():\n",
    "                                vocab_freq_dict[token] = freq_dict[token]\n",
    "\n",
    "                    data.append((headline, text))\n",
    "\n",
    "        # Now ready to build word indexes\n",
    "        vocab_freq_dict['unk'] = unk_count\n",
    "        sorted_words = sorted(vocab_freq_dict, key=vocab_freq_dict.get, reverse=True)\n",
    "        update_word_index(WORD_2_INDEX, INDEX_2_WORD, sorted_words)\n",
    "\n",
    "        return data, ignore_count\n",
    "        \n",
    "\n",
    "    logging.info(\"Load TRAIN data and remove low frequency tokens...\")\n",
    "    train_data, ignore_count = read_data(TRAIN_DIR)\n",
    "    assert len(WORD_2_INDEX) == len(INDEX_2_WORD)\n",
    "    VOCAB_SIZE = len(WORD_2_INDEX)\n",
    "    logging.info(\"Removed %d articles due to not enough known words in headline\", ignore_count[0])\n",
    "    logging.info(\"Removed %d articles due to headline length greater than MAX_HEADLINE_LENGTH\", ignore_count[1])\n",
    "    logging.info(\"Removed %d articles due to text length less than MIN_TEXT_LENGTH\", ignore_count[2])\n",
    "    logging.info(\"Number of unique tokens after removing low frequency ones: %d\", VOCAB_SIZE)\n",
    "\n",
    "    logging.info(\"Load DEV data and remove low frequency tokens...\")\n",
    "    dev_data, _ = read_data(DEV_DIR)\n",
    "\n",
    "\n",
    "    for i, item in enumerate([train_data, dev_data, WORD_2_INDEX, INDEX_2_WORD]):\n",
    "        with open(os.path.join(TMP, pkl_names[i]+\".pkl\"), 'wb') as handle:\n",
    "            pickle.dump(item, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "dev_text = [text for (_, text) in dev_data]\n",
    "dev_true_headline = [headline for (headline,_) in dev_data]\n",
    "#write_headlines_to_file(os.path.join(GOLD_DIR,TRUE_HEADLINE_FNAME), dev_true_headline)\n",
    "\n",
    "assert len(WORD_2_INDEX) == len(INDEX_2_WORD)\n",
    "VOCAB_SIZE = len(WORD_2_INDEX)\n",
    "\n",
    "\n",
    "'''\n",
    "class GloVe():\n",
    "    def __init__(self, path, dim):\n",
    "        self.dim = dim\n",
    "        self.word_embedding_dict = {}\n",
    "        with open(path) as f:\n",
    "            for line in f:\n",
    "                values = line.split()\n",
    "                embedding = values[-dim:]\n",
    "                word = ''.join(values[:-dim])\n",
    "                self.word_embedding_dict[word] = np.asarray(embedding, dtype=np.float32)\n",
    "    \n",
    "    def get_word_vector(self, word):\n",
    "        if word not in self.word_embedding_dict.keys():\n",
    "            embedding = np.random.uniform(low=-1, high=1, size=self.dim).astype(np.float32)\n",
    "            self.word_embedding_dict[word] = embedding\n",
    "            return embedding\n",
    "        else:\n",
    "            return self.word_embedding_dict[word]\n",
    "glvmodel = GloVe(os.path.join('..', 'models', 'glove', 'glove.6B.300d.txt'), dim=300)\n",
    "'''\n",
    "\n",
    "# ## Gather word embeddings for tokens in the training data\n",
    "# - Since the RNN needs machine-readable inputs (hence numbers instead of strings), we need to convert all labels to indices, and all words to embeddings with mappings to indices.\n",
    "# - For each token, we query the GloVe model for an embedding.\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "pretrained_embeddings = []\n",
    "'''\n",
    "for i in range(VOCAB_SIZE):\n",
    "    pretrained_embeddings.append(glvmodel.get_word_vector(INDEX_2_WORD[i]))\n",
    "\n",
    "'''\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "# Return a list of indexes, one for each word in the sentence, plus EOS\n",
    "def indexes_from_sentence(tokens,isHeadline):\n",
    "    default_idx = WORD_2_INDEX[UNKNOWN_TOKEN]\n",
    "    idxs = [WORD_2_INDEX.get(word, default_idx) for word in tokens]\n",
    "    if isHeadline:\n",
    "        idxs = idxs + [EOS_token]\n",
    "    return idxs\n",
    "\n",
    "# Pad a sentence with the PAD symbol\n",
    "def pad_seq(seq, max_length):\n",
    "    seq += [PAD_token for i in range(max_length - len(seq))]\n",
    "    return seq\n",
    "\n",
    "\n",
    "def sequence_mask(sequence_length, max_len=None):\n",
    "    if max_len is None:\n",
    "        max_len = sequence_length.data.max()\n",
    "    batch_size = sequence_length.size(0)\n",
    "    seq_range = torch.arange(0, max_len).long()\n",
    "    seq_range_expand = seq_range.unsqueeze(0).expand(batch_size, max_len)\n",
    "    seq_range_expand = Variable(seq_range_expand)\n",
    "    seq_range_expand = seq_range_expand.to(device)\n",
    "    seq_length_expand = (sequence_length.unsqueeze(1)\n",
    "                         .expand_as(seq_range_expand))\n",
    "    return seq_range_expand < seq_length_expand\n",
    "\n",
    "\n",
    "def masked_adasoft(logits, target, lengths):\n",
    "    loss = 0\n",
    "    for i in range(logits.size(0)):\n",
    "        mask = (np.array(lengths) > i).astype(int)\n",
    "        logits_i = logits[i] * torch.tensor(mask, dtype=torch.float).unsqueeze(1).to(device)\n",
    "        targets_i = target[i] * torch.tensor(mask, dtype=torch.long).to(device)\n",
    "        asm_output = crit(logits_i, targets_i)\n",
    "        loss += asm_output.loss\n",
    "\n",
    "    loss /= logits.size(0)\n",
    "    return loss\n",
    "\n",
    "\n",
    "def masked_cross_entropy(logits, target, length):\n",
    "    length = Variable(torch.LongTensor(length)).to(device)\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        logits: A Variable containing a FloatTensor of size\n",
    "            (batch, max_len, num_classes) which contains the\n",
    "            unnormalized probability for each class.\n",
    "        target: A Variable containing a LongTensor of size\n",
    "            (batch, max_len) which contains the index of the true\n",
    "            class for each corresponding step.\n",
    "        length: A Variable containing a LongTensor of size (batch,)\n",
    "            which contains the length of each data in a batch.\n",
    "\n",
    "    Returns:\n",
    "        loss: An average loss value masked by the length.\n",
    "    \"\"\"\n",
    "\n",
    "    # logits_flat: (batch * max_len, num_classes)\n",
    "    logits_flat = logits.view(-1, logits.size(-1))\n",
    "    # log_probs_flat: (batch * max_len, num_classes)\n",
    "    log_probs_flat = F.log_softmax(logits_flat, dim=1)\n",
    "    # target_flat: (batch * max_len, 1)\n",
    "    target_flat = target.view(-1, 1)\n",
    "    # losses_flat: (batch * max_len, 1)\n",
    "    losses_flat = -torch.gather(log_probs_flat, dim=1, index=target_flat)\n",
    "    # losses: (batch, max_len)\n",
    "    losses = losses_flat.view(*target.size())\n",
    "    # mask: (batch, max_len)\n",
    "    mask = sequence_mask(sequence_length=length, max_len=target.size(1))\n",
    "    losses = losses * mask.float()\n",
    "    loss = losses.sum() / length.float().sum()\n",
    "    return loss\n",
    "\n",
    "# # copy from model.py\n",
    "\n",
    "# In[ ]:\n",
    "def param_init(params):\n",
    "    for name, param in params:\n",
    "            if 'bias' in name:\n",
    "                 nn.init.constant_(param, 0.0)\n",
    "            elif 'weight' in name:\n",
    "                nn.init.xavier_normal(param)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ## copy from eval.py\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "def save_checkpoint(encoder, decoder, encoder_optimizer, decoder_optimizer,  name=\"gigaword_model.pt\"):\n",
    "    path = \"../models/\" + name\n",
    "    torch.save({\n",
    "                'encoder_model_state_dict': encoder.state_dict(),\n",
    "                'decoder_model_state_dict': decoder.state_dict(),\n",
    "                'encoder_optimizer_state_dict':encoder_optimizer.state_dict(),\n",
    "                'decoder_optimizer_state_dict':decoder_optimizer.state_dict(),\n",
    "                'timestamp': str(datetime.datetime.now()),\n",
    "                }, path)\n",
    "\n",
    "def load_checkpoint(encoder, decoder, encoder_optimizer, decoder_optimizer,  name=\"gigaword_model.pt\"):\n",
    "    path = \"../models/\" + name\n",
    "    if os.path.isfile(path):\n",
    "        logging.info(\"Loading checkpoint\")\n",
    "        checkpoint = torch.load(path)\n",
    "        encoder.load_state_dict(checkpoint['encoder_model_state_dict'])\n",
    "        decoder.load_state_dict(checkpoint['decoder_model_state_dict'])\n",
    "        encoder_optimizer.load_state_dict(checkpoint['encoder_optimizer_state_dict'])\n",
    "        decoder_optimizer.load_state_dict(checkpoint['decoder_optimizer_state_dict'])\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    \n",
    "    \"\"\" \n",
    "    Scalars: \n",
    "    input_size: vocabulary size\n",
    "    hidden_size: the hidden dimension\n",
    "    n_layers: number of hidden layers in GRU\n",
    "    \n",
    "    \"\"\" \n",
    "    def __init__(self, input_size, hidden_size, embed_size,pretrained_embeddings, n_layers=1, dropout=0.1):\n",
    "        \n",
    "        super(EncoderRNN, self).__init__()\n",
    "        \n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.n_layers = n_layers\n",
    "        self.dropout = dropout\n",
    "        self.embed_size = embed_size\n",
    "        \n",
    "        # glove_embeddings = torch.tensor(pretrained_embeddings)\n",
    "        # self.embedding = nn.Embedding(input_size, embed_size).from_pretrained(glove_embeddings, freeze=True)\n",
    "        \n",
    "        self.gru = nn.GRU(embed_size, hidden_size, n_layers, dropout=self.dropout, bidirectional=True)\n",
    "        param_init(self.gru.named_parameters())\n",
    "        \n",
    "    def forward(self, input_seqs, input_lengths, hidden=None):\n",
    "        embedded = input_seqs #self.embedding(input_seqs)\n",
    "\n",
    "        # try:\n",
    "\n",
    "        # except Exception as e:\n",
    "        #     print(e)\n",
    "        #     print(input_seqs)\n",
    "        #     print(input_lengths)\n",
    "        packed = torch.nn.utils.rnn.pack_padded_sequence(embedded, input_lengths)\n",
    "        outputs, hidden = self.gru(packed, hidden)\n",
    "        \n",
    "        # unpack (back to padded)\n",
    "        outputs, output_lengths = torch.nn.utils.rnn.pad_packed_sequence(outputs) \n",
    "        \n",
    "        # Sum bidirectional outputs\n",
    "        #outputs = outputs[:, :, :self.hidden_size] + outputs[:, : ,self.hidden_size:] \n",
    "        \n",
    "        return outputs, hidden\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attn(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super(Attn, self).__init__()\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "\n",
    "    def forward(self, hidden, encoder_outputs):\n",
    "        attn_energies = torch.bmm(hidden.transpose(0,1), encoder_outputs.permute(1,2,0)).squeeze(1)\n",
    "        # Normalize energies to weights in range 0 to 1, resize to 1 x B x S\n",
    "        return F.softmax(attn_energies, dim=1).unsqueeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size, embed_size, pretrained_embeddings, n_layers=1, dropout=0.1):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "\n",
    "        # Keep for reference\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "        self.dropout = dropout\n",
    "        self.embed_size = embed_size\n",
    "\n",
    "        # Define layers\n",
    "\n",
    "        # glove_embeddings = torch.tensor(pretrained_embeddings)\n",
    "        # self.embedding = nn.Embedding(output_size, hidden_size).                from_pretrained(glove_embeddings, freeze=True)\n",
    "\n",
    "        self.embedding_dropout = nn.Dropout(dropout)\n",
    "        self.gru = nn.GRU(embed_size, hidden_size, n_layers, dropout=dropout)\n",
    "        self.concat = nn.Linear(hidden_size * 2, hidden_size)\n",
    "        self.out = nn.Linear(hidden_size, 2048)\n",
    "        \n",
    "        # Choose attention model\n",
    "        self.attn = Attn(hidden_size)\n",
    "        param_init(self.gru.named_parameters())\n",
    "        param_init(self.concat.named_parameters())\n",
    "        param_init(self.out.named_parameters())\n",
    "\n",
    "    def forward(self, input_seq, last_hidden, encoder_outputs):\n",
    "        # Note: we run this one step at a time\n",
    "\n",
    "        # Get the embedding of the current input word (last output word)\n",
    "        batch_size = input_seq.size(0)\n",
    "        embedded = input_seq #self.embedding(input_seq)\n",
    "        embedded = self.embedding_dropout(embedded)\n",
    "        embedded = embedded.view(1, batch_size, self.embed_size) # S=1 x B x N\n",
    "\n",
    "        # Get current hidden state from input word and last hidden state\n",
    "        rnn_output, hidden = self.gru(embedded, last_hidden)\n",
    "\n",
    "        # Calculate attention from current RNN state and all encoder outputs;\n",
    "        # apply to encoder outputs to get weighted average\n",
    "        attn_weights = self.attn(rnn_output, encoder_outputs)\n",
    "        context = attn_weights.bmm(encoder_outputs.transpose(0, 1)) # B x S=1 x N\n",
    "\n",
    "        # Attentional vector using the RNN hidden state and context vector\n",
    "        # concatenated together (Luong eq. 5)\n",
    "        rnn_output = rnn_output.squeeze(0) # S=1 x B x N -> B x N\n",
    "        context = context.squeeze(1)       # B x S=1 x N -> B x N\n",
    "        concat_input = torch.cat((rnn_output, context), 1)\n",
    "        concat_output = torch.tanh(self.concat(concat_input))\n",
    "\n",
    "        # Finally predict next token (Luong eq. 6, without softmax)\n",
    "        output = self.out(concat_output)\n",
    "\n",
    "        # Return final output, hidden state, and attention weights (for visualization)\n",
    "        return output, hidden, attn_weights\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(input_seq, encoder, decoder, max_length=MAX_LENGTH):\n",
    "    with torch.no_grad(): \n",
    "        char_ids = batch_to_ids(input_sents)\n",
    "        input_embeds = elmo(char_ids)[\"elmo_representations\"][0]\n",
    "        input_seqs = [indexes_from_sentence( input_seq, isHeadline = False)]\n",
    "        input_lengths = [len(input_seq) for input_seq in input_seqs]\n",
    "        input_batches = Variable(torch.LongTensor(input_seqs)).transpose(0, 1).to(device)\n",
    "            \n",
    "        # Set to not-training mode to disable dropout\n",
    "        encoder.train(False)\n",
    "        decoder.train(False)\n",
    "        \n",
    "        # Run through encoder\n",
    "        encoder_outputs, encoder_hidden = encoder(input_batches, input_lengths, None)\n",
    "\n",
    "        # Create starting vectors for decoder\n",
    "        decoder_input = Variable(torch.LongTensor([SOS_token])).to(device) # SOS\n",
    "        decoder_hidden = torch.cat((encoder_hidden[0], encoder_hidden[1]),1)\n",
    "        for i in range(1, encoder.n_layers):\n",
    "            decoder_hidden = torch.stack((decoder_hidden,torch.cat((encoder_hidden[i*2],encoder_hidden[i*2+1]),1)))\n",
    "        decoder_hidden = decoder_hidden.to(device)\n",
    "      \n",
    "\n",
    "        # Store output words and attention states\n",
    "        decoded_words = []\n",
    "        decoder_attentions = torch.zeros(max_length + 1, max_length + 1).to(device)\n",
    "        \n",
    "        # Run through decoder\n",
    "        for di in range(max_length):\n",
    "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "                decoder_input, decoder_hidden, encoder_outputs\n",
    "            )\n",
    "            #decoder_attentions[di,:decoder_attention.size(2)] += decoder_attention.squeeze(0).squeeze(0).data\n",
    "            #decoder_attentions[di,:decoder_attention.size(2)] += decoder_attention.squeeze(0).squeeze(0).to(config.device).data\n",
    "            \n",
    "            # Choose top word from output\n",
    "            ni = crit.predict(decoder_output)\n",
    "            # topv, topi = decoder_output.data.topk(1)\n",
    "            # ni = topi[0][0]\n",
    "            if ni == EOS_token:\n",
    "                decoded_words.append('</S>')\n",
    "                break\n",
    "            else:\n",
    "                decoded_words.append(INDEX_2_WORD[int(ni)])\n",
    "                \n",
    "            # Next input is chosen word\n",
    "            decoder_input = Variable(torch.LongTensor([ni]))\n",
    "            decoder_input = decoder_input.to(device)\n",
    "\n",
    "        # Set back to training mode\n",
    "        encoder.train(True)\n",
    "        decoder.train(True)\n",
    "        \n",
    "        return decoded_words#, decoder_attentions[:di+1, :len(encoder_outputs)]\n",
    "\n",
    "def evaluate_randomly(encoder, decoder, pairs):\n",
    "    article = random.choice(pairs)\n",
    "    headline = article[0]\n",
    "    text = article[1]\n",
    "    print('>', ' '.join(text))\n",
    "    if headline is not None:\n",
    "        print('=', ' '.join(headline))\n",
    "\n",
    "    #output_words, attentions = evaluate(headline, encoder, decoder)\n",
    "    output_words = evaluate(text, encoder, decoder)\n",
    "    output_sentence = ' '.join(output_words)\n",
    "    \n",
    "    print('<', output_sentence)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_batch(input_batches, input_lengths, input_embeds, target_batches, target_lengths, target_embeds, batch_size, encoder, decoder, encoder_optimizer, decoder_optimizer, clip):\n",
    "    \n",
    "    # Zero gradients of both optimizers\n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "    loss = 0 # Added onto for each word\n",
    "\n",
    "    input_batches = input_batches.to(device)\n",
    "\n",
    "    # Run words through encoder\n",
    "    encoder_outputs, encoder_hidden = encoder(input_embeds, input_lengths, None)\n",
    "    \n",
    "    # Prepare input and output variables\n",
    "    decoder_input = Variable(SOS_emb.repeat(batch_size,1)).to(device)\n",
    "    decoder_hidden = torch.cat((encoder_hidden[0], encoder_hidden[1]),1)\n",
    "    for i in range(1, encoder.n_layers):\n",
    "        decoder_hidden = torch.stack((decoder_hidden,torch.cat((encoder_hidden[i*2],encoder_hidden[i*2+1]),1)))\n",
    "    decoder_hidden = decoder_hidden.to(device)\n",
    "\n",
    "    max_target_length = max(target_lengths)\n",
    "    all_decoder_outputs = Variable(torch.zeros(max_target_length, batch_size, 2048)).to(device)\n",
    "\n",
    "\n",
    "    # Run through decoder one time step at a time\n",
    "    for t in range(max_target_length):\n",
    "        decoder_output, decoder_hidden, decoder_attn = decoder(\n",
    "            decoder_input, decoder_hidden, encoder_outputs\n",
    "        )\n",
    "\n",
    "        all_decoder_outputs[t] = decoder_output\n",
    "        decoder_input = target_embeds[t] # Next input is current target\n",
    "\n",
    "    # Loss calculation and backpropagation\n",
    "    loss = masked_adasoft(all_decoder_outputs, target_batches, target_lengths)\n",
    "    # loss = masked_cross_entropy(\n",
    "    #     all_decoder_outputs.transpose(0, 1).contiguous(), # -> batch x seq\n",
    "    #     target_batches.transpose(0, 1).contiguous(), # -> batch x seq\n",
    "    #     target_lengths\n",
    "    # )\n",
    "    loss.backward()\n",
    "    \n",
    "    # Clip gradient norms\n",
    "    ec = torch.nn.utils.clip_grad_norm_(encoder.parameters(), clip)\n",
    "    dc = torch.nn.utils.clip_grad_norm_(decoder.parameters(), clip)\n",
    "\n",
    "    # Update parameters with optimizers\n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "    \n",
    "    #return loss.data[0], ec, dc\n",
    "    return loss.item(), ec, dc\n",
    "\n",
    "\n",
    "def train(pairs, encoder, decoder, encoder_optimizer, decoder_optimizer, n_epochs, batch_size, clip):\n",
    "\n",
    "    logging.info(\"Start training\")\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        logging.info(\"Starting epoch: %d\", epoch)\n",
    "        running_loss = 0\n",
    "        \n",
    "        # Get training data for this cycle\n",
    "        for batch_ind, batch_data in enumerate(random_batch(batch_size, pairs)):\n",
    "            input_seqs, input_lengths, target_seqs, target_lengths, input_embeds, target_embeds = batch_data\n",
    "\n",
    "            # Run the train function\n",
    "            loss, ec, dc = train_batch(\n",
    "                input_seqs, input_lengths, input_embeds, target_seqs, target_lengths, target_embeds, batch_size,\n",
    "                encoder, decoder,\n",
    "                encoder_optimizer, decoder_optimizer, clip\n",
    "            )\n",
    "            # Keep track of loss\n",
    "            running_loss += loss\n",
    "        \n",
    "\n",
    "            if batch_ind % 25 == 0:\n",
    "                avg_running_loss = running_loss / 25\n",
    "                running_loss = 0\n",
    "                logging.info(\"Iteration: %d running loss: %f\", batch_ind, avg_running_loss)\n",
    "            \n",
    "            if batch_ind % 50 == 0:\n",
    "                logging.info(\"Iteration: %d, evaluating\", batch_ind)\n",
    "                evaluate_randomly(encoder, decoder, pairs)\n",
    "\n",
    "            if batch_ind % 1000 == 0:\n",
    "                logging.info(\"Iteration: %d model saved\",batch_ind)\n",
    "                save_checkpoint(encoder, decoder, encoder_optimizer, decoder_optimizer, name=CHECKPOINT_FNAME)\n",
    "            break\n",
    "\n",
    "\n",
    "\n",
    "def random_batch(batch_size, data):\n",
    "    random.shuffle(data)\n",
    "    end_index = len(data) - len(data) % batch_size\n",
    "    input_seqs = []\n",
    "    target_seqs = []\n",
    "    # Choose random pairs\n",
    "    for i in range(0, end_index, batch_size):\n",
    "        pairs = data[i:i+batch_size]\n",
    "        input_seqs = [indexes_from_sentence( pair[1], isHeadline=False) for pair in pairs]\n",
    "\n",
    "        target_seqs = [indexes_from_sentence(pair[0], isHeadline=True) for pair in pairs]\n",
    "\n",
    "        input_sents = [pair[1] for pair in pairs]\n",
    "        char_ids = batch_to_ids(input_sents)\n",
    "        input_embeds = elmo(char_ids)[\"elmo_representations\"][0]\n",
    "      \n",
    "\n",
    "        target_sents = [pair[0] +['</S>']for pair in pairs]\n",
    "        \n",
    "        print(target_sents)\n",
    "        char_ids = batch_to_ids(target_sents)\n",
    "        target_embeds = elmo(char_ids)[\"elmo_representations\"][0]\n",
    "\n",
    "        seq_pairs = sorted(zip(input_seqs, target_seqs, input_embeds, target_embeds), key=lambda p: len(p[0]), reverse=True)\n",
    "        input_seqs, target_seqs, input_embeds, target_embeds = zip(*seq_pairs)\n",
    "\n",
    "        input_lengths = [len(s) for s in input_seqs]\n",
    "       \n",
    "        input_padded = [pad_seq(s, max(input_lengths)) for s in input_seqs]\n",
    "        \n",
    "        target_lengths = [len(s) for s in target_seqs]\n",
    "        target_padded = [pad_seq(s, max(target_lengths)) for s in target_seqs]\n",
    "\n",
    "        input_var = Variable(torch.LongTensor(input_padded)).transpose(0, 1)\n",
    "        target_var = Variable(torch.LongTensor(target_padded)).transpose(0, 1)\n",
    "        \n",
    "        input_var = input_var.to(device)\n",
    "        target_var = target_var.to(device)\n",
    "\n",
    "        input_embeds = torch.stack(list(input_embeds)).squeeze(0).transpose(0,1).to(device)\n",
    "        target_embeds = torch.stack(list(target_embeds)).squeeze(0).transpose(0,1).to(device)\n",
    "        \n",
    "        print(target_embeds.size())\n",
    "        print(target_lengths)\n",
    "        yield input_var, input_lengths, target_var, target_lengths, input_embeds, target_embeds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01:09:23 Initializing ELMo\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:348: UserWarning: nn.init.xavier_normal is now deprecated in favor of nn.init.xavier_normal_.\n",
      "01:09:28 Start training\n",
      "01:09:28 Starting epoch: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['chinese', 'state', 'councilor', 'meets', 'japanese', 'parliamentary', 'delegation', '</S>'], ['china', 'ends', 'anti-dumping', 'measures', 'on', 'rok-made', 'fiber', '</S>'], ['sudan', 'rejects', 'un', 'security', 'council', 'resolution', '</S>'], ['one', 'third', 'of', 'china', 's', 'copper', 'consumption', 'comes', 'from', 'recycled', 'waste', 'copper', '</S>']]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01:09:29 Iteration: 0 running loss: 0.360742\n",
      "01:09:29 Iteration: 0, evaluating\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([13, 4, 1024])\n",
      "[8, 7, 13, 8]\n",
      "> clashes broke out at a polling station in sudan on thursday killing at least nine people al jazeera news channel reported more\n",
      "= clashes break out at polling post in sudan 9 killed\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "input must have 2 dimensions, got 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-86-30f604814faf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0mcrit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdaptiveLogSoftmaxWithLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2048\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mVOCAB_SIZE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20000\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_optimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder_optimizer\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mn_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclip\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;31m#test(dev_text, encoder, decoder)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-85-ed7b6fdb5e18>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(pairs, encoder, decoder, encoder_optimizer, decoder_optimizer, n_epochs, batch_size, clip)\u001b[0m\n\u001b[1;32m     81\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbatch_ind\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m50\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m                 \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Iteration: %d, evaluating\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_ind\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m                 \u001b[0mevaluate_randomly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpairs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbatch_ind\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m1000\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-67-5bf084de43e4>\u001b[0m in \u001b[0;36mevaluate_randomly\u001b[0;34m(encoder, decoder, pairs)\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;31m#output_words, attentions = evaluate(headline, encoder, decoder)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m     \u001b[0moutput_words\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m     \u001b[0moutput_sentence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m' '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_words\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-67-5bf084de43e4>\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(input_seq, encoder, decoder, max_length)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0;31m# Run through encoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0mencoder_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_hidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_batches\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_lengths\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0;31m# Create starting vectors for decoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    475\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 477\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    478\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-10-69aafb9aebe3>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_seqs, input_lengths, hidden)\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0;31m#     print(input_lengths)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0mpacked\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpack_padded_sequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membedded\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_lengths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m         \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgru\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpacked\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0;31m# unpack (back to padded)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    475\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 477\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    478\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    176\u001b[0m             \u001b[0mflat_weight\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 178\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_forward_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    179\u001b[0m         func = self._backend.RNN(\n\u001b[1;32m    180\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mcheck_forward_args\u001b[0;34m(self, input, hidden, batch_sizes)\u001b[0m\n\u001b[1;32m    124\u001b[0m             raise RuntimeError(\n\u001b[1;32m    125\u001b[0m                 'input must have {} dimensions, got {}'.format(\n\u001b[0;32m--> 126\u001b[0;31m                     expected_input_dim, input.dim()))\n\u001b[0m\u001b[1;32m    127\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_size\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m             raise RuntimeError(\n",
      "\u001b[0;31mRuntimeError\u001b[0m: input must have 2 dimensions, got 1"
     ]
    }
   ],
   "source": [
    "hidden_size = 200\n",
    "n_layers = 2\n",
    "dropout = 0.5\n",
    "\n",
    "batch_size = 4\n",
    "\n",
    "# Configure training/optimization\n",
    "clip = 50.0\n",
    "learning_rate = 1e-3\n",
    "decoder_learning_ratio = 5.0\n",
    "n_epochs = 1\n",
    "weight_decay = 1e-4\n",
    "\n",
    "options_file = \"https://s3-us-west-2.amazonaws.com/allennlp/models/elmo/2x4096_512_2048cnn_2xhighway/elmo_2x4096_512_2048cnn_2xhighway_options.json\"\n",
    "weight_file = \"https://s3-us-west-2.amazonaws.com/allennlp/models/elmo/2x4096_512_2048cnn_2xhighway/elmo_2x4096_512_2048cnn_2xhighway_weights.hdf5\"\n",
    "\n",
    "elmo = Elmo(options_file, weight_file, 1, dropout=0)\n",
    "\n",
    "SOS_emb = elmo(batch_to_ids([['<S>']]))[\"elmo_representations\"][0].view(EMBEDDING_DIM)\n",
    "EOS_emb = elmo(batch_to_ids([['</S>']]))[\"elmo_representations\"][0].view(EMBEDDING_DIM)\n",
    "# Initialize models\n",
    "encoder = EncoderRNN(VOCAB_SIZE, hidden_size, EMBEDDING_DIM, pretrained_embeddings, n_layers, dropout=dropout).to(device)\n",
    "decoder = DecoderRNN(2*hidden_size, VOCAB_SIZE, EMBEDDING_DIM, pretrained_embeddings, n_layers, dropout=dropout).to(device)\n",
    "\n",
    "\n",
    "encoder_optimizer = torch.optim.Adam(encoder.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "decoder_optimizer = torch.optim.Adam(decoder.parameters(), lr=learning_rate * decoder_learning_ratio, weight_decay=weight_decay)\n",
    "\n",
    "load_checkpoint(encoder, decoder, encoder_optimizer, decoder_optimizer, CHECKPOINT_FNAME)\n",
    "\n",
    "crit = nn.AdaptiveLogSoftmaxWithLoss(2048, VOCAB_SIZE, [1000, 20000]).to(device)\n",
    "\n",
    "train(train_data, encoder, decoder, encoder_optimizer, decoder_optimizer,  n_epochs, batch_size, clip)\n",
    "\n",
    "#test(dev_text, encoder, decoder)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.0659, -0.3492,  0.2159,  ..., -0.3246,  0.3185,  0.6293],\n",
       "         [-0.2560,  0.3798, -0.3824,  ...,  0.1506,  0.7887,  0.0481],\n",
       "         [ 0.0506,  0.4091,  0.3630,  ...,  0.4102,  0.8430,  0.2998],\n",
       "         ...,\n",
       "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
       "\n",
       "        [[ 0.1235, -0.1027,  0.0209,  ...,  0.2093,  0.4526, -0.1729],\n",
       "         [ 0.0688, -0.5175,  0.3082,  ...,  0.5625,  0.0797, -0.3699],\n",
       "         [-0.1887,  0.1522,  0.1980,  ...,  0.6962, -0.0031, -0.5458],\n",
       "         ...,\n",
       "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
       "\n",
       "        [[-0.2156,  0.0518,  0.3470,  ..., -0.4018,  0.5654,  0.0124],\n",
       "         [ 0.0017, -0.0723,  0.6510,  ..., -0.3277,  0.6033, -0.0759],\n",
       "         [ 0.2680,  0.3269,  0.0507,  ..., -0.1349,  1.0713,  0.7081],\n",
       "         ...,\n",
       "         [-0.0866, -0.1267, -0.2314,  ..., -0.2268,  0.4654,  0.0234],\n",
       "         [ 0.0378, -0.3198,  0.4418,  ...,  0.1987,  0.6907, -0.2038],\n",
       "         [-0.2023, -0.3326,  0.2376,  ...,  0.2314,  0.3009, -0.0523]],\n",
       "\n",
       "        [[-0.2154,  0.2549, -0.3450,  ...,  0.0247,  0.5644, -0.1607],\n",
       "         [-0.5683, -0.3057,  0.6456,  ...,  0.3769, -0.2645,  0.2852],\n",
       "         [-0.7700, -0.1558,  0.1660,  ...,  0.1570,  0.5739,  0.1601],\n",
       "         ...,\n",
       "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]]],\n",
       "       grad_fn=<DropoutBackward>)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_sents = [['west', 'nile', 'virus', 'claims', 'first', 'nyc', 'resident'], ['shanghai', 'stock', 'index', 'up'], ['ctot', 'canada', 'has', 'no', 'plan', 'to', 'change', 'visa', 'policy', 'for', 'taiwanese', 'tourists'], ['un', 'chief', 'urges', 'african', 'countries', 'to', 'make', 'bold', 'reforms']]\n",
    "char_ids = batch_to_ids(input_sents)\n",
    "input_embeds = elmo(char_ids)[\"elmo_representations\"][0]\n",
    "input_embeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.1580, -0.0314, -0.1077,  0.0044,  0.0842,  0.1824, -0.0136,\n",
       "          -0.1287],\n",
       "         [ 0.3282,  0.1845, -0.0500, -0.0714, -0.3130, -0.4401,  0.0860,\n",
       "           0.2998],\n",
       "         [-0.0480,  0.2332,  0.1139, -0.1090,  0.2138,  0.4215,  0.2184,\n",
       "          -0.4970]],\n",
       "\n",
       "        [[ 0.0219, -0.1135, -0.0528,  0.0904, -0.0311, -0.0341, -0.0548,\n",
       "          -0.0990],\n",
       "         [-0.1595, -0.0663,  0.2609,  0.1326,  0.4124,  0.1383,  0.1380,\n",
       "           0.1593],\n",
       "         [ 0.0723, -0.1091,  0.0034, -0.1783, -0.0401, -0.0890, -0.0665,\n",
       "          -0.1622]]])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "tmp = torch.cat((a[0],a[1]),1)\n",
    "\n",
    "for i in range(1,2):\n",
    "    tmp = torch.stack((tmp,torch.cat((a[i*2],a[i*2+1]),1)))\n",
    "tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.1580, -0.0314, -0.1077,  0.0044],\n",
       "         [ 0.3282,  0.1845, -0.0500, -0.0714],\n",
       "         [-0.0480,  0.2332,  0.1139, -0.1090]],\n",
       "\n",
       "        [[ 0.0842,  0.1824, -0.0136, -0.1287],\n",
       "         [-0.3130, -0.4401,  0.0860,  0.2998],\n",
       "         [ 0.2138,  0.4215,  0.2184, -0.4970]],\n",
       "\n",
       "        [[ 0.0219, -0.1135, -0.0528,  0.0904],\n",
       "         [-0.1595, -0.0663,  0.2609,  0.1326],\n",
       "         [ 0.0723, -0.1091,  0.0034, -0.1783]],\n",
       "\n",
       "        [[-0.0311, -0.0341, -0.0548, -0.0990],\n",
       "         [ 0.4124,  0.1383,  0.1380,  0.1593],\n",
       "         [-0.0401, -0.0890, -0.0665, -0.1622]]])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
