{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "from shutil import copyfile\n",
    "from torch.autograd import Variable\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "from torch.nn.utils.rnn import pad_packed_sequence, pack_padded_sequence\n",
    "import random \n",
    "import time\n",
    "import math\n",
    "import datetime\n",
    "\n",
    "\n",
    "DATA_DIR = os.path.join(\"..\", \"data\", \"gigawordunsplit\")\n",
    "TRAIN_DIR = os.path.join(\"..\", \"data\", \"gigaword\",\"train\")\n",
    "DEV_DIR = os.path.join(\"..\", \"data\", \"gigaword\",\"dev\")\n",
    "\n",
    "for d in [DATA_DIR, TRAIN_DIR, DEV_DIR]:\n",
    "    if not os.path.exists(d):\n",
    "        os.makedirs(d)\n",
    "\n",
    "UNKNOWN_TOKEN = 'unk' \n",
    "MIN_FREQUENCY = 2\n",
    "MIN_KNOWN_COUNT = 3\n",
    "PAD_token = 0\n",
    "SOS_token = 1\n",
    "EOS_token = 2\n",
    "#-------------------------\n",
    "MIN_LENGTH = 3\n",
    "MAX_LENGTH = 100\n",
    "#-------------------------\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split data into 80% training and 20% dev."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fnames = np.asarray(os.listdir(DATA_DIR))\n",
    "train_end = int(len(fnames)*0.8)\n",
    "np.random.shuffle(fnames)\n",
    "i = 0\n",
    "for fname in fnames:\n",
    "    src = os.path.join(DATA_DIR, fname)\n",
    "    if i < train_end:\n",
    "        dst = os.path.join(TRAIN_DIR, fname)\n",
    "    else:\n",
    "        dst = os.path.join(DEV_DIR, fname)\n",
    "    copyfile(src, dst)\n",
    "    i = i + 1\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Count the frequency of each word appears in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data:\n",
      "Number of unique words before removing low frequency words: 312609\n"
     ]
    }
   ],
   "source": [
    "def update_freq_dict(freq_dict, tokens):\n",
    "    for t in tokens:\n",
    "        if t not in freq_dict:\n",
    "            freq_dict[t] = 0\n",
    "        freq_dict[t] += 1\n",
    "\n",
    "def build_freq_dict(data_dir):\n",
    "    freq_dict = dict()\n",
    "    for fname in os.listdir(data_dir):\n",
    "        fpath = os.path.join(data_dir, fname)\n",
    "        with open(fpath) as f:\n",
    "            for line in f:\n",
    "                obj = json.loads(line)\n",
    "                headline = [t for t in obj['Headline'].split()]\n",
    "                text = [t for t in obj['Text'].split()]\n",
    "                update_freq_dict(freq_dict, headline)\n",
    "                update_freq_dict(freq_dict, text)\n",
    "    return freq_dict\n",
    "\n",
    "            \n",
    "freq_dict = build_freq_dict(TRAIN_DIR)\n",
    "print(\"Train data:\")\n",
    "print(\"Number of unique words before removing low frequency words:\", len(freq_dict))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert words with frequency less than or equal to 2 to unk.  Ignore the article if it's headline has known word less than 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data:\n",
      "Number of articles removed due to too less known words in headline: 334\n",
      "Number of unique words after removing low frequency words: 110454\n"
     ]
    }
   ],
   "source": [
    "word2index = {\"PAD\": 0, \"SOS\": 1, \"EOS\": 2, \"unk\": 3}\n",
    "index2word = {0: \"PAD\", 1: \"SOS\", 2: \"EOS\", 3:\"unk\"}\n",
    "def remove_low_freq_words(freq_dict, tokens):\n",
    "    filtered_tokens = []\n",
    "    known_count = 0\n",
    "    for t in tokens:\n",
    "        if freq_dict[t] > MIN_FREQUENCY:\n",
    "            filtered_tokens.append(t)\n",
    "            known_count = known_count + 1\n",
    "        else:\n",
    "            filtered_tokens.append(UNKNOWN_TOKEN)\n",
    "    return filtered_tokens, known_count\n",
    "def update_word_index(word2index, index2word, tokens):\n",
    "    for t in tokens:\n",
    "        if t not in word2index:\n",
    "            next_index = len(word2index)\n",
    "            word2index[t] = next_index\n",
    "            index2word[next_index] = t\n",
    "def read_data(data_dir):\n",
    "    ignore_count = 0\n",
    "    data = []\n",
    "    for fname in os.listdir(data_dir):\n",
    "        fpath = os.path.join(data_dir, fname)\n",
    "        with open(fpath) as f:\n",
    "            for line in f:\n",
    "                obj = json.loads(line)\n",
    "                headline = [t for t in obj['Headline'].split()]\n",
    "                text = [t for t in obj['Text'].split()]\n",
    "                if data_dir == TRAIN_DIR:\n",
    "                    headline, known_count = remove_low_freq_words(freq_dict, headline)\n",
    "                    if known_count < MIN_KNOWN_COUNT:\n",
    "                        ignore_count = ignore_count + 1\n",
    "                        continue\n",
    "                    #-------------------------------------------\n",
    "                    #ignore if too short or too long?\n",
    "                    text, _ = remove_low_freq_words(freq_dict, text) \n",
    "                    #--------------------------------------------\n",
    "                    update_word_index(word2index, index2word, headline)\n",
    "                    update_word_index(word2index, index2word, text)\n",
    "                data.append({'Headline': headline, 'Text': text})\n",
    "    return data, ignore_count\n",
    "    \n",
    "train_data, ignore_count = read_data(TRAIN_DIR)\n",
    "print(\"Train data:\")\n",
    "print(\"Number of articles removed due to too less known words in headline:\", ignore_count)\n",
    "print(\"Number of unique words after removing low frequency words:\", len(word2index))\n",
    "dev_data, _ = read_data(DEV_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "total= 0\n",
    "L = []\n",
    "indexes = np.random.choice(len(train_data), 50000)\n",
    "for i in indexes:\n",
    "    if(len(train_data[i]['Text']) < 80):\n",
    "        L.append(len(train_data[i]['Text']))\n",
    "import matplotlib.pyplot as plt\n",
    "bw = 1\n",
    "plt.hist(L, bins=range(min(L), max(L) +bw, bw))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## GloVe word embeddings\n",
    "- We wrap this into a separate class for reusablility. Upon initialization, we will load the corresponding file containing all the pre-trained word embeddings (of a certain dimensionality), and we store them in a dictionary where keys are the words.\n",
    "- The get_word_vector function takes in a word and try to look for an existing embedding in the GloVe model. If it fails to find the word, it will initialize a random vector of the same dimension for that word, and put it into the dictionary. This way if we happen to query this word again, we will at least return a consistent vector (as opposed to returning an \"unkown\" or zero vector for all unseen words)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GloVe():\n",
    "    def __init__(self, path, dim):\n",
    "        self.dim = dim\n",
    "        self.word_embedding_dict = {}\n",
    "        with open(path) as f:\n",
    "            for line in f:\n",
    "                values = line.split()\n",
    "                embedding = values[-dim:]\n",
    "                word = ''.join(values[:-dim])\n",
    "                self.word_embedding_dict[word] = np.asarray(embedding, dtype=np.float32)\n",
    "    \n",
    "    def get_word_vector(self, word):\n",
    "        if word not in self.word_embedding_dict.keys():\n",
    "            embedding = np.random.uniform(low=-1, high=1, size=self.dim).astype(np.float32)\n",
    "            self.word_embedding_dict[word] = embedding\n",
    "            return embedding\n",
    "        else:\n",
    "            return self.word_embedding_dict[word]\n",
    "glvmodel = GloVe(os.path.join('..', 'models', 'glove', 'glove.6B.200d.txt'), dim=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gather word embeddings for tokens in the training data\n",
    "- Since the RNN needs machine-readable inputs (hence numbers instead of strings), we need to convert all labels to indices, and all words to embeddings with mappings to indices.\n",
    "- For each token, we query the GloVe model for an embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_embeddings = []\n",
    "assert len(word2index) == len(index2word)\n",
    "for i in range(len(index2word)):\n",
    "    pretrained_embeddings.append(glvmodel.get_word_vector(index2word[i]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Return a list of indexes, one for each word in the sentence, plus EOS\n",
    "def indexes_from_sentence(tokens):\n",
    "    default_idx = word2index[UNKNOWN_TOKEN]\n",
    "    idxs = [word2index.get(word, default_idx) for word in tokens]\n",
    "    return [SOS_token] + idxs + [EOS_token]\n",
    "\n",
    "# Pad a sentence with the PAD symbol\n",
    "def pad_seq(seq, max_length):\n",
    "    seq += [PAD_token for i in range(max_length - len(seq))]\n",
    "    return seq\n",
    "\n",
    "\n",
    "def sequence_mask(sequence_length, max_len=None):\n",
    "    if max_len is None:\n",
    "        max_len = sequence_length.data.max()\n",
    "    batch_size = sequence_length.size(0)\n",
    "    seq_range = torch.arange(0, max_len).long()\n",
    "    seq_range_expand = seq_range.unsqueeze(0).expand(batch_size, max_len)\n",
    "    seq_range_expand = Variable(seq_range_expand)\n",
    "    seq_range_expand = seq_range_expand.to(device)\n",
    "    seq_length_expand = (sequence_length.unsqueeze(1)\n",
    "                         .expand_as(seq_range_expand))\n",
    "    return seq_range_expand < seq_length_expand\n",
    "\n",
    "\n",
    "def masked_cross_entropy(logits, target, length):\n",
    "\n",
    "    length = Variable(torch.LongTensor(length)).to(device)\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        logits: A Variable containing a FloatTensor of size\n",
    "            (batch, max_len, num_classes) which contains the\n",
    "            unnormalized probability for each class.\n",
    "        target: A Variable containing a LongTensor of size\n",
    "            (batch, max_len) which contains the index of the true\n",
    "            class for each corresponding step.\n",
    "        length: A Variable containing a LongTensor of size (batch,)\n",
    "            which contains the length of each data in a batch.\n",
    "\n",
    "    Returns:\n",
    "        loss: An average loss value masked by the length.\n",
    "    \"\"\"\n",
    "\n",
    "    # logits_flat: (batch * max_len, num_classes)\n",
    "    logits_flat = logits.view(-1, logits.size(-1))\n",
    "    # log_probs_flat: (batch * max_len, num_classes)\n",
    "    log_probs_flat = F.log_softmax(logits_flat)\n",
    "    # target_flat: (batch * max_len, 1)\n",
    "    target_flat = target.view(-1, 1)\n",
    "    # losses_flat: (batch * max_len, 1)\n",
    "    losses_flat = -torch.gather(log_probs_flat, dim=1, index=target_flat)\n",
    "    # losses: (batch, max_len)\n",
    "    losses = losses_flat.view(*target.size())\n",
    "    # mask: (batch, max_len)\n",
    "    mask = sequence_mask(sequence_length=length, max_len=target.size(1))\n",
    "    losses = losses * mask.float()\n",
    "    loss = losses.sum() / length.float().sum()\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# copy from model.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    \n",
    "    \"\"\" \n",
    "    Scalars: \n",
    "    input_size: vocabulary size\n",
    "    hidden_size: the hidden dimension\n",
    "    n_layers: number of hidden layers in GRU\n",
    "    \n",
    "    \"\"\" \n",
    "    def __init__(self, input_size, hidden_size, pretrained_embeddings, n_layers=1, dropout=0.1):\n",
    "        \n",
    "        super(EncoderRNN, self).__init__()\n",
    "        \n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.n_layers = n_layers\n",
    "        self.dropout = dropout\n",
    "        \n",
    "        glove_embeddings = torch.tensor(pretrained_embeddings)\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size).\\\n",
    "                from_pretrained(glove_embeddings, freeze=False)\n",
    "        \n",
    "        self.gru = nn.GRU(hidden_size, hidden_size, n_layers, dropout=self.dropout, bidirectional=True)\n",
    "        \n",
    "    def forward(self, input_seqs, input_lengths, hidden=None):\n",
    "        embedded = self.embedding(input_seqs)\n",
    "        \n",
    "        packed = torch.nn.utils.rnn.pack_padded_sequence(embedded, input_lengths)\n",
    "        outputs, hidden = self.gru(packed, hidden)\n",
    "        \n",
    "        # unpack (back to padded)\n",
    "        outputs, output_lengths = torch.nn.utils.rnn.pad_packed_sequence(outputs) \n",
    "        \n",
    "        # Sum bidirectional outputs\n",
    "        outputs = outputs[:, :, :self.hidden_size] + outputs[:, : ,self.hidden_size:] \n",
    "        \n",
    "        return outputs, hidden\n",
    "\n",
    "\n",
    "\n",
    "class Attn(nn.Module):\n",
    "    def __init__(self, method, hidden_size):\n",
    "        super(Attn, self).__init__()\n",
    "        \n",
    "        self.method = method\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        if self.method == 'general':\n",
    "            self.attn = nn.Linear(self.hidden_size, hidden_size)\n",
    "\n",
    "        elif self.method == 'concat':\n",
    "            self.attn = nn.Linear(self.hidden_size * 2, hidden_size)\n",
    "            self.v = nn.Parameter(torch.FloatTensor(1, hidden_size))\n",
    "\n",
    "    def forward(self, hidden, encoder_outputs):\n",
    "        max_len = encoder_outputs.size(0)\n",
    "        this_batch_size = encoder_outputs.size(1)\n",
    "\n",
    "        # Create variable to store attention energies\n",
    "        attn_energies = Variable(torch.zeros(this_batch_size, max_len)) # B x S\n",
    "\n",
    "        attn_energies = attn_energies.to(device)\n",
    "\n",
    "        # For each batch of encoder outputs\n",
    "        for b in range(this_batch_size):\n",
    "            # Calculate energy for each encoder output\n",
    "            for i in range(max_len):\n",
    "                attn_energies[b, i] = self.score(hidden[:, b], encoder_outputs[i, b].unsqueeze(0))\n",
    "\n",
    "        # Normalize energies to weights in range 0 to 1, resize to 1 x B x S\n",
    "        return F.softmax(attn_energies).unsqueeze(1)\n",
    "    \n",
    "    def score(self, hidden, encoder_output):\n",
    "        if self.method == 'dot':\n",
    "            energy =torch.dot(hidden.view(-1), encoder_output.view(-1))\n",
    "        elif self.method == 'general':\n",
    "            energy = self.attn(encoder_output)\n",
    "            energy = torch.dot(hidden.view(-1), energy.view(-1))\n",
    "        elif self.method == 'concat':\n",
    "            energy = self.attn(torch.cat((hidden, encoder_output), 1))\n",
    "            energy = torch.dot(self.v.view(-1), energy.view(-1))\n",
    "        return energy\n",
    "\n",
    "\n",
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, attn_model, hidden_size, output_size, pretrained_embeddings, n_layers=1, dropout=0.1):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "\n",
    "        # Keep for reference\n",
    "        self.attn_model = attn_model\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "        self.dropout = dropout\n",
    "\n",
    "        # Define layers\n",
    "\n",
    "        glove_embeddings = torch.tensor(pretrained_embeddings)\n",
    "        self.embedding = nn.Embedding(output_size, hidden_size).\\\n",
    "                from_pretrained(glove_embeddings, freeze=False)\n",
    "\n",
    "        self.embedding_dropout = nn.Dropout(dropout)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size, n_layers, dropout=dropout)\n",
    "        self.concat = nn.Linear(hidden_size * 2, hidden_size)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "        \n",
    "        # Choose attention model\n",
    "        if attn_model != 'none':\n",
    "            self.attn = Attn(attn_model, hidden_size)\n",
    "\n",
    "    def forward(self, input_seq, last_hidden, encoder_outputs):\n",
    "        # Note: we run this one step at a time\n",
    "\n",
    "        # Get the embedding of the current input word (last output word)\n",
    "        batch_size = input_seq.size(0)\n",
    "        embedded = self.embedding(input_seq)\n",
    "        embedded = self.embedding_dropout(embedded)\n",
    "        embedded = embedded.view(1, batch_size, self.hidden_size) # S=1 x B x N\n",
    "\n",
    "        # Get current hidden state from input word and last hidden state\n",
    "        rnn_output, hidden = self.gru(embedded, last_hidden)\n",
    "\n",
    "        # Calculate attention from current RNN state and all encoder outputs;\n",
    "        # apply to encoder outputs to get weighted average\n",
    "        attn_weights = self.attn(rnn_output, encoder_outputs)\n",
    "        context = attn_weights.bmm(encoder_outputs.transpose(0, 1)) # B x S=1 x N\n",
    "\n",
    "        # Attentional vector using the RNN hidden state and context vector\n",
    "        # concatenated together (Luong eq. 5)\n",
    "        rnn_output = rnn_output.squeeze(0) # S=1 x B x N -> B x N\n",
    "        context = context.squeeze(1)       # B x S=1 x N -> B x N\n",
    "        concat_input = torch.cat((rnn_output, context), 1)\n",
    "        concat_output = F.tanh(self.concat(concat_input))\n",
    "\n",
    "        # Finally predict next token (Luong eq. 6, without softmax)\n",
    "        output = self.out(concat_output)\n",
    "\n",
    "        # Return final output, hidden state, and attention weights (for visualization)\n",
    "        return output, hidden, attn_weights\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## copy from eval.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(input_seq, encoder, decoder, max_length=MAX_LENGTH):\n",
    "    with torch.no_grad(): \n",
    "        input_seqs = [indexes_from_sentence( input_seq)]\n",
    "        input_lengths = [len(input_seq) for input_seq in input_seqs]\n",
    "        input_batches = Variable(torch.LongTensor(input_seqs)).transpose(0, 1).to(device)\n",
    "            \n",
    "        # Set to not-training mode to disable dropout\n",
    "        encoder.train(False)\n",
    "        decoder.train(False)\n",
    "        \n",
    "        # Run through encoder\n",
    "        encoder_outputs, encoder_hidden = encoder(input_batches, input_lengths, None)\n",
    "\n",
    "        # Create starting vectors for decoder\n",
    "        decoder_input = Variable(torch.LongTensor([SOS_token])) # SOS\n",
    "        decoder_hidden = encoder_hidden[:decoder.n_layers] # Use last (forward) hidden state from encoder\n",
    "        \n",
    "        decoder_input = decoder_input.to(device)\n",
    "\n",
    "        # Store output words and attention states\n",
    "        decoded_words = []\n",
    "        decoder_attentions = torch.zeros(max_length + 1, max_length + 1).to(device)\n",
    "        \n",
    "        # Run through decoder\n",
    "        for di in range(max_length):\n",
    "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "                decoder_input, decoder_hidden, encoder_outputs\n",
    "            )\n",
    "            decoder_attentions[di,:decoder_attention.size(2)] += decoder_attention.squeeze(0).squeeze(0).data\n",
    "            #decoder_attentions[di,:decoder_attention.size(2)] += decoder_attention.squeeze(0).squeeze(0).to(config.device).data\n",
    "\n",
    "            # Choose top word from output\n",
    "            topv, topi = decoder_output.data.topk(1)\n",
    "            ni = topi[0][0]\n",
    "            if ni == EOS_token:\n",
    "                decoded_words.append('<EOS>')\n",
    "                break\n",
    "            else:\n",
    "                decoded_words.append(index2word[int(ni)])\n",
    "                \n",
    "            # Next input is chosen word\n",
    "            decoder_input = Variable(torch.LongTensor([ni]))\n",
    "            decoder_input = decoder_input.to(device)\n",
    "\n",
    "        # Set back to training mode\n",
    "        encoder.train(True)\n",
    "        decoder.train(True)\n",
    "        \n",
    "        return decoded_words, decoder_attentions[:di+1, :len(encoder_outputs)]\n",
    "\n",
    "def evaluate_randomly(encoder, decoder, pairs):\n",
    "    article = random.choice(pairs)\n",
    "    headline = article['Headline']\n",
    "    text = article['Text']\n",
    "    print('>', text)\n",
    "    if headline is not None:\n",
    "        print('=', headline)\n",
    "\n",
    "    output_words, attentions = evaluate(headline, encoder, decoder)\n",
    "    output_words = output_words\n",
    "    output_sentence = ' '.join(output_words)\n",
    "    \n",
    "    print('<', output_sentence)\n",
    "    \n",
    "\n",
    "def eval_random_batch(batch_size):\n",
    "    \n",
    "    input_seqs = []\n",
    "    target_seqs = []\n",
    "\n",
    "    # Choose random pairs\n",
    "    for i in range(batch_size):\n",
    "        pair = random.choice(pairs)\n",
    "        input_seqs.append(indexes_from_sentence( pair['Text']))\n",
    "        target_seqs.append(indexes_from_sentence(pair['Headline']))\n",
    "\n",
    "    seq_pairs = sorted(zip(input_seqs, target_seqs), key=lambda p: len(p[0]), reverse=True)\n",
    "    input_seqs, target_seqs = zip(*seq_pairs)\n",
    "    \n",
    "    input_lengths = [len(s) for s in input_seqs]\n",
    "    input_padded = [pad_seq(s, max(input_lengths)) for s in input_seqs]\n",
    "    \n",
    "    target_lengths = [len(s) for s in target_seqs]\n",
    "    target_padded = [pad_seq(s, max(target_lengths)) for s in target_seqs]\n",
    "\n",
    "    input_var = Variable(torch.LongTensor(input_padded)).transpose(0, 1)\n",
    "    target_var = Variable(torch.LongTensor(target_padded)).transpose(0, 1)\n",
    "    \n",
    "    input_var = input_var.to(device)\n",
    "    target_var = target_var.to(device)\n",
    "        \n",
    "    return input_var, input_lengths, target_var, target_lengths\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## copy from train.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def as_minutes(s):\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "def time_since(since, percent):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    es = s / (percent)\n",
    "    rs = es - s\n",
    "    return '%s (- %s)' % (as_minutes(s), as_minutes(rs))\n",
    "\n",
    "def save_checkpoint(encoder, decoder, encoder_optimizer, decoder_optimizer,  name=\"eng_fra_model.pt\"):\n",
    "    path = \"../models/\" + name\n",
    "    torch.save({\n",
    "                'encoder_model_state_dict': encoder.state_dict(),\n",
    "                'decoder_model_state_dict': decoder.state_dict(),\n",
    "                'timestamp': str(datetime.datetime.now()),\n",
    "                }, path)\n",
    "\n",
    "                #'encoder_optimizer_state_dict': encoder_optimizer.state_dict(),\n",
    "                #'decoder_optimizer_state_dict': decoder_optimizer.state_dict(),\n",
    "\n",
    "\n",
    "def train(input_batches, input_lengths, target_batches, target_lengths, batch_size, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, clip):\n",
    "    \n",
    "    # Zero gradients of both optimizers\n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "    loss = 0 # Added onto for each word\n",
    "\n",
    "    input_batches = input_batches.to(device)\n",
    "\n",
    "    # Run words through encoder\n",
    "    encoder_outputs, encoder_hidden = encoder(input_batches, input_lengths, None)\n",
    "    \n",
    "    # Prepare input and output variables\n",
    "    decoder_input = Variable(torch.LongTensor([SOS_token] * batch_size)).to(device)\n",
    "    decoder_hidden = encoder_hidden[:decoder.n_layers] # Use last (forward) hidden state from encoder\n",
    "\n",
    "\n",
    "    max_target_length = max(target_lengths)\n",
    "    all_decoder_outputs = Variable(torch.zeros(max_target_length, batch_size, decoder.output_size)).to(device)\n",
    "\n",
    "\n",
    "    # Run through decoder one time step at a time\n",
    "    for t in range(max_target_length):\n",
    "        decoder_output, decoder_hidden, decoder_attn = decoder(\n",
    "            decoder_input, decoder_hidden, encoder_outputs\n",
    "        )\n",
    "\n",
    "        all_decoder_outputs[t] = decoder_output\n",
    "        decoder_input = target_batches[t] # Next input is current target\n",
    "\n",
    "    # Loss calculation and backpropagation\n",
    "    loss = masked_cross_entropy(\n",
    "        all_decoder_outputs.transpose(0, 1).contiguous(), # -> batch x seq\n",
    "        target_batches.transpose(0, 1).contiguous(), # -> batch x seq\n",
    "        target_lengths\n",
    "    )\n",
    "    loss.backward()\n",
    "    \n",
    "    # Clip gradient norms\n",
    "    ec = torch.nn.utils.clip_grad_norm_(encoder.parameters(), clip)\n",
    "    dc = torch.nn.utils.clip_grad_norm_(decoder.parameters(), clip)\n",
    "\n",
    "    # Update parameters with optimizers\n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "    \n",
    "    #return loss.data[0], ec, dc\n",
    "    return loss.item(), ec, dc\n",
    "\n",
    "def restore_training(encoder, decoder):\n",
    "    print(\"Restoring training environment\")\n",
    "    checkpt = torch.load('../models/sum_model.pt')\n",
    "    encoder.load_state_dict(checkpt[\"encoder_model_state_dict\"])\n",
    "    decoder.load_state_dict(checkpt[\"decoder_model_state_dict\"])\n",
    "    #encoder_optimizer.load_state_dict(checkpt[\"encoder_optimizer_state_dict\"])\n",
    "    #decoder_optimizer.load_state_dict(checkpt[\"decoder_optimizer_state_dict\"])\n",
    "    print(\"Restored training environment\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def train_iter(pairs, encoder, decoder, encoder_optimizer, decoder_optimizer, \\\n",
    "        epoch, n_epochs, batch_size, print_every, evaluate_every, plot_every, save_every, criterion, clip):\n",
    "\n",
    "    start = time.time()\n",
    "    print_loss_total = 0 # Reset every print_every\n",
    "\n",
    "    while epoch < n_epochs:\n",
    "        epoch += 1\n",
    "        \n",
    "        # Get training data for this cycle\n",
    "        input_batches, input_lengths, target_batches, target_lengths = \\\n",
    "                random_batch(batch_size, pairs)\n",
    "\n",
    "        # Run the train function\n",
    "        loss, ec, dc = train(\n",
    "            input_batches, input_lengths, target_batches, target_lengths, batch_size,\n",
    "            encoder, decoder,\n",
    "            encoder_optimizer, decoder_optimizer, criterion, clip\n",
    "        )\n",
    "\n",
    "        # Keep track of loss\n",
    "        print_loss_total += loss\n",
    "        \n",
    "        if epoch % print_every == 0:\n",
    "            print_loss_avg = print_loss_total / print_every\n",
    "            print_loss_total = 0\n",
    "            print_summary = '%s (%d %d%%) %.4f' % (time_since(start, epoch / n_epochs), epoch, epoch / n_epochs * 100, print_loss_avg)\n",
    "            print(print_summary)\n",
    "            \n",
    "\n",
    "        if epoch % save_every == 0:\n",
    "            print(\"The model is saved.\")\n",
    "            save_checkpoint(encoder, decoder, encoder_optimizer, decoder_optimizer)\n",
    "\n",
    "        if epoch % evaluate_every == 0:\n",
    "            evaluate_randomly(encoder, decoder, pairs)\n",
    "\n",
    "\n",
    "def random_batch(batch_size, pairs):\n",
    "    \n",
    "    input_seqs = []\n",
    "    target_seqs = []\n",
    "\n",
    "    # Choose random pairs\n",
    "    for i in range(batch_size):\n",
    "        pair = random.choice(pairs)\n",
    "        input_seqs.append(indexes_from_sentence( pair['Headline']))\n",
    "        target_seqs.append(indexes_from_sentence(pair['Text']))\n",
    "    seq_pairs = sorted(zip(input_seqs, target_seqs), key=lambda p: len(p[0]), reverse=True)\n",
    "    input_seqs, target_seqs = zip(*seq_pairs)\n",
    "    \n",
    "    input_lengths = [len(s) for s in input_seqs]\n",
    "    input_padded = [pad_seq(s, max(input_lengths)) for s in input_seqs]\n",
    "    \n",
    "    target_lengths = [len(s) for s in target_seqs]\n",
    "    target_padded = [pad_seq(s, max(target_lengths)) for s in target_seqs]\n",
    "\n",
    "    input_var = Variable(torch.LongTensor(input_padded)).transpose(0, 1)\n",
    "    target_var = Variable(torch.LongTensor(target_padded)).transpose(0, 1)\n",
    "    \n",
    "    input_var = input_var.to(device)\n",
    "    target_var = target_var.to(device)\n",
    "        \n",
    "    return input_var, input_lengths, target_var, target_lengths\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# copy from main.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_plot(points):\n",
    "    plt.figure()\n",
    "    fig, ax = plt.subplots()\n",
    "    # put ticks at regular intervals\n",
    "    loc = ticker.MultipleLocator(base=0.2) \n",
    "    ax.yaxis.set_major_locator(loc)\n",
    "    plt.plot(points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "attn_model = 'dot'\n",
    "hidden_size = 200\n",
    "n_layers = 2\n",
    "dropout = 0.0\n",
    "\n",
    "batch_size = 4\n",
    "\n",
    "# Configure training/optimization\n",
    "clip = 50.0\n",
    "learning_rate = 0.00001\n",
    "decoder_learning_ratio = 5.0\n",
    "n_epochs = 4000000\n",
    "#n_epochs = 10\n",
    "epoch = 0\n",
    "plot_every = 20\n",
    "save_every = 100\n",
    "print_every = 1\n",
    "evaluate_every = 3\n",
    "weight_decay=0\n",
    "\n",
    "# Initialize models\n",
    "encoder = EncoderRNN(len(word2index), hidden_size, pretrained_embeddings, n_layers, dropout=dropout)\n",
    "decoder = DecoderRNN(attn_model, hidden_size, len(word2index), pretrained_embeddings, n_layers, dropout=dropout)\n",
    "\n",
    "# Initialize optimizers and criterion\n",
    "#encoder_optimizer = optim.SGD(encoder.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "#decoder_optimizer = optim.SGD(decoder.parameters(), lr=learning_rate * decoder_learning_ratio, weight_decay=weight_decay)\n",
    "\n",
    "#restore_training(encoder, decoder)\n",
    "\n",
    "encoder_optimizer = optim.Adam(encoder.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "decoder_optimizer = optim.Adam(decoder.parameters(), lr=learning_rate * decoder_learning_ratio, weight_decay=weight_decay)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "encoder.to(device)\n",
    "decoder.to(device)\n",
    "\n",
    "\n",
    "train_iter(train_data, encoder, decoder, encoder_optimizer, decoder_optimizer, \\\n",
    "        epoch, n_epochs, batch_size, print_every, evaluate_every, \\\n",
    "        plot_every, save_every, criterion, clip)\n",
    "\n",
    "\n",
    "#plot_losses = []\n",
    "#show_plot(plot_losses)\n",
    "\n",
    "evaluate_randomly(encoder, decoder, train_data)\n",
    "\n",
    "save_checkpoint(encoder, decoder, encoder_optimizer, decoder_optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
