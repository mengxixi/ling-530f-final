{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import datetime\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals, print_function, division\n",
    "from io import open\n",
    "import unicodedata\n",
    "import string\n",
    "import re\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "SOS_token = 0\n",
    "EOS_token = 1\n",
    "\n",
    "\n",
    "class Lang:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.word2index = {}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {0: \"SOS\", 1: \"EOS\"}\n",
    "        self.n_words = 2  # Count SOS and EOS\n",
    "\n",
    "    def addSentence(self, sentence):\n",
    "        for word in sentence.split(' '):\n",
    "            self.addWord(word)\n",
    "\n",
    "    def addWord(self, word):\n",
    "        if word not in self.word2index:\n",
    "            self.word2index[word] = self.n_words\n",
    "            self.word2count[word] = 1\n",
    "            self.index2word[self.n_words] = word\n",
    "            self.n_words += 1\n",
    "        else:\n",
    "            self.word2count[word] += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The files are all in Unicode, to simplify we will turn Unicode\n",
    "characters to ASCII, make everything lowercase, and trim most\n",
    "punctuation.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn a Unicode string to plain ASCII, thanks to\n",
    "# http://stackoverflow.com/a/518232/2809427\n",
    "def unicodeToAscii(s):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "    )\n",
    "\n",
    "# Lowercase, trim, and remove non-letter characters\n",
    "\n",
    "\n",
    "def normalizeString(s):\n",
    "    s = unicodeToAscii(s.strip())\n",
    "#     s = unicodeToAscii(s.lower().strip())\n",
    "#     s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
    "#     s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n",
    "    return s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To read the data file we will split the file into lines, and then split\n",
    "lines into pairs. The files are all English → Other Language, so if we\n",
    "want to translate from Other Language → English I added the ``reverse``\n",
    "flag to reverse the pairs.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readLangs(lang1, lang2, reverse=False):\n",
    "    print(\"Reading lines...\")\n",
    "\n",
    "    # Read the file and split into lines\n",
    "#     lines = open('headline_text.txt', encoding='utf-8').\\\n",
    "#         read().strip().split('\\n')\n",
    "    lines = open('eng-fra.txt', encoding='utf-8').\\\n",
    "        read().strip().split('\\n')\n",
    "    \n",
    "    # Split every line into pairs and normalize\n",
    "    pairs = [[normalizeString(s) for s in l.split('\\t')] for l in lines]\n",
    "\n",
    "    # Reverse pairs, make Lang instances\n",
    "    if reverse:\n",
    "        pairs = [list(reversed(p)) for p in pairs]\n",
    "        input_lang = Lang(lang2)\n",
    "        output_lang = Lang(lang1)\n",
    "    else:\n",
    "        input_lang = Lang(lang1)\n",
    "        output_lang = Lang(lang2)\n",
    "    print(\"Done reading\")\n",
    "    return input_lang, output_lang, pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LENGTH = 10\n",
    "\n",
    "eng_prefixes = (\n",
    "    \"i am \", \"i m \",\n",
    "    \"he is\", \"he s \",\n",
    "    \"she is\", \"she s\",\n",
    "    \"you are\", \"you re \",\n",
    "    \"we are\", \"we re \",\n",
    "    \"they are\", \"they re \"\n",
    ")\n",
    "\n",
    "# def filterPair(p):\n",
    "#     return True\n",
    "\n",
    "def filterPair(p):\n",
    "    return len(p[0].split(' ')) < MAX_LENGTH and \\\n",
    "        len(p[1].split(' ')) < MAX_LENGTH \n",
    "\n",
    "# def filterPair(p):\n",
    "#     return len(p[0].split(' ')) < MAX_LENGTH and \\\n",
    "#         len(p[1].split(' ')) < MAX_LENGTH and \\\n",
    "#         p[1].startswith(eng_prefixes)\n",
    "\n",
    "\n",
    "def filterPairs(pairs):\n",
    "    return [pair for pair in pairs if filterPair(pair)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading lines...\n",
      "Done reading\n",
      "Read 160872 sentence pairs\n",
      "Trimmed to 135981 sentence pairs\n",
      "Counting words...\n",
      "Counted words:\n",
      "text 39038\n",
      "headline 22174\n",
      "['Je me soucie de vous toutes.', 'I care about all of you.']\n"
     ]
    }
   ],
   "source": [
    "def prepareData(lang1, lang2, reverse=False):\n",
    "    input_lang, output_lang, pairs = readLangs(lang1, lang2, reverse)\n",
    "    print(\"Read %s sentence pairs\" % len(pairs))\n",
    "    pairs = filterPairs(pairs)\n",
    "    print(\"Trimmed to %s sentence pairs\" % len(pairs))\n",
    "    print(\"Counting words...\")\n",
    "    for pair in pairs:\n",
    "        input_lang.addSentence(pair[0])\n",
    "        output_lang.addSentence(pair[1])\n",
    "    print(\"Counted words:\")\n",
    "    print(input_lang.name, input_lang.n_words)\n",
    "    print(output_lang.name, output_lang.n_words)\n",
    "    return input_lang, output_lang, pairs\n",
    "\n",
    "\n",
    "input_lang, output_lang, pairs = prepareData('headline', 'text', True)\n",
    "print(random.choice(pairs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        embedded = self.embedding(input).view(1, 1, -1)\n",
    "        output = embedded\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        output = self.embedding(input.cpu()).view(1, 1, -1)\n",
    "        output = F.relu(output)\n",
    "        output, hidden = self.gru(output.cpu(), hidden.cpu())\n",
    "        output = self.softmax(self.out(output[0]))\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def indexesFromSentence(lang, sentence):\n",
    "    return [lang.word2index[word] for word in sentence.split(' ')]\n",
    "\n",
    "\n",
    "def tensorFromSentence(lang, sentence):\n",
    "    indexes = indexesFromSentence(lang, sentence)\n",
    "    indexes.append(EOS_token)\n",
    "    return torch.tensor(indexes, dtype=torch.long, device=device).view(-1, 1)\n",
    "\n",
    "\n",
    "def tensorsFromPair(pair):\n",
    "    input_tensor = tensorFromSentence(input_lang, pair[0])\n",
    "    target_tensor = tensorFromSentence(output_lang, pair[1])\n",
    "    return (input_tensor, target_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, max_length=MAX_LENGTH):\n",
    "    encoder_hidden = encoder.initHidden()\n",
    "\n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "\n",
    "    # number of tokens of the single sentence\n",
    "    input_length = input_tensor.size(0)\n",
    "    target_length = target_tensor.size(0)\n",
    "\n",
    "\n",
    "    loss = 0\n",
    "\n",
    "    # loop through each token in a single input sentence\n",
    "    for ei in range(input_length):\n",
    "        encoder_output, encoder_hidden = encoder(input_tensor[ei], encoder_hidden)\n",
    "\n",
    "    # decoder takes a single token as input \n",
    "    decoder_input = torch.tensor([[SOS_token]], device=device)\n",
    "\n",
    "    # pass the final hidden state of encoder\n",
    "    decoder_hidden = encoder_hidden\n",
    "\n",
    "\n",
    "    # loop over each token of the target sentence\n",
    "    # decoder_input: single token\n",
    "    for di in range(target_length):\n",
    "        # decoder_output is a softmax vector over vocabulary size\n",
    "        decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden)\n",
    "        topv, topi = decoder_output.topk(1) \n",
    "        \n",
    "        # squeeze to get rid of dimensions. decoder_input is a scalar tensor\n",
    "        # it is important not to use the SOS, EOS in the language model\n",
    "        # because topi is the location of the most likely token\n",
    "        decoder_input = topi.squeeze().detach()  # detach from history as input\n",
    "\n",
    "        # compare the loss of the current predicted token (decoder_output)\n",
    "        # and the ground truth token\n",
    "        loss += criterion(decoder_output.cpu(), target_tensor[di].cpu())\n",
    "        \n",
    "        # if the most likely token is EOS, stop\n",
    "        if decoder_input.item() == EOS_token:\n",
    "            break\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "\n",
    "    return loss.item() / target_length\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import math\n",
    "\n",
    "\n",
    "def asMinutes(s):\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "\n",
    "def timeSince(since, percent):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    es = s / (percent)\n",
    "    rs = es - s\n",
    "    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainIters(encoder, decoder, n_iters, print_every=1000, plot_every=100, learning_rate=0.01):\n",
    "    start = time.time()\n",
    "    plot_losses = []\n",
    "    print_loss_total = 0  # Reset every print_every\n",
    "    plot_loss_total = 0  # Reset every plot_every\n",
    "\n",
    "    encoder_optimizer = optim.SGD(encoder.parameters(), lr=learning_rate)\n",
    "    decoder_optimizer = optim.SGD(decoder.parameters(), lr=learning_rate)\n",
    "#     encoder_optimizer = optim.Adam(encoder.parameters(), lr=learning_rate)\n",
    "#     decoder_optimizer = optim.Adam(decoder.parameters(), lr=learning_rate)\n",
    "\n",
    "    training_pairs = [tensorsFromPair(random.choice(pairs))\n",
    "                      for i in range(n_iters)]\n",
    "    criterion = nn.NLLLoss()\n",
    "    best_encoder = []\n",
    "    best_decoder = []\n",
    "    lowest_loss = float(\"inf\")\n",
    "    for iter in range(1, n_iters + 1):\n",
    "        # select 1 pair -> input, output tensors\n",
    "        training_pair = training_pairs[iter - 1]\n",
    "        input_tensor = training_pair[0]\n",
    "        target_tensor = training_pair[1]\n",
    "\n",
    "        # train based on 1 pair of input and output tensor\n",
    "        loss = train(input_tensor, target_tensor, encoder,\n",
    "                     decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
    "        if loss < lowest_loss: \n",
    "            lowest_loss = loss\n",
    "            best_encoder = encoder.state_dict()\n",
    "            best_decoder = decoder.state_dict()\n",
    "        print_loss_total += loss\n",
    "        plot_loss_total += loss\n",
    "\n",
    "        if iter % print_every == 0:\n",
    "            print_loss_avg = print_loss_total / print_every\n",
    "            print_loss_total = 0\n",
    "            print('%s (%d %d%%) %.4f' % (timeSince(start, iter / n_iters),\n",
    "                                         iter, iter / n_iters * 100, print_loss_avg))\n",
    "\n",
    "        if iter % plot_every == 0:\n",
    "            plot_loss_avg = plot_loss_total / plot_every\n",
    "            plot_losses.append(plot_loss_avg)\n",
    "            plot_loss_total = 0\n",
    "\n",
    "    showPlot(plot_losses)\n",
    "    return encoder, decoder, encoder_optimizer, decoder_optimizer, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.switch_backend('agg')\n",
    "import matplotlib.ticker as ticker\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def showPlot(points):\n",
    "    plt.figure()\n",
    "    fig, ax = plt.subplots()\n",
    "    # this locator puts ticks at regular intervals\n",
    "    loc = ticker.MultipleLocator(base=0.2)\n",
    "    ax.yaxis.set_major_locator(loc)\n",
    "    plt.plot(points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(encoder, decoder, sentence, max_length=MAX_LENGTH):\n",
    "    with torch.no_grad():\n",
    "        # single sentence as a tensor\n",
    "        input_tensor = tensorFromSentence(input_lang, sentence)\n",
    "        \n",
    "        # num tokens\n",
    "        input_length = input_tensor.size()[0]\n",
    "        \n",
    "        encoder_hidden = encoder.initHidden()\n",
    "\n",
    "        # loop over each token of the input sequence\n",
    "        for ei in range(input_length):\n",
    "            encoder_output, encoder_hidden = encoder(input_tensor[ei], encoder_hidden)\n",
    "\n",
    "        # use SOS as the decoder input token\n",
    "        decoder_input = torch.tensor([[SOS_token]], device=device)  # SOS\n",
    "\n",
    "        # feed the final hidden layer of encoder to the decoder\n",
    "        decoder_hidden = encoder_hidden\n",
    "\n",
    "        decoded_words = []\n",
    "\n",
    "        # max length to sample\n",
    "        for di in range(max_length):\n",
    "            # Note: decoder_input is SOS at first\n",
    "            decoder_output, decoder_hidden = decoder(\n",
    "                decoder_input, decoder_hidden)\n",
    "            \n",
    "            # pick the most likely word\n",
    "            topv, topi = decoder_output.data.topk(1)\n",
    "            \n",
    "            # if it is EOS then, stop. But add <EOS> to it. \n",
    "            if topi.item() == EOS_token:\n",
    "                decoded_words.append('<EOS>')\n",
    "                break\n",
    "            else:\n",
    "                # else, store the actual token as output. \n",
    "                decoded_words.append(output_lang.index2word[topi.item()])\n",
    "            \n",
    "            # squeeze the most likely token to a scalar for the next round\n",
    "            decoder_input = topi.squeeze().detach()\n",
    "\n",
    "        return decoded_words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluateRandomly(encoder, decoder, n=10):\n",
    "    for i in range(n):\n",
    "        pair = random.choice(pairs)\n",
    "        print('>', pair[0])\n",
    "        print('=', pair[1])\n",
    "        output_words = evaluate(encoder, decoder, pair[0])\n",
    "        output_sentence = ' '.join(output_words)\n",
    "        print('<', output_sentence)\n",
    "        print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size = 256\n",
    "encoder1 = EncoderRNN(input_lang.n_words, hidden_size).to(device)\n",
    "decoder1 = DecoderRNN(hidden_size, output_lang.n_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0m 7s (- 37m 37s) (100 0%) 5.1629\n",
      "0m 13s (- 34m 28s) (200 0%) 4.6088\n",
      "0m 20s (- 33m 41s) (300 1%) 4.4145\n",
      "0m 26s (- 32m 54s) (400 1%) 4.2882\n",
      "0m 32s (- 32m 22s) (500 1%) 3.9937\n",
      "0m 39s (- 31m 55s) (600 2%) 4.1445\n",
      "0m 46s (- 32m 10s) (700 2%) 4.4649\n",
      "0m 53s (- 32m 36s) (800 2%) 4.7612\n",
      "1m 0s (- 32m 45s) (900 3%) 4.4024\n",
      "1m 7s (- 32m 41s) (1000 3%) 4.4594\n",
      "1m 14s (- 32m 44s) (1100 3%) 4.2858\n",
      "1m 21s (- 32m 41s) (1200 4%) 4.4093\n",
      "1m 28s (- 32m 25s) (1300 4%) 4.1689\n",
      "1m 34s (- 32m 8s) (1400 4%) 3.8790\n",
      "1m 41s (- 32m 9s) (1500 5%) 4.2603\n",
      "1m 48s (- 32m 6s) (1600 5%) 4.0298\n",
      "1m 55s (- 32m 4s) (1700 5%) 4.2182\n",
      "2m 2s (- 31m 58s) (1800 6%) 4.1023\n",
      "2m 10s (- 32m 2s) (1900 6%) 4.4079\n",
      "2m 16s (- 31m 54s) (2000 6%) 4.2154\n",
      "2m 23s (- 31m 51s) (2100 7%) 4.2274\n",
      "2m 31s (- 31m 54s) (2200 7%) 4.3189\n",
      "2m 38s (- 31m 51s) (2300 7%) 4.2680\n",
      "2m 45s (- 31m 46s) (2400 8%) 4.3076\n",
      "2m 52s (- 31m 42s) (2500 8%) 4.4373\n",
      "3m 0s (- 31m 36s) (2600 8%) 4.3593\n",
      "3m 7s (- 31m 32s) (2700 9%) 4.3987\n",
      "3m 14s (- 31m 27s) (2800 9%) 4.1092\n",
      "3m 21s (- 31m 25s) (2900 9%) 4.3202\n",
      "3m 29s (- 31m 22s) (3000 10%) 4.2401\n",
      "3m 36s (- 31m 17s) (3100 10%) 4.0935\n",
      "3m 43s (- 31m 12s) (3200 10%) 4.3424\n",
      "3m 50s (- 31m 4s) (3300 11%) 4.1238\n",
      "3m 57s (- 30m 59s) (3400 11%) 4.3282\n",
      "4m 4s (- 30m 52s) (3500 11%) 4.1247\n",
      "4m 12s (- 30m 48s) (3600 12%) 4.0749\n",
      "4m 19s (- 30m 44s) (3700 12%) 4.4214\n",
      "4m 26s (- 30m 38s) (3800 12%) 4.2806\n",
      "4m 34s (- 30m 34s) (3900 13%) 4.3402\n",
      "4m 41s (- 30m 30s) (4000 13%) 4.2979\n",
      "4m 49s (- 30m 25s) (4100 13%) 4.2913\n",
      "4m 56s (- 30m 19s) (4200 14%) 4.3126\n",
      "5m 3s (- 30m 14s) (4300 14%) 4.1006\n",
      "5m 10s (- 30m 9s) (4400 14%) 4.1582\n",
      "5m 18s (- 30m 6s) (4500 15%) 4.4375\n",
      "5m 26s (- 30m 0s) (4600 15%) 4.4291\n",
      "5m 33s (- 29m 55s) (4700 15%) 4.1555\n",
      "5m 40s (- 29m 48s) (4800 16%) 4.2540\n",
      "5m 48s (- 29m 43s) (4900 16%) 4.2157\n",
      "5m 56s (- 29m 41s) (5000 16%) 4.3037\n",
      "6m 3s (- 29m 34s) (5100 17%) 4.2921\n",
      "6m 10s (- 29m 28s) (5200 17%) 3.8852\n",
      "6m 17s (- 29m 21s) (5300 17%) 4.2090\n",
      "6m 25s (- 29m 16s) (5400 18%) 4.2738\n",
      "6m 32s (- 29m 9s) (5500 18%) 4.1048\n",
      "6m 40s (- 29m 3s) (5600 18%) 4.1678\n",
      "6m 47s (- 28m 57s) (5700 19%) 4.1461\n",
      "6m 55s (- 28m 53s) (5800 19%) 4.2691\n",
      "7m 2s (- 28m 47s) (5900 19%) 4.2823\n",
      "7m 10s (- 28m 41s) (6000 20%) 4.1307\n",
      "7m 18s (- 28m 37s) (6100 20%) 4.2649\n",
      "7m 26s (- 28m 33s) (6200 20%) 4.2327\n",
      "7m 34s (- 28m 28s) (6300 21%) 4.3982\n",
      "7m 41s (- 28m 23s) (6400 21%) 4.1471\n",
      "7m 49s (- 28m 18s) (6500 21%) 4.2026\n",
      "7m 57s (- 28m 12s) (6600 22%) 4.1032\n",
      "8m 4s (- 28m 4s) (6700 22%) 4.1309\n",
      "8m 11s (- 27m 57s) (6800 22%) 3.8755\n",
      "8m 19s (- 27m 51s) (6900 23%) 4.0266\n",
      "8m 26s (- 27m 45s) (7000 23%) 4.3613\n",
      "8m 34s (- 27m 39s) (7100 23%) 4.2234\n",
      "8m 42s (- 27m 34s) (7200 24%) 4.5045\n",
      "8m 50s (- 27m 29s) (7300 24%) 4.2026\n",
      "8m 58s (- 27m 23s) (7400 24%) 4.1935\n",
      "9m 5s (- 27m 17s) (7500 25%) 4.1747\n",
      "9m 14s (- 27m 12s) (7600 25%) 4.1633\n",
      "9m 22s (- 27m 7s) (7700 25%) 4.2206\n",
      "9m 30s (- 27m 3s) (7800 26%) 4.5052\n",
      "9m 38s (- 26m 57s) (7900 26%) 4.1757\n",
      "9m 45s (- 26m 49s) (8000 26%) 3.9230\n",
      "9m 53s (- 26m 43s) (8100 27%) 4.0319\n",
      "10m 0s (- 26m 37s) (8200 27%) 4.1980\n",
      "10m 8s (- 26m 30s) (8300 27%) 4.1143\n",
      "10m 16s (- 26m 24s) (8400 28%) 4.0821\n",
      "10m 24s (- 26m 18s) (8500 28%) 4.1700\n",
      "10m 32s (- 26m 13s) (8600 28%) 4.1558\n",
      "10m 40s (- 26m 7s) (8700 28%) 4.0355\n",
      "10m 47s (- 26m 0s) (8800 29%) 4.1929\n",
      "10m 55s (- 25m 54s) (8900 29%) 4.1228\n",
      "11m 3s (- 25m 48s) (9000 30%) 3.9662\n",
      "11m 11s (- 25m 41s) (9100 30%) 4.3118\n",
      "11m 18s (- 25m 35s) (9200 30%) 4.0448\n",
      "11m 27s (- 25m 29s) (9300 31%) 4.0381\n",
      "11m 34s (- 25m 23s) (9400 31%) 3.9312\n",
      "11m 43s (- 25m 17s) (9500 31%) 3.9521\n",
      "11m 51s (- 25m 12s) (9600 32%) 4.1471\n",
      "11m 59s (- 25m 5s) (9700 32%) 4.1347\n",
      "12m 7s (- 24m 59s) (9800 32%) 4.2107\n",
      "12m 15s (- 24m 54s) (9900 33%) 4.0370\n",
      "12m 23s (- 24m 47s) (10000 33%) 4.1623\n",
      "12m 31s (- 24m 40s) (10100 33%) 4.0099\n",
      "12m 39s (- 24m 34s) (10200 34%) 3.9429\n",
      "12m 48s (- 24m 29s) (10300 34%) 4.3532\n",
      "12m 56s (- 24m 23s) (10400 34%) 4.1561\n",
      "13m 4s (- 24m 17s) (10500 35%) 4.2238\n",
      "13m 13s (- 24m 11s) (10600 35%) 4.3721\n",
      "13m 21s (- 24m 6s) (10700 35%) 4.4130\n",
      "13m 30s (- 24m 0s) (10800 36%) 4.3528\n",
      "13m 37s (- 23m 53s) (10900 36%) 4.0852\n",
      "13m 45s (- 23m 46s) (11000 36%) 4.1290\n",
      "13m 54s (- 23m 40s) (11100 37%) 4.1447\n",
      "14m 2s (- 23m 35s) (11200 37%) 4.3540\n",
      "14m 10s (- 23m 28s) (11300 37%) 4.0650\n",
      "14m 18s (- 23m 21s) (11400 38%) 4.2246\n",
      "14m 27s (- 23m 15s) (11500 38%) 4.2167\n",
      "14m 35s (- 23m 8s) (11600 38%) 4.1530\n",
      "14m 43s (- 23m 2s) (11700 39%) 4.0944\n",
      "14m 52s (- 22m 56s) (11800 39%) 4.3440\n",
      "15m 0s (- 22m 49s) (11900 39%) 4.2057\n",
      "15m 8s (- 22m 43s) (12000 40%) 4.3605\n",
      "15m 16s (- 22m 36s) (12100 40%) 4.1009\n",
      "15m 25s (- 22m 30s) (12200 40%) 4.0438\n",
      "15m 33s (- 22m 22s) (12300 41%) 4.2334\n",
      "15m 41s (- 22m 15s) (12400 41%) 4.1870\n",
      "15m 49s (- 22m 9s) (12500 41%) 4.2109\n",
      "15m 58s (- 22m 3s) (12600 42%) 4.1353\n",
      "16m 6s (- 21m 56s) (12700 42%) 4.0584\n",
      "16m 14s (- 21m 49s) (12800 42%) 4.4064\n",
      "16m 23s (- 21m 43s) (12900 43%) 3.9359\n",
      "16m 32s (- 21m 37s) (13000 43%) 4.1144\n",
      "16m 40s (- 21m 30s) (13100 43%) 4.0547\n",
      "16m 48s (- 21m 23s) (13200 44%) 4.1375\n",
      "16m 57s (- 21m 17s) (13300 44%) 4.2261\n",
      "17m 5s (- 21m 10s) (13400 44%) 4.0778\n",
      "17m 14s (- 21m 4s) (13500 45%) 4.2964\n",
      "17m 22s (- 20m 57s) (13600 45%) 4.0303\n",
      "17m 30s (- 20m 50s) (13700 45%) 4.1017\n",
      "17m 39s (- 20m 43s) (13800 46%) 3.9913\n",
      "17m 47s (- 20m 36s) (13900 46%) 4.2308\n",
      "17m 55s (- 20m 29s) (14000 46%) 3.8195\n",
      "18m 4s (- 20m 22s) (14100 47%) 4.0064\n",
      "18m 12s (- 20m 15s) (14200 47%) 3.9976\n",
      "18m 20s (- 20m 8s) (14300 47%) 4.1455\n",
      "18m 29s (- 20m 1s) (14400 48%) 3.9488\n",
      "18m 38s (- 19m 55s) (14500 48%) 4.0318\n",
      "18m 45s (- 19m 47s) (14600 48%) 4.0704\n",
      "18m 53s (- 19m 40s) (14700 49%) 4.1534\n",
      "19m 2s (- 19m 33s) (14800 49%) 4.1951\n",
      "19m 11s (- 19m 26s) (14900 49%) 4.0185\n",
      "19m 19s (- 19m 19s) (15000 50%) 4.3361\n",
      "19m 28s (- 19m 12s) (15100 50%) 4.4518\n",
      "19m 36s (- 19m 5s) (15200 50%) 4.2944\n",
      "19m 44s (- 18m 58s) (15300 51%) 4.1962\n",
      "19m 53s (- 18m 51s) (15400 51%) 4.0991\n",
      "20m 1s (- 18m 44s) (15500 51%) 4.1905\n",
      "20m 10s (- 18m 37s) (15600 52%) 4.1016\n",
      "20m 19s (- 18m 30s) (15700 52%) 4.1275\n",
      "20m 27s (- 18m 23s) (15800 52%) 4.0958\n",
      "20m 36s (- 18m 16s) (15900 53%) 4.1879\n",
      "20m 44s (- 18m 9s) (16000 53%) 4.1882\n",
      "20m 53s (- 18m 1s) (16100 53%) 4.0713\n",
      "21m 1s (- 17m 54s) (16200 54%) 3.8911\n",
      "21m 9s (- 17m 47s) (16300 54%) 4.1078\n",
      "21m 19s (- 17m 40s) (16400 54%) 4.2960\n",
      "21m 27s (- 17m 33s) (16500 55%) 4.3201\n",
      "21m 36s (- 17m 26s) (16600 55%) 4.0118\n",
      "21m 44s (- 17m 19s) (16700 55%) 3.9303\n",
      "21m 53s (- 17m 11s) (16800 56%) 4.2147\n",
      "22m 2s (- 17m 5s) (16900 56%) 3.9916\n",
      "22m 11s (- 16m 57s) (17000 56%) 3.9890\n",
      "22m 20s (- 16m 51s) (17100 56%) 4.2313\n",
      "22m 28s (- 16m 43s) (17200 57%) 4.1098\n",
      "22m 37s (- 16m 36s) (17300 57%) 3.9600\n",
      "22m 45s (- 16m 29s) (17400 57%) 4.0602\n",
      "22m 54s (- 16m 21s) (17500 58%) 3.8592\n",
      "23m 3s (- 16m 14s) (17600 58%) 3.9312\n",
      "23m 12s (- 16m 7s) (17700 59%) 3.8371\n",
      "23m 20s (- 16m 0s) (17800 59%) 3.9237\n",
      "23m 29s (- 15m 52s) (17900 59%) 4.2114\n",
      "23m 38s (- 15m 45s) (18000 60%) 4.0219\n",
      "23m 46s (- 15m 37s) (18100 60%) 4.0033\n",
      "23m 55s (- 15m 30s) (18200 60%) 4.0868\n",
      "24m 3s (- 15m 23s) (18300 61%) 4.1672\n",
      "24m 12s (- 15m 15s) (18400 61%) 4.2022\n",
      "24m 21s (- 15m 8s) (18500 61%) 4.2597\n",
      "24m 30s (- 15m 1s) (18600 62%) 4.0529\n",
      "24m 38s (- 14m 53s) (18700 62%) 3.8380\n",
      "24m 47s (- 14m 46s) (18800 62%) 3.7844\n",
      "24m 56s (- 14m 38s) (18900 63%) 4.2090\n",
      "25m 4s (- 14m 31s) (19000 63%) 4.0363\n",
      "25m 13s (- 14m 23s) (19100 63%) 3.9114\n",
      "25m 21s (- 14m 15s) (19200 64%) 3.8240\n",
      "25m 30s (- 14m 8s) (19300 64%) 4.1904\n",
      "25m 38s (- 14m 0s) (19400 64%) 3.9434\n",
      "25m 47s (- 13m 53s) (19500 65%) 4.1481\n",
      "25m 55s (- 13m 45s) (19600 65%) 4.0246\n",
      "26m 3s (- 13m 37s) (19700 65%) 4.0465\n",
      "26m 12s (- 13m 30s) (19800 66%) 3.9424\n",
      "26m 21s (- 13m 22s) (19900 66%) 3.9351\n",
      "26m 29s (- 13m 14s) (20000 66%) 3.9882\n",
      "26m 38s (- 13m 7s) (20100 67%) 3.9565\n",
      "26m 47s (- 12m 59s) (20200 67%) 4.1583\n",
      "26m 55s (- 12m 52s) (20300 67%) 3.7369\n",
      "27m 4s (- 12m 44s) (20400 68%) 3.9803\n",
      "27m 12s (- 12m 36s) (20500 68%) 3.8397\n",
      "27m 22s (- 12m 29s) (20600 68%) 4.0873\n",
      "27m 30s (- 12m 21s) (20700 69%) 4.1639\n",
      "27m 39s (- 12m 13s) (20800 69%) 3.9802\n",
      "27m 48s (- 12m 6s) (20900 69%) 3.8841\n",
      "27m 57s (- 11m 58s) (21000 70%) 4.1498\n",
      "28m 5s (- 11m 50s) (21100 70%) 3.7139\n",
      "28m 14s (- 11m 43s) (21200 70%) 4.0206\n",
      "28m 23s (- 11m 35s) (21300 71%) 4.0013\n",
      "28m 31s (- 11m 27s) (21400 71%) 3.7009\n",
      "28m 40s (- 11m 20s) (21500 71%) 3.8152\n",
      "28m 49s (- 11m 12s) (21600 72%) 3.9272\n",
      "28m 58s (- 11m 4s) (21700 72%) 4.2068\n",
      "29m 6s (- 10m 57s) (21800 72%) 3.8843\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29m 15s (- 10m 49s) (21900 73%) 3.9835\n",
      "29m 24s (- 10m 41s) (22000 73%) 3.7871\n",
      "29m 32s (- 10m 33s) (22100 73%) 3.8502\n",
      "29m 41s (- 10m 25s) (22200 74%) 3.9597\n",
      "29m 50s (- 10m 18s) (22300 74%) 3.9851\n",
      "29m 59s (- 10m 10s) (22400 74%) 3.9049\n",
      "30m 8s (- 10m 2s) (22500 75%) 4.1600\n",
      "30m 16s (- 9m 54s) (22600 75%) 3.7916\n",
      "30m 25s (- 9m 47s) (22700 75%) 3.8734\n",
      "30m 34s (- 9m 39s) (22800 76%) 4.0059\n",
      "30m 43s (- 9m 31s) (22900 76%) 3.8753\n",
      "30m 52s (- 9m 23s) (23000 76%) 3.8507\n",
      "31m 1s (- 9m 15s) (23100 77%) 4.0822\n",
      "31m 9s (- 9m 7s) (23200 77%) 3.7238\n",
      "31m 18s (- 9m 0s) (23300 77%) 3.9777\n",
      "31m 27s (- 8m 52s) (23400 78%) 4.0022\n",
      "31m 35s (- 8m 44s) (23500 78%) 3.8459\n",
      "31m 44s (- 8m 36s) (23600 78%) 4.0362\n",
      "31m 53s (- 8m 28s) (23700 79%) 3.9296\n",
      "32m 2s (- 8m 20s) (23800 79%) 3.8906\n",
      "32m 10s (- 8m 12s) (23900 79%) 3.7968\n",
      "32m 19s (- 8m 4s) (24000 80%) 3.9403\n",
      "32m 28s (- 7m 57s) (24100 80%) 3.8302\n",
      "32m 37s (- 7m 49s) (24200 80%) 3.9566\n",
      "32m 45s (- 7m 41s) (24300 81%) 3.8691\n",
      "32m 55s (- 7m 33s) (24400 81%) 3.9611\n",
      "33m 4s (- 7m 25s) (24500 81%) 4.0415\n",
      "33m 13s (- 7m 17s) (24600 82%) 3.9510\n",
      "33m 22s (- 7m 9s) (24700 82%) 3.8284\n",
      "33m 30s (- 7m 1s) (24800 82%) 3.6813\n",
      "33m 39s (- 6m 53s) (24900 83%) 3.7189\n",
      "33m 48s (- 6m 45s) (25000 83%) 3.9198\n",
      "33m 57s (- 6m 37s) (25100 83%) 4.0474\n",
      "34m 6s (- 6m 29s) (25200 84%) 3.6796\n",
      "34m 14s (- 6m 21s) (25300 84%) 3.7941\n",
      "34m 23s (- 6m 13s) (25400 84%) 3.7652\n",
      "34m 32s (- 6m 5s) (25500 85%) 3.7283\n",
      "34m 40s (- 5m 57s) (25600 85%) 3.7981\n",
      "34m 50s (- 5m 49s) (25700 85%) 3.9968\n",
      "34m 58s (- 5m 41s) (25800 86%) 3.8766\n",
      "35m 7s (- 5m 33s) (25900 86%) 3.7452\n",
      "35m 16s (- 5m 25s) (26000 86%) 3.9021\n",
      "35m 25s (- 5m 17s) (26100 87%) 3.9296\n",
      "35m 34s (- 5m 9s) (26200 87%) 3.8574\n",
      "35m 43s (- 5m 1s) (26300 87%) 3.7442\n",
      "35m 52s (- 4m 53s) (26400 88%) 3.8556\n",
      "36m 0s (- 4m 45s) (26500 88%) 3.8806\n",
      "36m 9s (- 4m 37s) (26600 88%) 3.6221\n",
      "36m 18s (- 4m 29s) (26700 89%) 3.9531\n",
      "36m 27s (- 4m 21s) (26800 89%) 4.0078\n",
      "36m 37s (- 4m 13s) (26900 89%) 3.8536\n",
      "36m 45s (- 4m 5s) (27000 90%) 3.8624\n",
      "36m 54s (- 3m 57s) (27100 90%) 3.6791\n",
      "37m 3s (- 3m 48s) (27200 90%) 3.8624\n",
      "37m 12s (- 3m 40s) (27300 91%) 3.9431\n",
      "37m 21s (- 3m 32s) (27400 91%) 3.8827\n",
      "37m 30s (- 3m 24s) (27500 91%) 3.7884\n",
      "37m 39s (- 3m 16s) (27600 92%) 3.6681\n",
      "37m 48s (- 3m 8s) (27700 92%) 3.8108\n",
      "37m 57s (- 3m 0s) (27800 92%) 3.9283\n",
      "38m 6s (- 2m 52s) (27900 93%) 3.5936\n",
      "38m 14s (- 2m 43s) (28000 93%) 3.7429\n",
      "38m 22s (- 2m 35s) (28100 93%) 3.5749\n",
      "38m 31s (- 2m 27s) (28200 94%) 3.8735\n",
      "38m 40s (- 2m 19s) (28300 94%) 3.6715\n",
      "38m 49s (- 2m 11s) (28400 94%) 3.8982\n",
      "38m 58s (- 2m 3s) (28500 95%) 3.7433\n",
      "39m 7s (- 1m 54s) (28600 95%) 3.8110\n",
      "39m 16s (- 1m 46s) (28700 95%) 3.6903\n",
      "39m 25s (- 1m 38s) (28800 96%) 3.9067\n",
      "39m 35s (- 1m 30s) (28900 96%) 3.7960\n",
      "39m 43s (- 1m 22s) (29000 96%) 3.7711\n",
      "39m 52s (- 1m 14s) (29100 97%) 3.9776\n",
      "40m 2s (- 1m 5s) (29200 97%) 3.9566\n",
      "40m 10s (- 0m 57s) (29300 97%) 3.8508\n",
      "40m 19s (- 0m 49s) (29400 98%) 3.5677\n",
      "40m 28s (- 0m 41s) (29500 98%) 3.5585\n",
      "40m 38s (- 0m 32s) (29600 98%) 4.0719\n",
      "40m 46s (- 0m 24s) (29700 99%) 3.9929\n",
      "40m 55s (- 0m 16s) (29800 99%) 3.7974\n",
      "41m 4s (- 0m 8s) (29900 99%) 3.7636\n",
      "41m 13s (- 0m 0s) (30000 100%) 3.8823\n"
     ]
    }
   ],
   "source": [
    "encoder, decoder, encoder_optimizer, decoder_optimizer, loss = \\\n",
    "            trainIters(encoder1, decoder1, n_iters=30000, print_every=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Je fais ca pour vivre.\n",
      "= I do this for a living.\n",
      "< I do for for <EOS>\n",
      "\n",
      "> Que suis-je suppose faire ?\n",
      "= What am I supposed to do?\n",
      "< What do you to <EOS>\n",
      "\n",
      "> Quel sport preferes-tu ?\n",
      "= Which sport do you like the most?\n",
      "< What is is of <EOS>\n",
      "\n",
      "> J'ai a nouveau faim.\n",
      "= I'm hungry again.\n",
      "< I have to <EOS>\n",
      "\n",
      "> Je n'ai pas apprecie mon instructrice.\n",
      "= I didn't like my teacher.\n",
      "< I didn't have my my <EOS>\n",
      "\n",
      "> Puis-je te parler une seconde ?\n",
      "= Can I talk to you a sec?\n",
      "< Can I want to me <EOS>\n",
      "\n",
      "> La facture se montait a 100 dollars.\n",
      "= The bill amounted to 100 dollars.\n",
      "< The is is to to to <EOS>\n",
      "\n",
      "> Je dois m'y rendre seule.\n",
      "= I must go alone.\n",
      "< I have to to to <EOS>\n",
      "\n",
      "> Tu peux te rendre ou bon te semble.\n",
      "= You can go wherever you want to go.\n",
      "< You can you to you <EOS>\n",
      "\n",
      "> Elle n'a pas aime son mari.\n",
      "= She didn't like her husband.\n",
      "< She didn't his to his his <EOS>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "evaluateRandomly(encoder1, decoder1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(encoder, decoder, encoder_optimizer, decoder_optimizer,  loss, name=\"eng_fra_model.pt\"):\n",
    "    path = \"./save/\" + name\n",
    "    torch.save({\n",
    "                'encoder_model_state_dict': encoder.state_dict(),\n",
    "                'decoder_model_state_dict': decoder.state_dict(),\n",
    "                'encoder_optimizer_state_dict': encoder_optimizer.state_dict(),\n",
    "                'decoder_optimizer_state_dict': decoder_optimizer.state_dict(),\n",
    "                'timestamp': str(datetime.datetime.now()),\n",
    "                'loss': loss,\n",
    "                }, path)\n",
    "save_checkpoint(encoder1, decoder1, encoder_optimizer, decoder_optimizer, loss)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
