{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"jumbotron\">\n",
    "  <h1 class=\"display-3\">LING 530F DL-NLP Project: Automatic Text Summarization</h1>\n",
    "  <p class=\"lead\"><font size=\"3\">Zicong Fan&nbsp;&nbsp;&nbsp;&nbsp;11205168 &nbsp;&nbsp;&nbsp;&nbsp;zfan@alumni.ubc.ca</font></p>\n",
    "  <p class=\"lead\"><font size=\"3\">Si Yi (Cathy) Meng&nbsp;&nbsp;&nbsp;&nbsp;32939118&nbsp;&nbsp;&nbsp;&nbsp;mengxixi@cs.ubc.ca</font></p>\n",
    "  <p class=\"lead\"><font size=\"3\">Zixuan Yin&nbsp;&nbsp;&nbsp;&nbsp;11687143 &nbsp;&nbsp;&nbsp;&nbsp;krystal_yzx@naver.com</font></p>\n",
    "  <hr class=\"my-4\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "import math\n",
    "import random \n",
    "import shutil\n",
    "import datetime\n",
    "import logging\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torch.nn.utils.rnn import pad_packed_sequence, pack_padded_sequence\n",
    "from allennlp.modules.elmo import Elmo, batch_to_ids\n",
    "from pyrouge import Rouge155"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# logging configurations\n",
    "LOG_FORMAT = \"%(asctime)s %(message)s\"\n",
    "logging.basicConfig(level=logging.INFO, format=LOG_FORMAT, datefmt=\"%H:%M:%S\")\n",
    "\n",
    "# seeding for reproducibility\n",
    "random.seed(1)\n",
    "np.random.seed(2)\n",
    "torch.manual_seed(3)\n",
    "torch.cuda.manual_seed(4)\n",
    "\n",
    "# define directory structure needed for data processing\n",
    "TMP_DIR = os.path.join(\"..\", \"data\", \"tmp\")\n",
    "TRAIN_DIR = os.path.join(\"..\", \"data\", \"gigaword\",\"train_sample\")\n",
    "DEV_DIR = os.path.join(\"..\", \"data\", \"gigaword\",\"valid\")\n",
    "TEST_DIR = os.path.join(\"..\", \"data\", \"gigaword\",\"test\")\n",
    "MODEL_DIR = os.path.join(\"..\", 'models')\n",
    "CHECKPOINT_FNAME = \"gigaword.ckpt\"\n",
    "GOLD_DIR = os.path.join(TMP_DIR, \"gold\")\n",
    "SYSTEM_DIR = os.path.join(TMP_DIR, \"system\")\n",
    "TRUE_HEADLINE_FNAME = 'gold.A.0.txt'\n",
    "PRED_HEADLINE_FNAME = 'system.0.txt'\n",
    "\n",
    "for d in [TRAIN_DIR, DEV_DIR, TEST_DIR, TMP_DIR, GOLD_DIR, SYSTEM_DIR, MODEL_DIR]:\n",
    "    if not os.path.exists(d):\n",
    "        os.makedirs(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "### Extract text body and headline from the Annotated English Gigaword dataset\n",
    "- This was a script ran separately (modified based on the provided preprocessing script) \n",
    "- Here we use the `CommunicationReader` in the `concrete` package to read the xml files\n",
    "    - After extracting the paired headline and body, we tokenize them using `nltk`\n",
    "    - We lowercased all tokens\n",
    "    - Removed punctuations\n",
    "    - Removed pairs where headline had less than 3 tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "from concrete.util import CommunicationReader\n",
    "from concrete.util import lun, get_tokens\n",
    "import json\n",
    "import os\n",
    "import glob\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "import string\n",
    "import regex as re\n",
    "import threading\n",
    "import queue\n",
    "import sys\n",
    "import time\n",
    "\n",
    "from multiprocessing import Process, Queue\n",
    "\n",
    "def f(q):\n",
    "    q.put([42, None, 'hello'])\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    q = Queue()\n",
    "    p = Process(target=f, args=(q,))\n",
    "    p.start()\n",
    "    print(q.get())    # prints \"[42, None, 'hello']\"\n",
    "    p.join()\n",
    "\n",
    "\n",
    "def readData(data_path):\n",
    "    '''\n",
    "    data_path -- path to the file that contains the preprossed data\n",
    "    '''\n",
    "    '''return a list of object {'Headline': string, 'Text': string}'''\n",
    "    data = []\n",
    "    with open(data_path) as f:\n",
    "        for line in f:\n",
    "            obj = json.loads(line)\n",
    "            data.append(obj)\n",
    "    return data\n",
    "\n",
    "\n",
    "def worker(in_queue, out_queue):\n",
    "    while not stopping.is_set():\n",
    "        try:\n",
    "            tar_file = in_queue.get(True, timeout=1)\n",
    "            print(\"Processing %s\" % tar_file)\n",
    "            t = time.time()\n",
    "            res = preprocess(tar_file, OUTPUT_PATH)\n",
    "            print(\"Elapsed Time %.2f\"%(time.time() - t))\n",
    "            out_queue.put(res)\n",
    "\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "        in_queue.task_done()\n",
    "\n",
    "\n",
    "def preprocess(tar_path, output_path):\n",
    "    '''\n",
    "    tar_path  -- tar file to process\n",
    "    output_path -- directory of the output file\n",
    "                   each line of the output file has the format {'Headline': string, 'Text': string}\n",
    "    '''\n",
    "\n",
    "    fname = \"%s.txt\" % tar_path.split('/')[-1].split('.')[0]\n",
    "    out_fname = os.path.join(output_path, fname)\n",
    "\n",
    "    mem = {}\n",
    "\n",
    "    with open(out_fname, 'w') as f:\n",
    "        for (comm, filename) in CommunicationReader(tar_path):\n",
    "            text = comm.text\n",
    "            headline_start = text.find(\"<HEADLINE>\")\n",
    "            headline_end = text.find('</HEADLINE>',headline_start)\n",
    "            par1_start = text.find(\"<P>\",headline_end)\n",
    "            par1_end = text.find(\"</P>\",par1_start)\n",
    "            headline = text[headline_start + len('<HEADLINE>'):headline_end].strip()\n",
    "            par1 = text[par1_start + len(\"<P>\"):par1_end].strip()\n",
    "            if headline in mem.keys():\n",
    "                continue\n",
    "            else:\n",
    "                mem[headline] = par1\n",
    "            \n",
    "            # print(headline)\n",
    "            # print(par1)\n",
    "\n",
    "            #process healline\n",
    "            if comm.id.startswith(\"XIN\"):\n",
    "                #for xinhua headline, remove anything before : or anything after :\n",
    "                #Example sentences that need to be modified:\n",
    "                #Roundup: Gulf Arab markets end on a mixed note\n",
    "                #Israelis more distrustful of gov't institutions: survey\n",
    "                a = headline.find(\":\")\n",
    "                if a != -1:\n",
    "                    b = headline.rfind(\":\")\n",
    "                    if a == b:\n",
    "                        if a < len(headline) / 2:\n",
    "                            headline = headline[a + 1:]\n",
    "                        else:\n",
    "                            headline = headline[:b]\n",
    "                    else:\n",
    "                        headline = headline[a + 1:b]\n",
    "            headline_token = word_tokenize(headline)\n",
    "            #remove punctuations\n",
    "            headline_token = [t.strip(string.punctuation).lower() for t in headline_token]\n",
    "\n",
    "            #ignore if headline is too short\n",
    "            if len(headline_token) < 3:\n",
    "                continue\n",
    "            \n",
    "            #process the first paragraph\n",
    "            par1_token = word_tokenize(par1)\n",
    "            #remove punctuations\n",
    "            par1_token = [t.strip(string.punctuation).lower() for t in par1_token]\n",
    "            \n",
    "            headline = \" \".join([t for t in headline_token])\n",
    "            par1 = \" \".join([t for t in par1_token])\n",
    "            obj = {'Headline': headline, \"Text\": par1}\n",
    "            json_str = json.dumps(obj)\n",
    "            f.write(json_str + '\\n')\n",
    "    print(\"completed file %s\" % fname)\n",
    "    return fname\n",
    "\n",
    "with open('todolist1.txt') as f:\n",
    "    content = f.readlines()\n",
    "SOURCES = [x.strip() for x in content] \n",
    "print(SOURCES)\n",
    "\n",
    "\n",
    "tars = []\n",
    "for s in SOURCES:\n",
    "    tars.extend(glob.glob(os.path.join(\"/media/sda1/gigaword/data/gigaword\", s)))\n",
    "\n",
    "\n",
    "OUTPUT_PATH = os.path.join(\".\", 'gigaword')\n",
    "if not os.path.exists(OUTPUT_PATH):\n",
    "    os.makedirs(OUTPUT_PATH)\n",
    "\n",
    "\n",
    "\n",
    "stopping = threading.Event()\n",
    "\n",
    "work = queue.Queue()\n",
    "results = queue.Queue()\n",
    "total = len(tars)\n",
    "\n",
    "# start for workers\n",
    "for i in range(4):\n",
    "    t = threading.Thread(target=worker, args=(work, results))\n",
    "    t.daemon = True\n",
    "    t.start()\n",
    "\n",
    "# produce data\n",
    "for i in range(total):\n",
    "    work.put(tars[i])\n",
    "\n",
    "print(\"waiting for workers to finish\")\n",
    "work.join()\n",
    "stopping.set()\n",
    "\n",
    "# get the results\n",
    "for i in range(total):\n",
    "    print(results.get())\n",
    "\n",
    "sys.exit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "### Downsampling the training set\n",
    "- The entire training set would yield a vocabulary that's too big for our memory even after removing low frequency tokens\n",
    "- Therefore we downsample the training set by randomly dropping data pairs with probability 0.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "# downsample the training set by dropping pairs with probability 0.4 \n",
    "random.seed(0)   # here we used a different seed since it was ran as a separate script\n",
    "output = os.path.join(\".\",\"data\",\"gigaword\",\"train_sample.txt\")\n",
    "with open(output,'w+') as fo:                \n",
    "    for fname in os.listdir(TRAIN_DIR):\n",
    "        fpath = os.path.join(TRAIN_DIR, fname)\n",
    "        with open(fpath) as fin:\n",
    "            for line in fin:\n",
    "                tmp = random.random()\n",
    "                if tmp < 0.4:\n",
    "                    continue\n",
    "                fo.write(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "PAD_token = 0  # padding\n",
    "SOS_token = 1  # start of sentence\n",
    "EOS_token = 2  # end of sentence\n",
    "UNKNOWN_TOKEN = 'unk' \n",
    "\n",
    "MAX_OUTPUT_LENGTH = 35    # max length of summary generated\n",
    "MAX_HEADLINE_LENGTH = 30  # max length of headline (target) from the data\n",
    "MAX_TEXT_LENGTH = 50      # max length of text body from the data\n",
    "MIN_TEXT_LENGTH = 5       # min length of text body for it to be a valid data point\n",
    "MIN_FREQUENCY   = 6       # token with frequency <= MIN_FREQUENCY will be converted to 'unk'\n",
    "MIN_KNOWN_COUNT = 3       # headline (target) must have at least MIN_KNOWN_COUNT number of known tokens\n",
    "\n",
    "EMBEDDING_DIM = 256\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess tokenized data\n",
    "- First, we build a frequency dict on the downsampled training set (referred to as the training set hereafter), including all words from text body and headline\n",
    "- Then we further process the training data \n",
    "    - **Truncate text body to MAX_TEXT_LENGTH**\n",
    "    - **Removed pairs where headline is too long (our aim is to generate concise 1-liner summaries)**\n",
    "    - **Removed pairs where body is too short (barely anything to summarize from)**\n",
    "    - **Removed pairs where headline does not have enough known (frequent) words**\n",
    "    - **Replaced all low frequency words with the 'unk' token**\n",
    "- We sorted all the tokens based on their frequency (from high to low)\n",
    "    - This is needed for Adaptive Softmax, explained in the paper\n",
    "- Finally, we build the word2index and the reverse mapping based on the sorted frequencies, giving each token an index based on how often they appear\n",
    "    - PAD, SOS and EOS appear in every sentence, so it makes sense to put them at the first 3 indices\n",
    "- We also pickle the data objects (train/dev/test data, word2idx and its reverse map) to allow us directly load them from disk without repetitively processing them to save time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "pkl_names = ['train_data', 'dev_data', 'test_data', 'word2index', 'index2word']\n",
    "pickles = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "15:53:18 Building frequency dict on TRAIN data...\n",
      "15:53:18 Working on file: train_sample.txt\n",
      "15:54:15 Number of unique tokens: 1016085\n",
      "15:54:15 Load TRAIN data and remove low frequency tokens...\n",
      "15:55:45 Removed 11405 pairs due to not enough known words in headline\n",
      "15:55:45 Removed 73576 pairs due to headline length greater than MAX_HEADLINE_LENGTH\n",
      "15:55:45 Removed 66603 pairs due to text length less than MIN_TEXT_LENGTH\n",
      "15:55:45 Number of unique tokens after replacing low frequency ones: 179038\n",
      "15:55:45 Load DEV data...\n",
      "15:55:47 Load TEST data and take a random subset of 2000 valid pairs...\n"
     ]
    }
   ],
   "source": [
    "vocab_freq_dict = {}\n",
    "WORD_2_INDEX = {\"PAD\": 0, \"SOS\": 1, \"EOS\": 2}\n",
    "INDEX_2_WORD = {0: \"PAD\", 1: \"SOS\", 2: \"EOS\"}\n",
    "\n",
    "def update_freq_dict(freq_dict, tokens):\n",
    "    for t in tokens:\n",
    "        if t not in freq_dict:\n",
    "            freq_dict[t] = 0\n",
    "        freq_dict[t] += 1\n",
    "\n",
    "def build_freq_dict(data_dir):\n",
    "    freq_dict = dict()\n",
    "    for fname in os.listdir(data_dir):\n",
    "        logging.info(\"Working on file: \" + fname)\n",
    "        fpath = os.path.join(data_dir, fname)\n",
    "        with open(fpath) as f:\n",
    "            for line in f:\n",
    "                obj = json.loads(line)\n",
    "                headline = [t for t in obj['Headline'].split()]\n",
    "                text = [t for t in obj['Text'].split()]\n",
    "                update_freq_dict(freq_dict, headline)\n",
    "                update_freq_dict(freq_dict, text)\n",
    "    return freq_dict\n",
    "\n",
    "def remove_low_freq_words(freq_dict, tokens):\n",
    "    filtered_tokens = []\n",
    "    known_count = 0\n",
    "    for t in tokens:\n",
    "        if freq_dict[t] > MIN_FREQUENCY:\n",
    "            filtered_tokens.append(t)\n",
    "            known_count += 1\n",
    "        else:\n",
    "            filtered_tokens.append(UNKNOWN_TOKEN)\n",
    "    return filtered_tokens, known_count\n",
    "\n",
    "\n",
    "def update_word_index(word2index, index2word, tokens):\n",
    "    for t in tokens:\n",
    "        if t not in word2index:\n",
    "            next_index = len(word2index)\n",
    "            word2index[t] = next_index\n",
    "            index2word[next_index] = t\n",
    "\n",
    "def read_data(data_dir):\n",
    "    ignore_count = [0,0,0]\n",
    "    data = []\n",
    "    unk_count = 0\n",
    "    for fname in os.listdir(data_dir):\n",
    "\n",
    "        fpath = os.path.join(data_dir, fname)\n",
    "        with open(fpath) as f:\n",
    "            for line in f:\n",
    "                obj = json.loads(line)\n",
    "                headline = [t for t in obj['Headline'].split()]\n",
    "                text = [t for t in obj['Text'].split()][:MAX_TEXT_LENGTH]\n",
    "                if data_dir == TRAIN_DIR:\n",
    "                    if len(headline) > MAX_HEADLINE_LENGTH:\n",
    "                        ignore_count[1] += 1\n",
    "                        continue\n",
    "                    if len(text) < MIN_TEXT_LENGTH:\n",
    "                        ignore_count[2] +=1\n",
    "                        continue\n",
    "                    headline, known_count = remove_low_freq_words(freq_dict, headline)\n",
    "                    if known_count < MIN_KNOWN_COUNT:\n",
    "                        ignore_count[0] += 1\n",
    "                        continue\n",
    "\n",
    "                    text, _ = remove_low_freq_words(freq_dict, text) \n",
    "                    for token in (headline + text):\n",
    "                        if token == 'unk':\n",
    "                            unk_count += 1\n",
    "                        elif token not in vocab_freq_dict.keys():\n",
    "                            vocab_freq_dict[token] = freq_dict[token]\n",
    "\n",
    "                data.append((headline, text))\n",
    "\n",
    "    # Now ready to build word indexes\n",
    "    if data_dir == TRAIN_DIR:\n",
    "        vocab_freq_dict['unk'] = unk_count\n",
    "        sorted_words = sorted(vocab_freq_dict, key=vocab_freq_dict.get, reverse=True)\n",
    "        update_word_index(WORD_2_INDEX, INDEX_2_WORD, sorted_words)\n",
    "\n",
    "    return data, ignore_count\n",
    "\n",
    "logging.info(\"Building frequency dict on TRAIN data...\")\n",
    "freq_dict = build_freq_dict(TRAIN_DIR)\n",
    "\n",
    "logging.info(\"Number of unique tokens: %d\", len(freq_dict))\n",
    "\n",
    "logging.info(\"Load TRAIN data and remove low frequency tokens...\")\n",
    "train_data, ignore_count = read_data(TRAIN_DIR)\n",
    "assert len(WORD_2_INDEX) == len(INDEX_2_WORD)\n",
    "VOCAB_SIZE = len(WORD_2_INDEX)\n",
    "\n",
    "logging.info(\"Removed %d pairs due to not enough known words in headline\", ignore_count[0])\n",
    "logging.info(\"Removed %d pairs due to headline length greater than MAX_HEADLINE_LENGTH\", ignore_count[1])\n",
    "logging.info(\"Removed %d pairs due to text length less than MIN_TEXT_LENGTH\", ignore_count[2])\n",
    "logging.info(\"Number of unique tokens after replacing low frequency ones: %d\", VOCAB_SIZE)\n",
    "\n",
    "logging.info(\"Load DEV data...\")\n",
    "dev_data, _ = read_data(DEV_DIR)\n",
    "\n",
    "logging.info(\"Load TEST data and take a random subset of 2000 valid pairs...\")\n",
    "test_data, _ = read_data(TEST_DIR)\n",
    "test_data = [data for data in test_data if len(data[1])>0]\n",
    "random.shuffle(test_data)\n",
    "test_data = test_data[:2000]\n",
    "\n",
    "# persist data objects\n",
    "for i, item in enumerate([train_data, dev_data, test_data, WORD_2_INDEX, INDEX_2_WORD]):\n",
    "    with open(os.path.join(TMP_DIR, pkl_names[i]+\".pkl\"), 'wb') as handle:\n",
    "        pickle.dump(item, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Load pickles without re-loading from scratch\n",
    "- TODO: After loaidng the following, get summary stats on the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples:  3768318\n",
      "Number of dev examples:  346462\n",
      "Number of test examples:  2000\n",
      "Vocabulary size:  214322\n"
     ]
    }
   ],
   "source": [
    "for i, name in enumerate(pkl_names):\n",
    "    with open(os.path.join(TMP_DIR, name+'.pkl'), 'rb') as handle:\n",
    "        pickles.append(pickle.load(handle))\n",
    "train_data = pickles[0]\n",
    "dev_data = pickles[1]\n",
    "test_data = pickles[2]\n",
    "WORD_2_INDEX = pickles[3]\n",
    "INDEX_2_WORD = pickles[4]\n",
    "\n",
    "assert len(WORD_2_INDEX) == len(INDEX_2_WORD)\n",
    "VOCAB_SIZE = len(WORD_2_INDEX)\n",
    "print(\"Number of training examples: \", len(train_data))\n",
    "print(\"Number of dev examples: \", len(dev_data))\n",
    "print(\"Number of test examples: \", len(test_data))\n",
    "print(\"Vocabulary size: \", VOCAB_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load ELMo Embeddings\n",
    "- We use the ELMo model with dimension 256 to generate pre_trained embeddings for our vocabulary\n",
    "- Since ELMo is context-based, meaning it may give a different embedding for a token that appears in a different sentence, we need to perform the following\n",
    "    - Pass in the entire training set (where the vocabulary is taken from)\n",
    "    - For each pair, we concatenate the text body and the headline as if it was all in one sentence, and pass that into ELMo (in a batch)\n",
    "    - For each embedding we get back, we check if we already have an embedding for this token, if we do, we'll take the average of the embeddings for this same token (over all counts of this token)\n",
    "- Since this process takes hours, we ran it once and pickle the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "options_file = \"https://s3-us-west-2.amazonaws.com/allennlp/models/elmo/2x1024_128_2048cnn_1xhighway/elmo_2x1024_128_2048cnn_1xhighway_options.json\"\n",
    "weight_file = \"https://s3-us-west-2.amazonaws.com/allennlp/models/elmo/2x1024_128_2048cnn_1xhighway/elmo_2x1024_128_2048cnn_1xhighway_weights.hdf5\"\n",
    "\n",
    "class ELMoEmbedding():\n",
    "    def __init__(self, corpus, options, weights, dim, batch_size=32):\n",
    "        self.elmo = Elmo(options, weights, 1, dropout=0).to(device)\n",
    "        self.dim = dim\n",
    "        self.corpus = corpus\n",
    "        self.word_embedding_dict = {}\n",
    "\n",
    "        # Start loading embeddings\n",
    "        random.shuffle(corpus)\n",
    "        end_index = len(corpus) - len(corpus) % batch_size\n",
    "        input_seqs = []\n",
    "        target_seqs = []\n",
    "        \n",
    "        # Choose random pairs\n",
    "        for i in range(0, end_index, batch_size):\n",
    "            pairs = corpus[i:i+batch_size]\n",
    "            sentences = [pair[0] + pair[1] for pair in pairs]\n",
    "            character_ids = batch_to_ids(sentences).to(device)\n",
    "            embeddings = self.elmo(character_ids)[\"elmo_representations\"][0].cpu().data.numpy()\n",
    "\n",
    "            for i, sent in enumerate(sentences):\n",
    "                for j, token in enumerate(sent):\n",
    "                    token_count = freq_dict[token]\n",
    "                    token_emb = embeddings[i,j,:]\n",
    "                    if token not in self.word_embedding_dict.keys():\n",
    "                        self.word_embedding_dict[token] = token_emb/token_count\n",
    "\n",
    "                    else:\n",
    "                        token_emb = np.sum([token_emb/token_count, self.word_embedding_dict[token]], axis=0)\n",
    "                        self.word_embedding_dict[token] = token_emb\n",
    "\n",
    "    def get_word_vector(self, word):\n",
    "        if word not in self.word_embedding_dict.keys():\n",
    "            embedding = np.random.uniform(low=-1, high=1, size=self.dim).astype(np.float32)\n",
    "            self.word_embedding_dict[word] = embedding\n",
    "            return embedding\n",
    "        else:\n",
    "            return self.word_embedding_dict[word]\n",
    "\n",
    "logging.info(\"Start loading training data embeddings with ELMo\")\n",
    "elmo_embedding = ELMoEmbedding(train_data, options_file, weight_file, dim=EMBEDDING_DIM)\n",
    "\n",
    "logging.info(\"Start gathering pretrained embeddings\")\n",
    "\n",
    "pretrained_embeddings = []\n",
    "for i in range(VOCAB_SIZE):\n",
    "    pretrained_embeddings.append(elmo_embedding.get_word_vector(INDEX_2_WORD[i]))\n",
    "\n",
    "with open(os.path.join(TMP,  \"elmo_pretrained.pkl\"), 'wb') as handle:\n",
    "    pickle.dump(pretrained_embeddings, handle, protocol=pickle.HIGHEST_PROTOCOL)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Load pickled embeddings without generating from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(TMP_DIR,  \"elmo_pretrained.pkl\"), 'rb') as handle:\n",
    "    pretrained_embeddings = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Some helper functions for training\n",
    "- When we retrieve the token indices, we append the `EOS` to let the model learn to predict the next word as `EOS` when it should stop\n",
    "- We also pad a sequence with `PAD` when it doesn't meet max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Return a list of indexes, one for each word in the sentence, plus EOS\n",
    "def indexes_from_sentence(tokens,isHeadline):\n",
    "    default_idx = WORD_2_INDEX[UNKNOWN_TOKEN]\n",
    "    idxs = [WORD_2_INDEX.get(word, default_idx) for word in tokens]\n",
    "    if isHeadline:\n",
    "        idxs = idxs + [EOS_token]\n",
    "    return idxs\n",
    "\n",
    "# Pad a sentence with the PAD token\n",
    "def pad_seq(seq, max_length):\n",
    "    seq += [PAD_token for i in range(max_length - len(seq))]\n",
    "    return seq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adaptive Softmax\n",
    "- explained in the paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def masked_adasoft(logits, target, lengths, adasoft):\n",
    "    loss = 0\n",
    "    for i in range(logits.size(0)):\n",
    "        mask = (np.array(lengths) > i).astype(int)\n",
    "\n",
    "        mask = torch.LongTensor(np.nonzero(mask)[0]).to(device)\n",
    "        logits_i = logits[i].index_select(0, mask)\n",
    "        logits_i = logits_i.to(device)\n",
    "        \n",
    "        targets_i = target[i].index_select(0, mask).to(device)\n",
    "      \n",
    "        asm_output = adasoft(logits_i, targets_i)\n",
    "        loss += asm_output.loss*len(targets_i)\n",
    "   \n",
    "    loss /= sum(lengths)\n",
    "  \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Architecture\n",
    "- seq2seq (GRU encoder, GRU decoder, Luong Attention) \n",
    "- more explanations in the paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def param_init(params):\n",
    "    for name, param in params:\n",
    "        if 'bias' in name:\n",
    "             nn.init.constant_(param, 0.0)\n",
    "        elif 'weight' in name:\n",
    "            nn.init.xavier_normal_(param)\n",
    "\n",
    "class EncoderRNN(nn.Module):\n",
    "    \"\"\" \n",
    "    Scalars: \n",
    "    input_size: vocabulary size\n",
    "    hidden_size: the hidden dimension\n",
    "    n_layers: number of hidden layers in GRU\n",
    "    \n",
    "    \"\"\" \n",
    "    def __init__(self, input_size, hidden_size, embed_size,pretrained_embeddings, n_layers, dropout):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        \n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.n_layers = n_layers\n",
    "        self.dropout = dropout\n",
    "        self.embed_size = embed_size\n",
    "        \n",
    "        self.embedding = nn.Embedding(input_size, embed_size).from_pretrained(torch.FloatTensor(pretrained_embeddings), freeze=True)\n",
    "        \n",
    "        self.gru = nn.GRU(embed_size, hidden_size, n_layers, dropout=self.dropout, bidirectional=True)\n",
    "        param_init(self.gru.named_parameters())\n",
    "        \n",
    "    def forward(self, input_seqs, input_lengths, hidden=None):\n",
    "        embedded = self.embedding(input_seqs)\n",
    "        packed = torch.nn.utils.rnn.pack_padded_sequence(embedded, input_lengths)\n",
    "\n",
    "        outputs, hidden = self.gru(packed, hidden)\n",
    "        \n",
    "        # unpack (back to padded)\n",
    "        outputs, output_lengths = torch.nn.utils.rnn.pad_packed_sequence(outputs) \n",
    "        return outputs, hidden\n",
    "\n",
    "\n",
    "class Attn(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super(Attn, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "    def forward(self, hidden, encoder_outputs):\n",
    "        attn_energies = torch.bmm(hidden.transpose(0,1), encoder_outputs.permute(1,2,0)).squeeze(1)\n",
    "        return F.softmax(attn_energies, dim=1).unsqueeze(1)\n",
    "\n",
    "\n",
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size, embed_size, pretrained_embeddings, n_layers=1, dropout=0.1):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "\n",
    "        # Keep for reference\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "        self.dropout = dropout\n",
    "        self.embed_size = embed_size\n",
    "\n",
    "        # Define layers\n",
    "        self.embedding = nn.Embedding(output_size, hidden_size).from_pretrained(torch.FloatTensor(pretrained_embeddings), freeze=True)\n",
    "        self.embedding_dropout = nn.Dropout(dropout)\n",
    "        self.gru = nn.GRU(embed_size, hidden_size, n_layers, dropout=dropout)\n",
    "        self.concat = nn.Linear(hidden_size * 2, hidden_size)\n",
    "        self.out = nn.Linear(hidden_size, FC_DIM)\n",
    "        \n",
    "        # Use Attention\n",
    "        self.attn = Attn(hidden_size)\n",
    "        param_init(self.gru.named_parameters())\n",
    "        param_init(self.concat.named_parameters())\n",
    "        param_init(self.out.named_parameters())\n",
    "\n",
    "    def forward(self, input_seq, last_hidden, encoder_outputs):\n",
    "        # Note: we run this one step at a time\n",
    "\n",
    "        # Get the embedding of the current input word (last output word)\n",
    "        batch_size = input_seq.size(0)\n",
    "        embedded = self.embedding(input_seq)\n",
    "        embedded = self.embedding_dropout(embedded)\n",
    "        embedded = embedded.view(1, batch_size, self.embed_size) # S=1 x B x N\n",
    "\n",
    "        # Get current hidden state from input word and last hidden state\n",
    "        rnn_output, hidden = self.gru(embedded, last_hidden)\n",
    "\n",
    "        # Calculate attention from current RNN state and all encoder outputs;\n",
    "        # apply to encoder outputs to get weighted average\n",
    "        attn_weights = self.attn(rnn_output, encoder_outputs)\n",
    "        context = attn_weights.bmm(encoder_outputs.transpose(0, 1)) # B x S=1 x N\n",
    "\n",
    "        # Attentional vector using the RNN hidden state and context vector\n",
    "        # concatenated together (Luong eq. 5)\n",
    "        rnn_output = rnn_output.squeeze(0) # S=1 x B x N -> B x N\n",
    "        context = context.squeeze(1)       # B x S=1 x N -> B x N\n",
    "        concat_input = torch.cat((rnn_output, context), 1)\n",
    "        concat_output = torch.tanh(self.concat(concat_input))\n",
    "\n",
    "        # Finally predict next token (Luong eq. 6, without softmax)\n",
    "        output = self.out(concat_output)\n",
    "\n",
    "        # Return final output, hidden state, and attention weights (for visualization)\n",
    "        return output, hidden, attn_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Batching helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_batch(batch_size, data):\n",
    "    random.shuffle(data)\n",
    "    end_index = len(data) - len(data) % batch_size\n",
    "    input_seqs = []\n",
    "    target_seqs = []\n",
    "    \n",
    "    # Choose random pairs\n",
    "    for i in range(0, end_index, batch_size):\n",
    "        pairs = data[i:i+batch_size]\n",
    "        input_seqs = [indexes_from_sentence( pair[1], isHeadline=False) for pair in pairs]\n",
    "        target_seqs = [indexes_from_sentence(pair[0], isHeadline=True) for pair in pairs]\n",
    "        seq_pairs = sorted(zip(input_seqs, target_seqs), key=lambda p: len(p[0]), reverse=True)\n",
    "        input_seqs, target_seqs = zip(*seq_pairs)\n",
    "    \n",
    "        input_lengths = [len(s) for s in input_seqs]\n",
    "        input_padded = [pad_seq(s, max(input_lengths)) for s in input_seqs]\n",
    "        \n",
    "        target_lengths = [len(s) for s in target_seqs]\n",
    "        target_padded = [pad_seq(s, max(target_lengths)) for s in target_seqs]\n",
    "\n",
    "        input_var = Variable(torch.LongTensor(input_padded)).transpose(0, 1)\n",
    "        target_var = Variable(torch.LongTensor(target_padded)).transpose(0, 1)\n",
    "        \n",
    "        input_var = input_var.to(device)\n",
    "        target_var = target_var.to(device)\n",
    "        yield input_var, input_lengths, target_var, target_lengths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Training subroutine for each batch\n",
    "- Here we run each batch data through the encoder\n",
    "- Encoder outputs (combined with previous step's decoder output) are ran through the decoder one step at a time until max_target_length is reached as teacher forcing\n",
    "- Loss is computed for all decoder outputs against the target sequence\n",
    "- Backpropagate, clip the gradient's norm to prevent gradient explosion\n",
    "- Finally, weights are updated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_batch(input_batches, input_lengths, target_batches, target_lengths, batch_size, encoder, decoder, encoder_optimizer, decoder_optimizer, clip):\n",
    "    \n",
    "    # Zero gradients of both optimizers\n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "    loss = 0 # Added onto for each word\n",
    "\n",
    "    input_batches = input_batches.to(device)\n",
    "\n",
    "    # Run words through encoder\n",
    "    encoder_outputs, encoder_hidden = encoder(input_batches, input_lengths, None)\n",
    "    \n",
    "    # Prepare input and output variables\n",
    "    decoder_input = Variable(torch.LongTensor([SOS_token] * batch_size)).to(device)\n",
    "    decoder_hidden = torch.cat((encoder_hidden[0], encoder_hidden[1]),1)\n",
    "    for i in range(1, encoder.n_layers):\n",
    "        decoder_hidden = torch.stack((decoder_hidden,torch.cat((encoder_hidden[i*2],encoder_hidden[i*2+1]),1)))\n",
    "    decoder_hidden = decoder_hidden.to(device)\n",
    "\n",
    "    max_target_length = max(target_lengths)\n",
    "    all_decoder_outputs = Variable(torch.zeros(max_target_length, batch_size, FC_DIM)).to(device)\n",
    "\n",
    "    # Run through decoder one time step at a time\n",
    "    for t in range(max_target_length):\n",
    "        decoder_output, decoder_hidden, decoder_attn = decoder(\n",
    "            decoder_input, decoder_hidden, encoder_outputs\n",
    "        )\n",
    "\n",
    "        all_decoder_outputs[t] = decoder_output\n",
    "        decoder_input = target_batches[t] # Next input is current target \n",
    "    # Loss calculation and backpropagation\n",
    "    loss = masked_adasoft(all_decoder_outputs, target_batches, target_lengths, crit)\n",
    "    loss.backward()\n",
    "    \n",
    "    # Clip gradient norms\n",
    "    ec = torch.nn.utils.clip_grad_norm_(encoder.parameters(), clip)\n",
    "    dc = torch.nn.utils.clip_grad_norm_(decoder.parameters(), clip)\n",
    "\n",
    "    # Update parameters with optimizers\n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "    \n",
    "    #return loss.data[0], ec, dc\n",
    "    return loss.item(), ec, dc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main train loop\n",
    "- For each epoch, we go through the dataset once and train in batches\n",
    "- We log the running loss every 25 batches\n",
    "- We evaluate on a random pair every 100 batches\n",
    "    - Run the text through the model, print the generated headline/summary, compare it with ground truth\n",
    "- Every 1000 batches we update a checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(pairs, encoder, decoder, encoder_optimizer, decoder_optimizer, n_epochs, batch_size, clip):\n",
    "    logging.info(\"Start training\")\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        logging.info(\"Starting epoch: %d\", epoch)\n",
    "        running_loss = 0\n",
    "        \n",
    "        # Get training data for this epoch\n",
    "        for batch_ind, batch_data in enumerate(random_batch(batch_size, pairs)):\n",
    "            input_seqs, input_lengths, target_seqs, target_lengths = batch_data\n",
    "            # Run the train subroutine\n",
    "            loss, ec, dc = train_batch(\n",
    "                input_seqs, input_lengths, target_seqs, target_lengths, batch_size,\n",
    "                encoder, decoder,\n",
    "                encoder_optimizer, decoder_optimizer, clip\n",
    "            )\n",
    "            # Keep track of loss\n",
    "            running_loss += loss\n",
    "\n",
    "            if batch_ind % 25 == 0:\n",
    "                avg_running_loss = running_loss / 25\n",
    "                running_loss = 0\n",
    "                logging.info(\"Iteration: %d running loss: %f\", batch_ind, avg_running_loss)\n",
    "            \n",
    "            if batch_ind % 100 == 0:\n",
    "                logging.info(\"Iteration: %d, evaluating\", batch_ind)\n",
    "                evaluate_randomly(encoder, decoder, pairs)\n",
    "\n",
    "            if batch_ind % 1000 == 0:\n",
    "                logging.info(\"Iteration: %d model saved\",batch_ind)\n",
    "                save_checkpoint(encoder, decoder, encoder_optimizer, decoder_optimizer, name=CHECKPOINT_FNAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(encoder, decoder, encoder_optimizer, decoder_optimizer, name=CHECKPOINT_FNAME):\n",
    "    path = os.path.join(MODEL_DIR, name)\n",
    "    torch.save({'encoder_model_state_dict': encoder.state_dict(),\n",
    "                'decoder_model_state_dict': decoder.state_dict(),\n",
    "                'encoder_optimizer_state_dict':encoder_optimizer.state_dict(),\n",
    "                'decoder_optimizer_state_dict':decoder_optimizer.state_dict(),\n",
    "                'timestamp': str(datetime.datetime.now()),\n",
    "                }, path)\n",
    "\n",
    "def load_checkpoint(encoder, decoder, encoder_optimizer, decoder_optimizer, name=CHECKPOINT_FNAME):\n",
    "    path = os.path.join(MODEL_DIR, name)\n",
    "    if os.path.isfile(path):\n",
    "        logging.info(\"Loading checkpoint\")\n",
    "        checkpoint = torch.load(path)\n",
    "        encoder.load_state_dict(checkpoint['encoder_model_state_dict'])\n",
    "        decoder.load_state_dict(checkpoint['decoder_model_state_dict'])\n",
    "        encoder_optimizer.load_state_dict(checkpoint['encoder_optimizer_state_dict'])\n",
    "        decoder_optimizer.load_state_dict(checkpoint['decoder_optimizer_state_dict'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(input_seq, encoder, decoder, max_length=MAX_OUTPUT_LENGTH):\n",
    "    with torch.no_grad(): \n",
    "        input_seqs = [indexes_from_sentence( input_seq, isHeadline = False)]\n",
    "        input_lengths = [len(input_seq) for input_seq in input_seqs]\n",
    "        input_batches = Variable(torch.LongTensor(input_seqs)).transpose(0, 1).to(device)\n",
    "            \n",
    "        # Set to eval mode to disable dropout\n",
    "        encoder.train(False)\n",
    "        decoder.train(False)\n",
    "        \n",
    "        # Run through encoder\n",
    "        encoder_outputs, encoder_hidden = encoder(input_batches, input_lengths, None)\n",
    "\n",
    "        # Create starting vectors for decoder\n",
    "        decoder_input = Variable(torch.LongTensor([SOS_token])).to(device) # SOS\n",
    "        decoder_hidden = torch.cat((encoder_hidden[0], encoder_hidden[1]),1)\n",
    "        for i in range(1, encoder.n_layers):\n",
    "            decoder_hidden = torch.stack((decoder_hidden,torch.cat((encoder_hidden[i*2],encoder_hidden[i*2+1]),1)))\n",
    "        decoder_hidden = decoder_hidden.to(device)\n",
    "      \n",
    "        # Store output words and attention states\n",
    "        decoded_words = []\n",
    "        decoder_attentions = torch.zeros(max_length + 1, max_length + 1).to(device)\n",
    "        \n",
    "        # Run through decoder\n",
    "        for di in range(max_length):\n",
    "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "                decoder_input, decoder_hidden, encoder_outputs\n",
    "            )\n",
    "\n",
    "            # Choose top word from output\n",
    "            ni = crit.predict(decoder_output)\n",
    "            if ni == EOS_token:\n",
    "                decoded_words.append('<EOS>')\n",
    "                break\n",
    "            else:\n",
    "                decoded_words.append(INDEX_2_WORD[int(ni)])\n",
    "                \n",
    "            # Next input is chosen word\n",
    "            decoder_input = Variable(torch.LongTensor([ni]))\n",
    "            decoder_input = decoder_input.to(device)\n",
    "\n",
    "        # Set back to training mode\n",
    "        encoder.train(True)\n",
    "        decoder.train(True)\n",
    "        \n",
    "        return decoded_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_randomly(encoder, decoder, pairs):\n",
    "    article = random.choice(pairs)\n",
    "    headline = article[0]\n",
    "    text = article[1]\n",
    "    print('>', ' '.join(text))\n",
    "    print('=', ' '.join(headline))\n",
    "\n",
    "    output_words = evaluate(text, encoder, decoder)\n",
    "    output_sentence = ' '.join(output_words)\n",
    "    \n",
    "    print('<', output_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing with Rouge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = Rouge155()\n",
    "r.system_dir = SYSTEM_DIR\n",
    "r.model_dir = GOLD_DIR\n",
    "r.system_filename_pattern = 'system.(\\d+).txt'\n",
    "r.model_filename_pattern = 'gold.[A-Z].#ID#.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_rouge(data, encoder, decoder):\n",
    "    logging.info(\"Start testing\")\n",
    "\n",
    "    texts = [text for (_, text) in data]\n",
    "    true_headlines = [headline for (headline,_) in data]\n",
    "    write_headlines_to_file(os.path.join(GOLD_DIR,TRUE_HEADLINE_FNAME), true_headlines)\n",
    "\n",
    "    pred_headlines = [evaluate(text, encoder, decoder) for text in texts]\n",
    "    assert len(dev_true_headline) == len(pred_headlines)\n",
    "    write_headlines_to_file(os.path.join(SYSTEM_DIR, PRED_HEADLINE_FNAME), pred_headlines)\n",
    "    output = r.convert_and_evaluate()\n",
    "    print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters\n",
    "- Choices explained in the paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model architecture related\n",
    "HIDDEN_SIZE = 200\n",
    "N_LAYERS = 2\n",
    "DROPOUT_PROB = 0.5\n",
    "DECODER_LEARNING_RATIO = 5.0\n",
    "\n",
    "# Training and optimization related\n",
    "N_EPOCHS = 2\n",
    "BATCH_SIZE = 32\n",
    "GRAD_CLIP = 50.0\n",
    "LR = 1e-4\n",
    "WEIGHT_DECAY = 1e-4\n",
    "\n",
    "# Adasoft related\n",
    "CUTOFFS = [1000, 20000]\n",
    "FC_DIM = 512"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Kick off training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "17:23:38 Start training\n",
      "17:23:38 Starting epoch: 0\n",
      "17:23:40 Iteration: 0 running loss: 0.429141\n",
      "17:23:40 Iteration: 0, evaluating\n",
      "17:23:40 Iteration: 0 model saved\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> the croatian port of dubrovnik reopened saturday after being paralysed for three years due to a serb siege that caused extensive damage the croatian news agency hina reported\n",
      "= dubrovnik port reopens after three-year serb siege\n",
      "< vietnam vietnam points important his <EOS>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "17:23:44 Iteration: 25 running loss: 8.892239\n",
      "17:23:47 Iteration: 50 running loss: 8.244532\n",
      "17:23:51 Iteration: 75 running loss: 8.089746\n",
      "17:23:54 Iteration: 100 running loss: 8.014745\n",
      "17:23:54 Iteration: 100, evaluating\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> canada announced measures to restrict its relationship with zimbabwe sunday after president robert mugabe was sworn in for a new term following a victory in the election held earlier this week\n",
      "= canada announces measures against zimbabwe after mugabe sworn in for sixth term of presidency\n",
      "< s s to to to to to to in <EOS>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "17:23:57 Iteration: 125 running loss: 7.971904\n",
      "17:24:00 Iteration: 150 running loss: 7.963484\n",
      "17:24:04 Iteration: 175 running loss: 7.924144\n",
      "17:24:07 Iteration: 200 running loss: 7.920338\n",
      "17:24:07 Iteration: 200, evaluating\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> news item big banks have been lining up at the federal reserve's discount window to borrow money\n",
      "= lined up for fed handout\n",
      "< the s s s s s the the the the <EOS>\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-a0075005e853>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mcrit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdaptiveLogSoftmaxWithLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFC_DIM\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mVOCAB_SIZE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCUTOFFS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_optimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder_optimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mN_EPOCHS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mGRAD_CLIP\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-12-ad4da909966c>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(pairs, encoder, decoder, encoder_optimizer, decoder_optimizer, n_epochs, batch_size, clip)\u001b[0m\n\u001b[1;32m     13\u001b[0m                 \u001b[0minput_seqs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_lengths\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_seqs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_lengths\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m                 \u001b[0mencoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m                 \u001b[0mencoder_optimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder_optimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclip\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m             )\n\u001b[1;32m     17\u001b[0m             \u001b[0;31m# Keep track of loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-11-35ab06112fe2>\u001b[0m in \u001b[0;36mtrain_batch\u001b[0;34m(input_batches, input_lengths, target_batches, target_lengths, batch_size, encoder, decoder, encoder_optimizer, decoder_optimizer, clip)\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0mdecoder_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtarget_batches\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m# Next input is current target\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0;31m# Loss calculation and backpropagation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmasked_adasoft\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_decoder_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_batches\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_lengths\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcrit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-ad83d28e9e10>\u001b[0m in \u001b[0;36mmasked_adasoft\u001b[0;34m(logits, target, lengths, adasoft)\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mtargets_i\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex_select\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0masm_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0madasoft\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits_i\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets_i\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0masm_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtargets_i\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    475\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 477\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    478\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/adaptive.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m    184\u001b[0m                 \u001b[0mcluster_logprob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlog_softmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcluster_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    185\u001b[0m                 \u001b[0mlocal_logprob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcluster_logprob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgather\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrelative_target\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 186\u001b[0;31m                 \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex_copy_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrow_indices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocal_logprob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    187\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m             \u001b[0mused_rows\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mrow_indices\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Init models\n",
    "encoder = EncoderRNN(VOCAB_SIZE, HIDDEN_SIZE, EMBEDDING_DIM, pretrained_embeddings, N_LAYERS, dropout=DROPOUT_PROB).to(device)\n",
    "decoder = DecoderRNN(2*HIDDEN_SIZE, VOCAB_SIZE, EMBEDDING_DIM, pretrained_embeddings, N_LAYERS, dropout=DROPOUT_PROB).to(device)\n",
    "\n",
    "# Init optimizers\n",
    "encoder_optimizer = torch.optim.Adam(encoder.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\n",
    "decoder_optimizer = torch.optim.Adam(decoder.parameters(), lr=LR*DECODER_LEARNING_RATIO, weight_decay=WEIGHT_DECAY)\n",
    "\n",
    "# Load from checkpoint if has one\n",
    "load_checkpoint(encoder, decoder, encoder_optimizer, decoder_optimizer, CHECKPOINT_FNAME)\n",
    "\n",
    "# Init adasoft \n",
    "crit = nn.AdaptiveLogSoftmaxWithLoss(FC_DIM, VOCAB_SIZE, CUTOFFS).to(device)\n",
    "\n",
    "train(train_data, encoder, decoder, encoder_optimizer, decoder_optimizer, N_EPOCHS, BATCH_SIZE, GRAD_CLIP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Evaluate with Rouge metric on dev data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_rouge(dev_data, encoder, decoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Evaluate with Rouge metric on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_rouge(test_data, encoder, decoder)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
