{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "# from skimage import io, transform\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import os\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import re\n",
    "import unicodedata # ??\n",
    "import nltk\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "import csv\n",
    "import json\n",
    "from torchvision import transforms\n",
    "from torch.autograd import Variable\n",
    "np.random.seed(1)\n",
    "random.seed(1)\n",
    "\n",
    "\n",
    "# define directory structure needed for data processing\n",
    "RAW_DATA_DIR = os.path.join('..', 'data/', 'raw_data/')\n",
    "FORMAL_DATA_DIR = os.path.join('..', 'data/', 'formal_data/')\n",
    "UNKNOWN_TOKEN = \"unk\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split data into `train`, `val` and `test`\n",
    "Split and write raw data as `acsii` format. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pandas.io.formats.csvs'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-71beede92b36>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;31m# # TODO: check whether \"text\" is in fact the summary and corresponds to the headline\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m \u001b[0msplitData\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"news_summary.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-18-71beede92b36>\u001b[0m in \u001b[0;36msplitData\u001b[0;34m(fname, test_size, val_size)\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0mdf_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtest_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0mdf_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m     \u001b[0mdf_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mRAW_DATA_DIR\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"train.csv\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"\\t\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m     \u001b[0mdf_val\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mRAW_DATA_DIR\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"val.csv\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"\\t\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0mdf_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mRAW_DATA_DIR\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"test.csv\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"\\t\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36mto_csv\u001b[0;34m(self, path_or_buf, sep, na_rep, float_format, columns, header, index, index_label, mode, encoding, compression, quoting, quotechar, line_terminator, chunksize, tupleize_cols, date_format, doublequote, escapechar, decimal)\u001b[0m\n\u001b[1;32m   1730\u001b[0m                                            \u001b[0mmax_cols\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_cols\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1731\u001b[0m                                            \u001b[0mshow_dimensions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshow_dimensions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1732\u001b[0;31m                                            decimal=decimal)\n\u001b[0m\u001b[1;32m   1733\u001b[0m         \u001b[0;31m# TODO: a generic formatter wld b in DataFrameFormatter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1734\u001b[0m         \u001b[0mformatter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_html\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclasses\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclasses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnotebook\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnotebook\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mborder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mborder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pandas.io.formats.csvs'"
     ]
    }
   ],
   "source": [
    "# Turn a Unicode string to plain ASCII, thanks to\n",
    "# http://stackoverflow.com/a/518232/2809427\n",
    "def unicodeToAscii(s):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "    )\n",
    "def normalizeString(s):\n",
    "    s = unicodeToAscii(s.strip()).replace(\"\\t\", \" \")\n",
    "    return s\n",
    "\n",
    "\n",
    "# precondition: two fields with name: \"headline\" and \"text\"\n",
    "def splitData(fname, test_size=0.2, val_size=0.2): \n",
    "    df = pd.read_csv('../data/' + fname, encoding='latin-1')\n",
    "    df = df[[\"headlines\", \"text\"]] # summary text, not the entire article\n",
    "    df[\"headlines\"] = df[\"headlines\"].apply(normalizeString)\n",
    "    df[\"text\"] = df[\"text\"].apply(normalizeString)\n",
    "\n",
    "    df = df.sample(frac=1).reset_index(drop=True) # shuffle data\n",
    "\n",
    "    df_train, df_test = train_test_split(df, test_size=test_size, random_state=1)\n",
    "    df_train, df_val = train_test_split(df_train, test_size=val_size, random_state=1)\n",
    "    df_train.to_csv(RAW_DATA_DIR + \"train.csv\", index=False, sep=\"\\t\", header=False)\n",
    "    df_val.to_csv(RAW_DATA_DIR + \"val.csv\", index=False, sep=\"\\t\", header=False)\n",
    "    df_test.to_csv(RAW_DATA_DIR + \"test.csv\", index=False, sep=\"\\t\", header=False)\n",
    "    \n",
    "# # TODO: check whether \"text\" is in fact the summary and corresponds to the headline\n",
    "\n",
    "splitData(\"news_summary.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess `raw_data` to `formal_data`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class to store string transformation\n",
    "class Transform(object):\n",
    "    tokenizer = TweetTokenizer(preserve_case=False, strip_handles=True, reduce_len=True)\n",
    "    @staticmethod\n",
    "    def word_tokenize(s): \n",
    "        return json.dumps(Transform.tokenizer.tokenize(s))\n",
    "    \n",
    "    @staticmethod\n",
    "    def cap(s): \n",
    "        return s.upper()\n",
    "    \n",
    "# tf: a transformation apply to each individual headline and text\n",
    "# Apply a transformation to the dataframe in \"raw_data\" \n",
    "# Results of the transformation is in \"formal_data\"\n",
    "def preprocess(fname, tf, chunksize=1000, num_chunk=-1): \n",
    "    header = True\n",
    "    mode = \"w\"\n",
    "    i = 0\n",
    "    for df in pd.read_csv(RAW_DATA_DIR + fname, sep=\"\\t\", header = 0, chunksize=chunksize):\n",
    "        df[\"headlines\"] = df[\"headlines\"].apply(tf)\n",
    "        df[\"text\"] = df[\"text\"].apply(tf)\n",
    "        df.to_csv(FORMAL_DATA_DIR+fname, columns = ['headlines','text'], mode=mode,index=False, header=header, sep=\"\\t\")\n",
    "        if header == True:  \n",
    "            # no header, and write in append mode from the 2nd chunk\n",
    "            header = False; \n",
    "            mode = \"a\"\n",
    "        \n",
    "        i = i + 1 if i >= 0 else -1\n",
    "        if i == num_chunk: \n",
    "            break; \n",
    "    \n",
    "\n",
    "# preprocess(\"train.csv\", tf=Transform.word_tokenize)\n",
    "# preprocess(\"val.csv\", tf=Transform.word_tokenize)\n",
    "# preprocess(\"test.csv\", tf=Transform.word_tokenize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Language model from train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GloVe():\n",
    "    def __init__(self, path, dim):\n",
    "        self.dim = dim\n",
    "        self.word_embedding_dict = {}\n",
    "        with open(path) as f:\n",
    "            for line in f:\n",
    "                values = line.split()\n",
    "                embedding = values[-dim:]\n",
    "                word = ''.join(values[:-dim])\n",
    "                self.word_embedding_dict[word] = np.asarray(embedding, dtype=np.float32)\n",
    "    \n",
    "    def get_word_vector(self, word):\n",
    "        if word not in self.word_embedding_dict.keys():\n",
    "            embedding = np.random.uniform(low=-1, high=1, size=self.dim).astype(np.float32)\n",
    "            self.word_embedding_dict[word] = embedding\n",
    "            return embedding\n",
    "        else:\n",
    "            return self.word_embedding_dict[word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "glvmodel = GloVe(os.path.join('..', 'models', 'glove.twitter.27B.200d.txt'), dim=EMBEDDING_DIM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Modified from: Sean Robertson <https://github.com/spro/practical-pytorch>\n",
    "class Lang: # language model\n",
    "    def __init__(self, glvmodel, fname=None):\n",
    "        self.word2index = {}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {}\n",
    "        self.gloveEmbed = []\n",
    "        self.glvmodel = glvmodel\n",
    "        self.size = 0  \n",
    "        self.embed_dim = EMBEDDING_DIM\n",
    "        \n",
    "        self.addWord(\"<PAD>\")\n",
    "        self.addWord(\"<SOS>\")\n",
    "        self.addWord(\"<EOS>\")\n",
    "        \n",
    "        self.addWord(UNKNOWN_TOKEN)\n",
    "        if fname is not None: \n",
    "            self.addCSV(fname)\n",
    "        \n",
    "    # add words of a sentence into the language model\n",
    "    # split by ' '\n",
    "    def addSentence(self, sentence):\n",
    "        for word in sentence: \n",
    "            self.addWord(word)\n",
    "            \n",
    "    def addGlove(self, glove): \n",
    "        self.glove = glove\n",
    "\n",
    "    def getGloveLayer(self): \n",
    "        # Initialize word embeddings from our pre-training GloVe embeddings\n",
    "        glove_embeddings = torch.from_numpy(np.asarray(self.gloveEmbed))\n",
    "        return nn.Embedding(self.size, self.embed_dim).from_pretrained(glove_embeddings, freeze=False)\n",
    "        \n",
    "    # add a word to language model\n",
    "    def addWord(self, word):\n",
    "        if word not in self.word2index:\n",
    "            # assign id for word\n",
    "            self.word2index[word] = self.size\n",
    "            \n",
    "            # count word\n",
    "            self.word2count[word] = 1\n",
    "            \n",
    "            self.index2word[self.size] = word\n",
    "            self.size += 1\n",
    "            \n",
    "            self.gloveEmbed.append(glvmodel.get_word_vector(word))\n",
    "        else:\n",
    "            self.word2count[word] += 1\n",
    "            \n",
    "    def addDataFrame(self, df): \n",
    "        # df: headline, text\n",
    "        for index, row in df.iterrows():\n",
    "            self.addSentence(json.loads(row['headlines']))\n",
    "            self.addSentence(json.loads(row['text']))\n",
    "            \n",
    "    def addCSV(self, fname): \n",
    "        # construct language model based on a file\n",
    "        # file is a dataframe csv file with \"headlines\" and \"text\"\n",
    "        for df in pd.read_csv(FORMAL_DATA_DIR + fname, sep=\"\\t\", header = 0, chunksize=2000):\n",
    "            self.addDataFrame(df)     \n",
    "    \n",
    "    def size(self): \n",
    "        return len(self.word2index)\n",
    "    \n",
    "    def wordSeq2IdxSeq(self, word_seqs):\n",
    "        default_idx = lang.word2index[UNKNOWN_TOKEN]\n",
    "        idxs = []\n",
    "        for word_seq in word_seqs:\n",
    "            idxs.append([lang.word2index.get(w, default_idx) for w in word_seq])\n",
    "#             idxs.append(torch.LongTensor([lang.word2index.get(w, default_idx) for w in word_seq]))\n",
    "        return idxs\n",
    "\n",
    "lang = Lang([])\n",
    "lang.addCSV(\"train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_param(params, bias_std=0.0, weight_std=0.05): \n",
    "    for name, param in params:\n",
    "        if 'bias' in name:\n",
    "            nn.init.constant_(param, bias_std)\n",
    "        elif 'weight' in name:\n",
    "            nn.init.normal_(param, std = weight_std)\n",
    "            \n",
    "# class EncoderRNN(nn.Module):\n",
    "#     def __init__(self, input_size, hidden_size, n_layers=1, dropout=0.1):\n",
    "#         super(EncoderRNN, self).__init__()\n",
    "        \n",
    "#         self.input_size = input_size\n",
    "#         self.hidden_size = hidden_size\n",
    "#         self.n_layers = n_layers\n",
    "#         self.dropout = dropout\n",
    "        \n",
    "#         self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "#         self.gru = nn.GRU(hidden_size, hidden_size, n_layers, dropout=self.dropout, bidirectional=True)\n",
    "        \n",
    "#     def forward(self, input_seqs, input_lengths, hidden=None):\n",
    "#         # Note: we run this all at once (over multiple batches of multiple sequences)\n",
    "#         embedded = self.embedding(input_seqs)\n",
    "#         packed = torch.nn.utils.rnn.pack_padded_sequence(embedded, input_lengths)\n",
    "#         outputs, hidden = self.gru(packed, hidden)\n",
    "#         outputs, output_lengths = torch.nn.utils.rnn.pad_packed_sequence(outputs) # unpack (back to padded)\n",
    "#         outputs = outputs[:, :, :self.hidden_size] + outputs[:, : ,self.hidden_size:] # Sum bidirectional outputs\n",
    "#         return outputs, hidden\n",
    "    \n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, hidden_dim, batch_size, num_layers, lang):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.batch_size = batch_size\n",
    "        self.num_layers = num_layers\n",
    "        self.word_embeddings = lang.getGloveLayer()\n",
    "        \n",
    "        # Initialize a Gated Recurrent Unit RNN\n",
    "        self.gru = nn.GRU(input_size=lang.embed_dim, hidden_size=hidden_dim, num_layers=num_layers)\n",
    "        \n",
    "#         self.hidden2label = nn.Linear(hidden_dim, n_classes)\n",
    "        self.hidden = self.init_hidden()\n",
    "        \n",
    "        # Custom initialization of the weights and biases\n",
    "        init_param(self.gru.named_parameters())\n",
    "#         init_param(self.hidden2label.named_parameters())\n",
    "\n",
    "\n",
    "    def init_hidden(self):\n",
    "        return Variable(torch.zeros(self.num_layers, self.batch_size, self.hidden_dim).cuda())\n",
    "\n",
    "    \n",
    "    def forward(self, input_seqs):\n",
    "        \n",
    "        embedded = self.word_embeddings(input_seqs)\n",
    "        packed_embeds = nn.utils.rnn.PackedSequence(input_embed, packed_sequence.batch_sizes)\n",
    "        output, hidden = self.gru(packed_embeds, self.hidden)\n",
    "        return output, hidden\n",
    "    \n",
    "#     def forward(self, input_seqs, input_lengths, hidden=None):\n",
    "#         # Note: we run this all at once (over multiple batches of multiple sequences)\n",
    "#         embedded = self.word_embeddings(input_seqs)\n",
    "#         packed = torch.nn.utils.rnn.pack_padded_sequence(embedded, input_lengths)\n",
    "#         outputs, hidden = self.gru(packed, hidden)\n",
    "#         outputs, output_lengths = torch.nn.utils.rnn.pad_packed_sequence(outputs) # unpack (back to padded)\n",
    "#         outputs = outputs[:, :, :self.hidden_size] + outputs[:, : ,self.hidden_size:] # Sum bidirectional outputs\n",
    "#         return outputs, hidden\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
    "        \n",
    "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
    "        \n",
    "        # map to output space\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1) # ? what is LogSoftMax, dim = 1??\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        # ? why the input is is 'output_size' dimension? \n",
    "        # ? why need embedding? \n",
    "        output = self.embedding(input).view(1, 1, -1)\n",
    "        output = F.relu(output)\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "        \n",
    "        output = self.softmax(self.out(output[0]))\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)\n",
    "    \n",
    "HIDDEN_DIM = 128\n",
    "N_EPOCHS = 5\n",
    "BATCH_SIZE = 4\n",
    "NUM_LAYERS = 1\n",
    "encoder = Encoder(HIDDEN_DIM, BATCH_SIZE, NUM_LAYERS, lang)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Data\n",
    "### Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SummaryDataset(Dataset):\n",
    "    def __init__(self, fname, transform=None):\n",
    "        self.df = pd.read_csv(FORMAL_DATA_DIR + fname, sep=\"\\t\", header = 0)\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        headline = self.df[\"headlines\"][idx]\n",
    "        text = self.df[\"text\"][idx]\n",
    "        \n",
    "        sample = {'headlines': headline, 'text': text}\n",
    "\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "\n",
    "        return sample\n",
    "\n",
    "dataset = SummaryDataset(fname=\"train.csv\", transform=None)\n",
    "dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pad a with the PAD symbol\n",
    "def pad_seq(seq, max_length):\n",
    "    seq += [PAD_token for i in range(max_length - len(seq))]\n",
    "    return seq\n",
    "\n",
    "\"\"\"\n",
    "Create a batch ready for feeding into the network. \n",
    "Paddings are added to ensure all sequences are the same length. \n",
    "Precondition: \n",
    "    input_seqs: a list of sequences; each element is a sequencce which is a list of words\n",
    "    target_seqs: same. \n",
    "    lang: the language model\n",
    "Postcondition: \n",
    "    indices version of the input and target in tensor form. \n",
    "\"\"\"\n",
    "def batch(input_seqs, target_seqs, lang):\n",
    "    # input_seqs and target_seqs are in string format\n",
    "    \n",
    "    input_seqs = lang.wordSeq2IdxSeq(input_seqs)\n",
    "    target_seqs = lang.wordSeq2IdxSeq(target_seqs)\n",
    "    \n",
    "    # Zip into pairs, sort by length (descending), unzip\n",
    "    seq_pairs = sorted(zip(input_seqs, target_seqs), key=lambda p: len(p[0]), reverse=True)\n",
    "    input_seqs, target_seqs = zip(*seq_pairs)\n",
    "    \n",
    "    # For input and target sequences, get array of lengths and pad with 0s to max length\n",
    "    input_lengths = [len(s) for s in input_seqs]\n",
    "    input_padded = [pad_seq(s, max(input_lengths)) for s in input_seqs]\n",
    "    target_lengths = [len(s) for s in target_seqs]\n",
    "    target_padded = [pad_seq(s, max(target_lengths)) for s in target_seqs]\n",
    "    \n",
    "    # Turn padded arrays into (batch_size x max_len) tensors, transpose into (max_len x batch_size)\n",
    "    input_var = Variable(torch.LongTensor(input_padded)).transpose(0, 1)\n",
    "    target_var = Variable(torch.LongTensor(target_padded)).transpose(0, 1)\n",
    "    \n",
    "#     input_var = input_var.cuda()\n",
    "#     target_var = target_var.cuda()\n",
    "    \n",
    "    return input_seqs, input_lengths, target_seqs, target_lengths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'PAD_token' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-0afd59d1fe8b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mtarget_seqs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtarget_seqs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0minput_seqs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_seqs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_seqs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_seqs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlang\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0minput_seqs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpack_sequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_seqs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# .cuda()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-9-9014ddf58712>\u001b[0m in \u001b[0;36mbatch\u001b[0;34m(input_seqs, target_seqs, lang)\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0;31m# For input and target sequences, get array of lengths and pad with 0s to max length\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0minput_lengths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minput_seqs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m     \u001b[0minput_padded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mpad_seq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_lengths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minput_seqs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m     \u001b[0mtarget_lengths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtarget_seqs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0mtarget_padded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mpad_seq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget_lengths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtarget_seqs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-9-9014ddf58712>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0;31m# For input and target sequences, get array of lengths and pad with 0s to max length\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0minput_lengths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minput_seqs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m     \u001b[0minput_padded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mpad_seq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_lengths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minput_seqs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m     \u001b[0mtarget_lengths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtarget_seqs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0mtarget_padded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mpad_seq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget_lengths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtarget_seqs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-9-9014ddf58712>\u001b[0m in \u001b[0;36mpad_seq\u001b[0;34m(seq, max_length)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Pad a with the PAD symbol\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mpad_seq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mseq\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mPAD_token\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_length\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mseq\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-9-9014ddf58712>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Pad a with the PAD symbol\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mpad_seq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mseq\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mPAD_token\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_length\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mseq\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'PAD_token' is not defined"
     ]
    }
   ],
   "source": [
    "for i_batch, sample_batched in enumerate(dataloader):\n",
    "    # clear gradients, clear hidden state from the last timestep\n",
    "#     gru_model.zero_grad()\n",
    "#     gru_model.hidden = gru_model.init_hidden()\n",
    "\n",
    "    input_seqs = sample_batched[\"text\"]\n",
    "    target_seqs = sample_batched[\"headlines\"]\n",
    "    \n",
    "    input_seqs = [json.loads(s) for s in input_seqs]\n",
    "    target_seqs = [json.loads(s) for s in target_seqs]\n",
    "    \n",
    "    input_seqs, target_seqs = batch(input_seqs, target_seqs, lang)\n",
    "    \n",
    "    input_seqs = nn.utils.rnn.pack_sequence(input_seqs) # .cuda()\n",
    "    target_seqs = nn.utils.rnn.pack_sequence(target_seqs) #.cuda(); \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "sb = next(iter(dataloader))\n",
    "input_seqs = sb[\"text\"]\n",
    "target_seqs = sb[\"headlines\"]\n",
    "input_seqs = [json.loads(s) for s in input_seqs]\n",
    "target_seqs = [json.loads(s) for s in target_seqs]\n",
    "input_seqs, input_lengths, target_seqs, target_lengths = batch(input_seqs, target_seqs, lang)\n",
    "\n",
    "\n",
    "for i_batch, sample_batched in enumerate(dataloader):\n",
    "    # clear gradients, clear hidden state from the last timestep\n",
    "#     gru_model.zero_grad()\n",
    "#     gru_model.hidden = gru_model.init_hidden()\n",
    "\n",
    "    input_seqs = sample_batched[\"text\"]\n",
    "    target_seqs = sample_batched[\"headlines\"]\n",
    "    \n",
    "    input_seqs = [json.loads(s) for s in input_seqs]\n",
    "    target_seqs = [json.loads(s) for s in target_seqs]\n",
    "    \n",
    "    input_seqs, input_lengths, target_seqs, target_lengths = batch(input_seqs, target_seqs, lang)\n",
    "    \n",
    "    input_seqs = nn.utils.rnn.pack_sequence(input_seqs) # .cuda()\n",
    "    target_seqs = nn.utils.rnn.pack_sequence(target_seqs) #.cuda()\n",
    "\n",
    "#     # forward pass\n",
    "#     label_scores = gru_model(tweet_in)\n",
    "\n",
    "#     # compute loss against true labels\n",
    "#     loss = loss_function(label_scores, target)\n",
    "\n",
    "#     # backprop the gradients and update the model parameters\n",
    "#     loss.backward()\n",
    "#     optimizer.step()\n",
    "\n",
    "#     # keep track of the loss\n",
    "#     running_loss += loss.item()\n",
    "#     i += BATCH_SIZE\n",
    "#     if i % 2000 == 0:\n",
    "#         average_loss = running_loss/2000\n",
    "#         if average_loss < lowest_loss:\n",
    "#             lowest_loss = running_loss\n",
    "#             # save our checkpoint if it is the current best\n",
    "#             torch.save(gru_model.state_dict(), CHECKPOINT_FILE)\n",
    "#         logging.info(\"running loss: %.3f @ batch %d\", average_loss, batch_ind)\n",
    "#         running_loss = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(target_seqs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf = transforms.Compose([ToTensor(lang)])\n",
    "tf = None\n",
    "dataset = SummaryDataset(fname=\"train.csv\", transform=tf)\n",
    "dataloader = DataLoader(dataset, batch_size=4, shuffle=True, num_workers=4)\n",
    "\n",
    "\n",
    "sb = next(iter(dataloader))\n",
    "input_seqs = sb[\"text\"]\n",
    "target_seqs = sb[\"headlines\"]\n",
    "input_seqs = [json.loads(s) for s in input_seqs]\n",
    "target_seqs = [json.loads(s) for s in target_seqs]\n",
    "input_seqs, input_lengths, target_seqs, target_lengths = batch(input_seqs, target_seqs, lang)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train(input_batches, input_lengths, target_batches, target_lengths, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, max_length=MAX_LENGTH):\n",
    "    \n",
    "    # Zero gradients of both optimizers\n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "    loss = 0 # Added onto for each word\n",
    "\n",
    "    # Run words through encoder\n",
    "    encoder_outputs, encoder_hidden = encoder(input_batches, input_lengths, None)\n",
    "    \n",
    "    # Prepare input and output variables\n",
    "    decoder_input = Variable(torch.LongTensor([SOS_token] * batch_size))\n",
    "    decoder_hidden = encoder_hidden[:decoder.n_layers] # Use last (forward) hidden state from encoder\n",
    "\n",
    "    max_target_length = max(target_lengths)\n",
    "    all_decoder_outputs = Variable(torch.zeros(max_target_length, batch_size, decoder.output_size))\n",
    "\n",
    "    # Move new Variables to CUDA\n",
    "    if USE_CUDA:\n",
    "        decoder_input = decoder_input.cuda()\n",
    "        all_decoder_outputs = all_decoder_outputs.cuda()\n",
    "\n",
    "    # Run through decoder one time step at a time\n",
    "    for t in range(max_target_length):\n",
    "        decoder_output, decoder_hidden, decoder_attn = decoder(\n",
    "            decoder_input, decoder_hidden, encoder_outputs\n",
    "        )\n",
    "\n",
    "        all_decoder_outputs[t] = decoder_output\n",
    "        decoder_input = target_batches[t] # Next input is current target\n",
    "\n",
    "    # Loss calculation and backpropagation\n",
    "    loss = masked_cross_entropy(\n",
    "        all_decoder_outputs.transpose(0, 1).contiguous(), # -> batch x seq\n",
    "        target_batches.transpose(0, 1).contiguous(), # -> batch x seq\n",
    "        target_lengths\n",
    "    )\n",
    "    loss.backward()\n",
    "    \n",
    "    # Clip gradient norms\n",
    "    ec = torch.nn.utils.clip_grad_norm(encoder.parameters(), clip)\n",
    "    dc = torch.nn.utils.clip_grad_norm(decoder.parameters(), clip)\n",
    "\n",
    "    # Update parameters with optimizers\n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "    \n",
    "    return loss.data[0], ec, dc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DF = []\n",
    "for df in pd.read_csv(RAW_DATA_DIR + 'train.csv', sep=\"\\t\", header = 0, chunksize=10):\n",
    "    DF  = df\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
