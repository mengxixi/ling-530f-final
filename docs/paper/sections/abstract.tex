\begin{abstract}

As the amount of textual data grows exponentially in the last few decades, effective information retrieval becomes central in high-level decision making. In this work, we provide an abstractive text summarization model under the attentional encoder-decoder framework. We employ one of the most cutting-edge language model -- ELMo, along with the adaptive softmax technique for GPU memory efficiency. We evaluate our model performance both quantitatively and qualitatively on the Annotated English Gigaword dataset. The qualitative results demonstrate our model's adequacy in the headline generation task. A quantitative comparison of the effectiveness using different word embeddings under this framework is also provided. 

\end{abstract}
