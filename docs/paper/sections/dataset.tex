\section{Dataset}
\label{sec: dataset}

For training and evaluation, we use the Annotated English Gigaword dataset \cite{graff2003english}, which contains nearly 10 million documents from 7 news sources. Due to computational constraints, instead of using the full document as the input source, we extract only the first paragraph of each document as the input sequence, and the corresponding news headline is treated as our target summary. This formulation of the text summarization task is commonly adopted in the literature \cite{rush2015neural}. We refer the input sequence and its target summary the \textit{input-output pair}. For comparison purposes, we use the same data split \footnote{https://github.com/facebookarchive/NAMAS} as in \cite{rush2015neural} to form the training, validation and test set (roughly 90-5-5). We tokenize the sequences, remove most punctuations, and convert all tokens to lowercase in the input-output pairs. Numeric tokens were left unchanged since they often contain important information for the model to generalize from. Due to limited computational resources, the training set is downsampled by randomly dropping $40\%$ of the input-output pairs to reduce the vocabulary size as well as the number of embeddings that we need to store. During data cleaning, we remove pairs where the input text has less than $5$ tokens, otherwise there are barely anything to summarize from. We also remove pairs with headline length greater than $30$ to enforce summary conciseness. Furthermore, we replace low-frequency tokens (less than $4$ occurences) with the ``UNK'' token. To prevent learning from uninformative examples, we remove pairs whose headlines have less than $3$ known tokens. 

Finally, we arrive at about $3.8$ million training pairs with a vocabulary size of about $214,000$. The average sentence length is $9$ tokens for the headlines and $30$ tokens for the body text. The development set contains about $346,000$ pairs which we use for hyperparameter tuning. Following the same strategy as \cite{rush2015neural}, the test set was generated by randomly sampling $2,000$ examples from the test split. The sentence length distribution and the word frequency distribution of the test set are similar to that of the training set.
