\section{Dataset}
\label{sec: dataset}

For training and evaluation, we used the Annotated English Gigaword dataset \cite{graff2003english}, which contains nearly 10 million documents from 7 news sources. Due to computational constraints, instead of using the full document as the input source, we extracted the first paragraph of each document as training input, as well as the news headline as our target summary. In fact, many works in text summarization formulate the headline generation problem in the same way \cite{rush2015neural}. For comparison purpose, we used the same data split \footnote{https://github.com/facebookarchive/NAMAS} in \cite{rush2015neural} to form the training, validation and test set (in a roughly 90-5-5 split). We tokenized the sequences, removed most punctuations, and lower-cased all tokens in the input-output pairs. Numeric tokens were unchanged since they might contain important information for the headline. Additionally, the training set was downsampled by randomly dropping $40\%$ of the data pairs to reduce the training set vocabulary size (along with the corresponding embeddings) because of limited computational resources. During data cleansing, we removed examples with the input text length less than $5$ tokens (since news often have more tokens) and removed examples with headline length greater than $30$ tokens to enforce summary conciseness. Furthermore, we replaced low-frequency tokens (lower than $4$ occurences) with the ``UNK'' token. To prevent learning from uninformative examples, we removed examples whose headlines have less than $3$ known tokens. 

As a result, we obtained about $3.8$ million training examples with a vocabulary size of about $214,000$. The average sentence length is $9$ tokens for the headlines and $30$ tokens for the body text. The development set contained about $346,000$ examples which we used for hyperparameter tuning. Following the same strategy as \cite{rush2015neural}, the test set was generated by randomly sampling $2,000$ examples from the test split. The sentence length distribution and the word frequency distribution of the test set are similar to that of the training set.
