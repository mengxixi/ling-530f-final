\section{Experiments}
\label{sec: exp}

Since the process of gathering the embeddings involving running each sentence in our corpus through \texttt{ELMo}, it is very time-consuming (took about 5 hours). Therefore, we perform this only once and save it to disk and for every subsequent experiments we simply load the persisted embeddings. We have also tried using the \texttt{ELMo} embeddings directly without averaging, i.e. for every minibatch, we generate the \texttt{ELMo} embeddings on the fly as the input to our text summarization model. However, the results were unsatisfactory as the decoder also relies on the previously generated word to generate the next word. If we only have \texttt{ELMo} return an embedding based on a single word, there will be no context and therefore the embedding would not be very meaningful. Even if we run the entire sequence currently genereated through \texttt{ELMo}, at the beginning of training this sequence would likely be non-sensical, rendering \texttt{ELMo} ineffective and therefore prohibits learning of the decoder as the error accumulates throughout training.

Our model is trained with a 2-layer Gated Recurrent Unit (GRU) on both the encoder and decoder with $200$ hidden units for every hidden layer. The adaptive softmax still requires a fully connected layer at the decoder output layer but does not retrict it to be the same size of the vocabulary, we manually tuned it to be $512$ for best performance. To avoid the problem of exploding gradients, we monitored the gradients' norm (L2-norm) during training, then clipped the gradients to have a maximum norm of $50.0$. Since our model is quite complicated, to avoid overfitting, we employ regularization techniques such as a weight decay of $0.0001$ and dropout with probability $0.5$ during training. We used the Adam optimizer with an initial learning rate of $0.001$, which we lower by a factor of $0.1$ for $4$ times during training according to a schedule, which is determined based on when the loss starts to plateau (\todo{insert convergence figure}). In addition, we set the decoder learning rate to be $5$ times higher than that of the encoder for the summarization result to have a larger impact on the decoder parameters. For the output space partitions, we used cutoffs at $1,000$ and $20,000$ based on the frequency distribution of our vocabulary. \todo{modify the parameters based on code}

The text summarization model is trained on an NVIDIA GTX 1060 GPU in shuffled batches of size $32$. Since our training set is quite large, we only ran our model for $2$ epochs and the loss would converge by the end, as shown in \todo{reference the same convergence figure}. Excluding any preprocessing time, training one model takes about $10$ hours.

We also compared the same model architecture and training procedure while using $300$-dimension \texttt{Glove} embeddings and $300$-dimension \texttt{FastText} embeddings, the results are summarized in Table \ref{tab: compare_embed}.



