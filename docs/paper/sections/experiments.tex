\section{Experiments}
\label{sec: exp}

The process of gathering \texttt{ELMo} embeddings is exceptionally time-consuming (about 5 hours) because it involves running each sentence in our corpus through \texttt{ELMo}. Therefore, we cached the gathered embeddings onto disk and loaded them before every subsequent experiment to amortize the cost. 

There are some problems to overcome in integrating \texttt{ELMo} into our encoder-decoder framework. The natural way to use \texttt{ELMo} embeddings is to generate them on the fly as the program spawns the text summarization model. However, this approach is computationally costly because the decoder's generating the next word depends on the previously generated word. Another method would be to create each embedding based on a single word (instead of a context), which defeats the purpose of using context-dependent word representation. Another option is to run the currently generated sequence through \texttt{ELMo}. However, this approach makes the beginning part of the sequence non-sensical and errors of the part accumulate through time. Thus, it prohibits the learning of the decoder.\todo{confirm I rephrased it correctly}
As a result, we compromised to average the \texttt{ELMo} embeddings for each word \todo{a bit more detail}.

The model was trained with a 2-layer Gated Recurrent Unit (GRU) on both the encoder and decoder with $200$ hidden units for every hidden layer. The adaptive softmax was used to replace softmax for computational efficiency. However, adaptive softmax requires a fully connected layer following the decoder's output layer. \todo{confirm: what is 512 dimension??} The adaptive softmax's output dimension was manually tuned to be $512$ using our development set. To avoid exploding gradient, we performed gradient clipping by monitoring the gradient's L2-norm during training. The gradient was capped to have a maximum magnitude of $50.0$.

Since our model has a large number of weights, to avoid overfitting, regularization techniques --- a weight decay of $0.0001$ and a dropout of probability $0.5$ --- were applied for training. The Adam optimizer \cite{kingma2014adam} was used to jointly train \todo{does it call jointly?} our encoder and decoder with an initial learning rate of $0.001$. The learning rate was adjusted by multiplying it with a factor of $0.1$ whenever the loss reaches a plateau (see Fig. \ref{fig: plateu}) \todo{insert convergence figure}. As a result, the learning rate was adjusted $4$ times during training. Since the decoder's performance is conditioned on the encoder's quality, we set the decoder's learning rate to be $5$ times of the encoder's to make the decoder's learning more flexible. For the output space partitions, we used cutoffs at $1,000$ and $20,000$ based on the frequency distribution of our vocabulary. \todo{modify the parameters based on code} \todo{What does this sentence mean?}

The text summarization model was trained on an NVIDIA GTX 1060 GPU using shuffled 32-sized batches. Since our training set is quite large, we only ran our model for $2$ epochs, but the loss converged at the end (see Fig. \ref{fig: convergence}) \todo{reference the same convergence figure}. It is important to mention that due to computational limitations, disregarding the preprocessing time, training a model with a set of hyperparameters to experiment with consumes more than $10$ hours, which was the major bottleneck of our study. 

We have also compared the use of two other different embeddings ($300$-dimension \texttt{GloVe} embeddings, and $300$-dimension \texttt{FastText} embeddings) against \texttt{ELMo} using the same architecture and training procedure. The results are shown in Table \ref{tab: compare_embed}.




