\section{Introduction}

As the amount of textual data in the world grows in an ever-increasing rate, the ability to quickly index and search from these documents becomes crucial. To provide a comprehensive summary that captures all the relevant pieces usually requires human experts to perform the task manually, which is prohibitively costly as the amount of work scales. Furthermore, given relatively neutral corpora to learn from, machine-generated summaries are less prone to human-bias (either intended or not). Such need for automatically condensing text motivates our work in abstractive text summarization --- a task of generating a concise summary that retains the key ideas from a document with high coherence. 

In this work, we explore a closely related task --- news headline generation --- as a reduced form of abstractive text summarization. We employ deep learning architectures such as sequence-to-sequence recurrent neural network models with attention as the basic framework. The ELMo contextual word representation \cite{peters2018deep} is used as our word embedding layer. To boost performance, we adopted the Adaptive Softmax technique \cite{grave2016efficient} for large vocabulary. To justify the choice of ELMo embedding, a quantitative comparison is provided with several alternative word embeddings. 


This report is organized as follows. Section \ref{sec: related_work} provides introduction of related research to our task. Section \ref{sec: dataset} describes the Annotated English Gigaword corpus \cite{graff2003english} that we use to train and evaluate our model. Section \ref{sec: method} provides a detailed description of our ELMo embedded global attentional model. Furthermore, section \ref{sec: exp} and \ref{sec: result} provide the evaluation setup and various aspects of analysis on our model (including examples of headline generation). Lastly, section \ref{sec: conclusion} concludes our work and contributions. 

