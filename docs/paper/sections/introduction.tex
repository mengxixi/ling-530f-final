\section{Introduction}

As the amount of textual data in the world grows in an ever-increasing rate, the ability to quickly index and search from documents becomes crucial. To provide a readable and comprehensive summary capturing all the relevant pieces often requires human experts to perform the task manually, which is prohibitively costly as the amount of work scales. Furthermore, given relatively neutral corpora to learn from, machine-generated summaries are less prone to human-bias (either intentional or not). Such need for automatically condensing text motivates our work in abstractive text summarization --- a task of generating a concise and readable summary that retains the key ideas of a document with high coherence. 

In this work, we explored a closely related task --- news headline generation --- as a reduced form of abstractive text summarization. We used deep learning architectures such as sequence-to-sequence recurrent neural network models with attention as the basic framework. The ELMo contextual word representation \cite{peters2018deep} was used as our word embedding layer. To boost performance, we adopted the adaptive softmax technique \cite{grave2016efficient} for large vocabulary. To justify the choice of ELMo embedding, a quantitative comparison between several alternative word embeddings is provided. 

This report is organized as follows. Section \ref{sec: related_work} provides an introduction of related research to our task. Section \ref{sec: dataset} describes the Annotated English Gigaword corpus \cite{graff2003english} that we used to train and evaluate our model. Section \ref{sec: method} provides a detailed description of our ELMo embedded global attentional model. Furthermore, section \ref{sec: exp} and \ref{sec: result} provide the evaluation setup and various aspects of analysis on our model (including examples of outputs from our headline generation model). Lastly, section \ref{sec: conclusion} concludes our work and contributions. 

