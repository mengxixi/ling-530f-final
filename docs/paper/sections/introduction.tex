\section{Introduction}

As the amount of textual data in the world grows in an ever-increasing rate, the ability to quickly index and search from documents becomes crucial. To provide a readable and comprehensive summary capturing all the relevant pieces often requires the knowledge of human experts, which is prohibitively costly as the amount of work scales. Furthermore, given a relatively neutral corpus to learn from, machine-generated summaries should be less prone to human-bias (either intentional or not). These issues motivate the use of modern technology to train computers to accomplish this task. 

In this work, we explore a closely related task --- news headline generation --- as a reduced form of abstractive text summarization. We use deep learning architectures such as sequence-to-sequence recurrent neural network models with attention as the basic framework. The \texttt{ELMo} contextual word representation \cite{peters2018deep} was used as the source of our input word embeddings. To reduce GPU memory usage with a large vocabulary, we adopt the adaptive softmax technique \cite{grave2016efficient} instead of the regular softmax. 

This report is organized as follows. Section \ref{sec: related_work} provides a discussion of the related research to our task. Section \ref{sec: dataset} describes the Annotated English Gigaword dataset \cite{graff2003english} that we use to train and evaluate our model. Section \ref{sec: method} provides a detailed description of our \texttt{ELMo} embedded global attentional model. Section \ref{sec: exp} and \ref{sec: result} provide the evaluation setup and various aspects of the performance analysis. Lastly, section \ref{sec: conclusion} concludes our work and contributions. 

