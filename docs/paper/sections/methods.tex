\section{Methods}
\label{sec: method}

\todo{Subsections for methods?}
\todo{Model architecture goes first}


For model input, we used a pretrained ELMo language model \cite{peters2018deep} to generate 256-dimension embeddings for our vocabulary. As discussed earlier, ELMo's main advantage is its ability to generate contextualized word embeddings. In other words, it is able to generate a different embedding for the same word that appears in different contexts. This is crucial as it allows our text summarization model to disambiguate between multiple meanings of a given word, but the implication is that we can no longer use a fixed dictionary as a pretrained embeddings input. To address this issue, we ran every sentence in our training set through the pretrained ELMo model, and save the average of all the embeddings we receive for a given word. One may argue that by taking the average -- thereby having the same embedding for a given word regardless of its context -- defeats the purpose of employing the contextualized representation. However, we believe that the average essentially incorporates all the meanings of a given word into the same embedding, and therefore should do no harm in the downstream task. \todo{show comparison between using ELMo vs Glove vs fastText} In table [x] \todo{table?} we compare the results of the same architecture using Glove embeddings vs ELMo average embeddings. 

One of the main challenges we had was battling with the size of the dataset. Even after downsampling to retain only $60\%$ of the training data and replacing low frequency words, our vocabulary size was still very high and it would not fit onto the GPU memory. The bottleneck is when the decoder predicts the next word, it relies on the last fully connected layer to map from hidden space to the output space in order to perform softmax, which is the same size as our vocabulary. The number of model parameters then immediately scales with the size of the vocabulary due to the fully connected layer. To circumvent this issue, we use adaptive softmax which is an efficient approximation to the regular softmax function \cite{grave2016efficient}. This strategy is most effective when the output space is large and highly unbalanced which accurately describes our vocabulary. Adaptive softmax works by partitioning the output space into clusters depending on the label frequencies, and thus approximating the regular softmax in a hierarchical fashion. Since the word frequencies roughly follows the Zipf's law \cite{wilson1949human} where a large proportion of the articles is covered by only a small number of words in our vocabulary, therefore it would make sense to prioritize the prediction of higher frequency words than the others, thus reducing memory requirement.


