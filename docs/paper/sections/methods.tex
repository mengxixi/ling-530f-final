\section{Methods}
\label{sec: method}


For model input, we used a pretrained ELMo language model \cite{peters2018deep} to generate 256-dimension embeddings for our vocabulary. As discussed earlier, ELMo's main advantage is its ability to generate contextualized word embeddings. In other words, it is able to generate a different embedding for the same word that appears in different contexts. This is crucial as it allows our text summarization model to disambiguate between multiple meanings of a given word, but the implication is that we can no longer use a fixed dictionary as a pretrained embeddings input. To address this issue, we ran every sentence in our training set through the pretrained ELMo model, and save the average of all the embeddings we receive for a given word. Since this is a time-consuming process, we only perform this once and save the generated embedding to disk. One may argue that by taking the average -- thereby having the same embedding for a given word regardless of its context -- defeats the purpose of employing the contextualized representation. However, we believe that the average essentially incorporates all the meanings of a given word into the same embedding, and therefore should do no harm in the downstream task. \todo{show comparison between using ELMo vs Glove} In table [x] \todo{table?} we compare the results of the same architecture using Glove embeddings vs ELMo average embeddings. 
\begin{enumerate}
\item model architecture
\item adaptive softmax 
\end{enumerate} 