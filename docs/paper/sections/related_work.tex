\section{Related Work}
\label{sec: related_work}

\subsection{Automatic Text Summarization}

The task of automatic text summarization has been extensively studied in hte past \cite{gambhir2017recent}. Two general approaches are either extractive or abstractive methods \cite{gupta2010survey}. Extractive methods explicitly select parts of the original document (verbatim words, phrases or even complete sentences) to combine into a summary. On the other hand, abstractive methods focus on understanding the original document and rephrasing the main ideas in a shorter form, often with words and phrases that do not occur in the original text. Early attempts in text summarization often focused on extractive methods utilizing lexical occurence probabilities to extract parts of the input document \cite{mathis1973improvement}. Extractive methods have traditionally been more straightforward to interpret and implement; however, as deep neural networks shed light on various natural language processing (NLP) problems, abstractive methods become progressively feasible \cite{nallapati2016sequence, nallapati2016abstractive, rush2015neural}. For instance, \cite{rush2015neural} proposed a local attention-based model by incorporating a neural language model, a contextual input encoder and a beam-search decoder to explore such tasks, while \cite{nallapati2016abstractive} used an attentional encoder-decoder framework with additional rich linguistic features such as part-of-speech tags and named-entity recognition tags. Both works were evaluated quantitatively in a news headline generation setting on the Gigaword dataset \cite{graff2003english} with the ROUGE metrics \cite{lin2004rouge}, which will be discussed in more detail in \ref{sec: result}. 

\subsection{Recurrent Neural Networks}
Neural networks models have been widely adopted to solve various NLP problems. In particular, recurrent neural networks (RNNs) has gained popularity due to its ability to handle arbitrary-length sequences using a hidden state vector that autoregressively encodes sequential information \cite{goodfellow2016}. Since the vanilla RNN performs poorly with long sequences due to vanishing gradient or exploding gradient issues, the Long Short-Term Memory (LSTM) units \cite{hochreiter1997long} and Gated Recurrent Units (GRU) \cite{cho2014properties} have been proposed. Both architectures have shown their success in many tasks such as supervised sequence labelling \cite{graves2012supervised}, image captioning \cite{vinyals_2015} and speech recognition \cite{graves2013speech} with comparable performance \cite{yin2017comparative}.

\subsection{Word Embedding}

Text summarization systems (and other NLP systems) often rely on a competent underlying language model to accurately represent text input in a vectorized form. In general, there are two kinds of word representations: context-independent and context-dependent. The context-independent representations learn a \textit{single} meaning for each word by either predicting a word based on its surrounding or by performing a matrix decomposition on the coocurrence matrix. Canonical examples of context-independent word representations are \texttt{word2vec} \cite{mikolov2013efficient}, \texttt{FastText} \cite{bojanowski2017enriching}, and \texttt{GloVe} \cite{pennington2014glove}. However, due to the polysemic nature of words, these representations are not flexible enough to capture word meanings in different contexts. To address this problem, novel insights in context-dependent representations start to emerge. In particular, Embeddings from Language Models (\texttt{ELMo}) was introduced in \cite{peters2018deep} by combinging all layers of a pre-trained deep bidirectional language model. Instead of finalizing word embeddings into a fixed dictionary, \texttt{ELMo} is able to generate a unique representation of a word when appeared in different sentences. Bidirectional Encoder Representations from Transformers (\texttt{BERT}) is another recent work introduced by \cite{devlin2018bert} where a deep bidirectional Transformer architecture was proposed, which has shown compelling results on many NLP benchmark tasks.  

\subsection{Machine Translation}
Abstractive text summarization is closely related to machine translation, where variations of the encoder-decoder architecture (also known as the sequence-to-sequence model) is commonly adopted. The encoder -- often an RNN such as LSTM -- iterates forward in time over the input sequences while updating its hidden state to encode sequential information into a fixed-length vector. The decoder -- another RNN -- then generates the output sequence by maximizing the probability of the next word found in the vocabulary space given the encoded vector \cite{sutskever2014sequence,cho2014learning}. This approach often works well for short sequences, but performance can deteriorate for longer input sequences, which is often the case in machine translation and text summarization. To alleviate the problem, \cite{bahdanau2014neural} introduced the attention mechanism, which assigns weights over the entire input sequence to dictate the location and level of attention required for the decoder to focus on at a particular time step. However, computational bottleneck also arises in the attention weight computation for long sequences. A local attention approach is proposed by \cite{luong2015effective} to speed up the process by only generating attention over a window of the input sequence based on the current input time step. Finally, \cite{vaswani2017attention} proposed a highly parallelizable and efficient Transformer network to replace any convolutional and recurrent component with only the attention mechanism, and it has demonstrated significant improvement over the computational requirement while maintaining the state-of-the-art performance. Many of these works were originally introduced to develop methods for machine translation tasks, however, one can think of text summarization being analagous to machine translation in a sense that we are trying to ``translate'' the original text into a concise form. Thus, it is reasonable to apply similar techniques in text summarization settings, which we will explore in this work.

