\section{Related Work}
\label{sec: related_work}
\todo{rephrase this subsection a bit}
\subsection{Recurrent Neural Networks}
In recent years, neural networks have been widely applied in various NLP problems. In particular, recurrent neural networks (RNNs) has gained popularity due to its ability to handle sequential data of arbitrary length, which coincides with the properties of textual data. By sharing model parameters across different timesteps over the input sequence (i.e., a sentence), with enough data, RNNs are able to learn the relevant information encoded in the sequence regardless of how the sequence is presented. The key component of an RNN is a hidden state variable that accumulates the information extracted throughout history in a recursive fashion. The hidden state is then carried over to the current timestep via a non-linear activation function \cite{goodfellow2016}.

Unfortunately, the vanilla RNN does not perform very well with long sequences. When backpropagating through time to update the network parameters, the gradient will likely vanish by the end of the sequence, rendering gradient descent ineffective towards learning \cite{goodfellow2016}. Several variations of RNNs have been proposed to address this issue. Among those, the most popular models are Long Short-Term Memory (LSTM) units \cite{hochreiter1997long} and Gated Recurrent Units (GRU) \cite{cho2014properties}. Both models have been applied in various problems such as supervised sequence labelling \cite{graves2012supervised}, image captioning \cite{vinyals_2015} and speech recognition \cite{graves2013speech} with comparable performance \cite{yin2017comparative}.

\todo{rephrase this subsection a bit}
\subsection{Word Embedding}
Compact representations of words have then been developed with two main approaches: the context-predicting models and the context-counting models \cite{baroni2014don}. The idea behind the predict models is that a word's context defines its meaning. One popular model is \texttt{word2vec} \cite{mikolov2013efficient}, which constructs a neural network predicting a word in a sequence given its surrounding words. The word vectors, or embeddings, are then taken from the weights of a layer in the network. \texttt{FastText} \cite{bojanowski2017enriching} can be seen as an extension of \texttt{word2vec} by representing each word from the character-level n-grams. In comparison, the count models rely on the dimensionality reduction of the word co-occurrence matrix, where each word vector in the lower dimensional co-occurrence matrix preserves some meaning in the higher dimensional space. \texttt{GloVe} \cite{pennington2014glove} is one of the most popular count model. 

However, the above word representations do not deal with polysemy (some words might have multiple meanings). To address this problem, the ELMo (Embeddings from Language Models) was introduced by \cite{peters2018deep} to represent word meanings depending on its context of use. ELMo was created by combinging all layers of a pre-trained deep bidirectional language model. Like GloVe, the representation is character-based, which handles out-of-vocabulary words. 
\todo{mention BERT}

\subsection{Machine Translation}
In Machine Translation, the encoder-decoder framework, which is often composed of two RNNs (such as LSTM), is commonly adopted. The encoder iterates forward in time over the input sequence while updating its hidden state to summarize the input sequence in a fixed vector. The decoder then creates the output sequence by sampling in the vocabulary space based on the summary vector \cite{sutskever2014sequence,cho2014learning}. This approach often works well for short sequences but starts to deteriorate for longer sequences. To alleviate the problem, \cite{bahdanau2014neural} introduces an attention mechanism, which assigns weights over the entire input sequence to represent how much attention is required for decoding at a particular time step. However, computational bottleneck appears in attention weight computation for long sequences. A local attention approach is proposed by \cite{luong2015effective} to speed up the process by only generating attention over a window of the input sequence based on the current input. Finally, \cite{vaswani2017attention} proposed a highly parallelizable and efficient Transformer network to replace CNN and RNN using only attention mechanism.

\subsection{Automatic Text Summarization}

As textual data becomes more and more excess on the Internet, systems that summarize longer documents would be beneficial to a significant amount of users. Thus, there have been extensive researches on models that compress a long document into a shorter summary \cite{gambhir2017recent}. There are two general approaches to automatic text summarization: extractive and abstractive \cite{gupta2010survey}. The extractive methods often select parts of the original document (verbatim words, phrases or even sentences) to generate a summary. On the other hand, the abstractive methods focus on understanding the original document and rephrasing the main ideas in a shorter form (especially with words and phrases that do not occur in the original text). Historically, lots of researches have been done in extractive methods as the approach is more feasible. However, as neural networks shed light on various NLP research communities, abstractive methods start to become more popular \cite{nallapati2016sequence, nallapati2016abstractive, rush2015neural}. For example, \cite{rush2015neural} proposed a local attention-based model by incorporating a neural language model, a contextual input encoder and a beam-search decoder. Furthermore, \cite{nallapati2016abstractive} used an attentional encoder-decoder framework with additional rich linguistic features such as part of speech tags and named entity recognition tags. Both works were evaluated quantitatively in a news headline generation task on the Gigaword dataset \cite{graff2003english} with the ROUGE summarization evaluation package \cite{lin2004rouge}. 