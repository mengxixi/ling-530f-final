\section{Related Work}
\label{sec: related_work}

\subsection{Recurrent Neural Networks}
Neural networks have been widely applied in various NLP problems. In particular, recurrent neural networks (RNNs) has gained popularity due to its ability to handle arbitrary-length sequential data using a hidden state vector that summarizes sequential data autoregressively \cite{goodfellow2016}. Since the vanilla RNN performs poorly with long sequences due to vanishing gradient and exploding gradient issues, the Long Short-Term Memory (LSTM) units \cite{hochreiter1997long} and Gated Recurrent Units (GRU) \cite{cho2014properties} have been proposed. Both models have been successfully applied in various problems such as supervised sequence labelling \cite{graves2012supervised}, image captioning \cite{vinyals_2015} and speech recognition \cite{graves2013speech} with comparable performance \cite{yin2017comparative}.

\subsection{Word Embedding}

Throughout the past five years, many compact word representations have been developed. In general, there are two kinds of word representations: context-independent and context-dependent. The context-independent representations learn the \textit{single} meaning of each word by either predicting a word based on its surrounding or by performing a matrix decomposition on the coocurrence matrix. Examples of context-independent word representations are \texttt{word2vec} \cite{mikolov2013efficient}, \texttt{FastText} \cite{bojanowski2017enriching}, and \texttt{GloVe} \cite{pennington2014glove}. However, due to the polysemy nature of words (words have different meanings in different context), these representations are not flexible and resourceful enough to capture word meanings in different contexts. To address this problem, the context-dependent representations start to emerge. In particular, \texttt{ELMo} (Embeddings from Language Models) was introduced in \cite{peters2018deep} by combinging all layers of a pre-trained deep bidirectional language model; \texttt{BERT} (Bidirectional Encoder Representations from Transformers) was introduced by \cite{devlin2018bert} using deep bidirectional Transformer architecture. 

\subsection{Machine Translation}
In Machine Translation, the encoder-decoder framework, which is often composed of two RNNs (such as LSTM), is commonly adopted. The encoder iterates forward in time over the input sequence while updating its hidden state to summarize the input sequence in a fixed vector. The decoder then creates the output sequence by sampling in the vocabulary space based on the summary vector \cite{sutskever2014sequence,cho2014learning}. This approach often works well for short sequences but starts to deteriorate for longer sequences. To alleviate the problem, \cite{bahdanau2014neural} introduces an attention mechanism, which assigns weights over the entire input sequence to represent how much attention is required for decoding at a particular time step. However, computational bottleneck appears in attention weight computation for long sequences. A local attention approach is proposed by \cite{luong2015effective} to speed up the process by only generating attention over a window of the input sequence based on the current input. Finally, \cite{vaswani2017attention} proposed a highly parallelizable and efficient Transformer network to replace CNN and RNN using only attention mechanism.

\subsection{Automatic Text Summarization}

As textual data becomes more and more excess on the Internet, systems that summarize longer documents would be beneficial to a significant amount of users. Thus, there have been extensive researches on models that compress a long document into a shorter summary \cite{gambhir2017recent}. There are two general approaches to automatic text summarization: extractive and abstractive \cite{gupta2010survey}. The extractive methods often select parts of the original document (verbatim words, phrases or even sentences) to generate a summary. On the other hand, the abstractive methods focus on understanding the original document and rephrasing the main ideas in a shorter form (especially with words and phrases that do not occur in the original text). Historically, lots of researches have been done in extractive methods as the approach is more feasible. However, as neural networks shed light on various NLP research communities, abstractive methods start to become more popular \cite{nallapati2016sequence, nallapati2016abstractive, rush2015neural}. For example, \cite{rush2015neural} proposed a local attention-based model by incorporating a neural language model, a contextual input encoder and a beam-search decoder. Furthermore, \cite{nallapati2016abstractive} used an attentional encoder-decoder framework with additional rich linguistic features such as part of speech tags and named entity recognition tags. Both works were evaluated quantitatively in a news headline generation task on the Gigaword dataset \cite{graff2003english} with the ROUGE summarization evaluation package \cite{lin2004rouge}. 