\section{Related Work}
\label{sec: related_work}

\subsection{Automatic Text Summarization}

The task of automatic text summarization has been extensively studied \cite{gambhir2017recent}. The two general approaches are the extractive method and the abstractive method \cite{gupta2010survey}. Extractive methods explicitly select parts of the original document (verbatim words, phrases or even complete sentences) and combine them into a summary. On the other hand, abstractive methods focus on comprehending the original document and rephrasing the main ideas in a shorter form, often with words and phrases that do not occur in the original text. Early attempts in text summarization focused on extractive methods utilizing the lexical occurrence probabilities to extract parts of the input document \cite{mathis1973improvement}. Extractive methods are traditionally more straightforward to interpret and implement; however, as deep neural networks shed light on various natural language processing (NLP) problems, abstractive methods become progressively feasible \cite{nallapati2016sequence, nallapati2016abstractive, rush2015neural}. For instance, \cite{rush2015neural} proposed a local attention-based model by incorporating a neural language model, a contextual input encoder and a beam-search decoder to explore such tasks. Similarly, \cite{nallapati2016abstractive} used an attentional encoder-decoder framework with additional rich linguistic features such as part-of-speech tags and named-entity recognition tags. Both works were evaluated quantitatively in a news headline generation setting on the Annotated English Gigaword dataset \cite{graff2003english} with the ROUGE metrics \cite{lin2004rouge}, which will be discussed in more detail in section \ref{sec: result}. 

\subsection{Recurrent Neural Networks}
Neural network models have been widely adopted to solve various NLP problems. In particular, recurrent neural networks (RNNs) has gained popularity for its ability to handle arbitrary-length sequences using a hidden state vector that autoregressively encodes sequential information \cite{goodfellow2016}. Since the vanilla RNN performs poorly with long sequences due to vanishing gradient and exploding gradient problems, the Long Short-Term Memory (LSTM) units \cite{hochreiter1997long} and the Gated Recurrent Units (GRU) \cite{cho2014properties} have been proposed. Both architectures have shown their successes in various tasks including supervised sequence labelling \cite{graves2012supervised}, image captioning \cite{vinyals_2015} and speech recognition \cite{graves2013speech} with competitive performance \cite{yin2017comparative}.

\subsection{Word Embedding}

Text summarization systems (and other NLP systems) often rely on a competent underlying language model to accurately represent meanings in a vector form. Generally, there are two kinds of word representations: context-independent and context-dependent. The context-independent representations learn a \textit{single} meaning for each word by either predicting a word based on its surrounding or by performing a matrix decomposition on the co-occurrence matrix. Canonical examples of context-independent word representations are \texttt{word2vec} \cite{mikolov2013efficient}, \texttt{FastText} \cite{bojanowski2017enriching}, and \texttt{GloVe} \cite{pennington2014glove}. However, due to the polysemic nature of most natural languages, these representations are not flexible enough to capture word meanings in different contexts. To address this problem, novel insights in context-dependent representations start to emerge. Particularly, Embeddings from Language Models (\texttt{ELMo}) was introduced in \cite{peters2018deep} by combining all layers of a pre-trained deep bi-directional language model. The language model \texttt{ELMo} generates unique word representation when the word appears in different sentences instead of finalizing word embeddings into a fixed dictionary. Bidirectional Encoder Representations from Transformers (\texttt{BERT}) is another recent work introduced by \cite{devlin2018bert} where a deep bidirectional Transformer architecture was proposed, which has shown compelling results on many NLP benchmark tasks.  

\subsection{Machine Translation}
Abstractive text summarization is closely related to machine translation, where variations of the encoder-decoder architecture (also known as the sequence-to-sequence model) are commonly adopted. The encoder --- often an RNN such as LSTM --- iterates forward in time over the input sequences while updating its hidden states to encode sequential information into a fixed-length vector. The decoder --- another RNN --- generates the output sequence by maximizing the probability of the next word found in the vocabulary space given the encoded vector \cite{sutskever2014sequence,cho2014learning}. This approach often works well for short sequences, but the performance deteriorates for longer input sequences, which is often the case in text summarization. To alleviate the problem, \cite{bahdanau2014neural} introduced the attention mechanism, which assigns weights over the entire input sequence to guide the decoder's focus on parts of the input sequence at a particular time step. However, computational bottleneck arises in the attention weight calculation for long sequences. A local attention approach was proposed by \cite{luong2015effective} to speed up the calculation by only computing attention over a window on the input sequence chosen by the current decoder hidden state. Finally, \cite{vaswani2017attention} proposed a highly parallelizable and efficient Transformer model to replace convolutional and recurrent components with only attention mechanism, and it showed significant improvement over the computational requirement while maintaining the state-of-the-art performance. Many of these works were originally introduced in developing methods for machine translation tasks; however, one can think of text summarization being analogous to machine translation in a sense that we are trying to convert a signal to its condensed form. Thus, it is reasonable to apply similar techniques under the text summarization setting, which we will explore in this work.

