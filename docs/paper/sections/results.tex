\section{Results}
\label{sec: result}

We use the Recall-Oriented Understudy for Gisting Evaluation (ROUGE) metrics \cite{lin2004rouge} to evaluate our text summarization system. To be consistent with the literature, our models are evaluated with ROUGE-1 (overlap of 1-grams), ROUGE-2 (overlap of bigrams) and ROUGE-L (longest common subsequence) using full-length F1-scores. Although ROUGE is the most widely adopted metric in text summarization, it is not always effective when measuring the success of such systems. As noted by \cite{cohan2016revisiting}, ROUGE calculates the scores purely based on the lexical overlap between the documents. Therefore, it can be misleading when the system outputs a paraphrase of the ground-truth summary but using different words, or even just a different ordering of the words.


\begin{enumerate}
\item table for our model and others
\item what's our baseline? what's everyone's baseline?
\item briefly explain how ROUGE works
\item briefly talk about the interpretation for why ROUGE 2 is usually low for all published work
\item talk about what might be the reason for our ROUGE to be especially low (we did not replace numbers with pound, etc)
\item give positive and negative examples for qualitative assessment
\end{enumerate}