\section{Motivation}
Automatic text summarization is an important task in natural language processing (NLP). With the ever-growing amount of textual material emerging, the ability to efficiently and accurately generate summaries becomes crucial \cite{gambhir2017recent}. Summaries can allow us to quickly index documents and make selections with ease. It is also less prone to personal bias when performed by a machine. Therefore, in this project, we will explore deep learning methods and aim to provide a general approach to this problem.

\section{Problem Statement}
We will implement a recurrent neural network (RNN) based model which given a short article (e.g., short news stories), it will generate a summary (around 1 to 3 sentences) that is fluent and concise. The process is essentially a sequence-to-sequence learning problem. If time and computational resources allow, we will extend the problem to a larger setting such as generating abstracts from academic publications. 


\section{Methods}
We will be experimenting with recent advancements in sequence-to-sequence models, which are encoder-decoder RNNs. We will feed in the input article as a sequence of word embeddings from pre-trained language models such as \texttt{GloVe}, \texttt{fastText} or \texttt{Word2Vec} \cite{pennington2014glove, bojanowski2017enriching, mikolov2013efficient}. We will also explore options that may enhance our performance, such as attention mechanism, rich sentence features \cite{nallapati2016abstractive}, convolution neural networks (CNNs) and other techniques based on further research. 

To ensure productive progress of our project, we will take the following steps as a breakdown of the more significant task. First, we will start with a small dataset of news articles, and we will treat the title as the target summary that we try to generate. This way, we can ensure our overall seq2seq framework is working and able to produce sensible one-line summaries. Then we will add in enhancement techniques to further improve our model based on the ROUGE metric system \cite{lin2004rouge}. Finally, we will scale up the problem by using a more comprehensive dataset in which the summaries are more informative. 

\section{Dataset}
We will be using the Annotated English Gigaword Dataset (Gigaword) produced by the Linguistic Data Consortium \cite{napoles2012annotated}. It contains about 10 million documents from 7 news sources (over 4 billion words in total). We will use a subset of the 10 million articles, and for each article, we will use its headline as the summary and the body as the sequence-to-sequence training input.

\section{Evaluation}
For quantitative evaluation, we will use the Recall-Oriented Understudy for Gisting Evaluation (ROUGE) metrics that are often used for evaluating automatic summarization and machine translation systems. Specifically, we will use the ROUGE-1 (overlap of 1-gram), ROUGE-2 (overlap of bigrams) and ROUGE-L (longest common subsequence) measures to compare our model against the baseline model. To avoid reinventing the wheel, we will use the \texttt{pyrouge}\footnote{https://pypi.org/project/pyrouge} Python package for ROUGE evaluation scripts and configuration files. 
We will use the seq2seq RNN model introduced in \cite{nallapati2016abstractive} as our baseline model because it also uses the aforementioned Gigaword dataset evaluated on ROUGE metrics. We will assess our model qualitatively by randomly selecting a few summaries generated by our model and manually compare them with the corresponding human-written summaries. We will then examine the fluency, accuracy, and comprehensiveness of the machine-generated results.

\section{Tasks and Assignment}

The following is a breakdown of the project into subtasks:

\noindent
\textbf{Dataset}
\begin{compactitem}
\item Data cleaning (Fan)
\item Summary statistics for the dataset (Yin)
\end{compactitem}

\noindent
\textbf{Model}
\begin{compactitem}
\item Baseline neural network (Fan)
\item Format dataset in input form  (Meng)
\item Develop model against the baseline (Fan, Meng, Yin).
\item Word embedding on input sentence (Meng)
\end{compactitem}

\noindent
\textbf{Evaluation}
\begin{compactitem}
\item \texttt{pyrouge} setup for evaluation (Yin, Meng)
\item Evaluate model quantitatively (Yin)
\item Evaluate model qualitatively (Yin)
\end{compactitem}

\noindent
\textbf{Related work reading}
\begin{compactitem}
\item RNN neural network (Yin)
\item Text summarization in general (Fan)
\item Abstractive text summarization (Meng)
\item Word embedding (Meng)
\end{compactitem}

\noindent
\textbf{Report Writing}
\begin{compactitem}
\item Experimental result generation and formatting (Fan)
\item Abstract (Meng)
\item Introduction (Yin, Meng)
\item Related work (Fan)
\item Methodology (Fan)
\item Experiment (Yin, Meng)
\item Result (Meng)
\item Conclusion (Yin)
\end{compactitem}


\section{Timeline}

\noindent
\textbf{Week 1 (from Nov 5): Basic ML framework}
\begin{compactitem}
\item Data cleaning 
\item Baseline neural network 
\item Word embedding on input sentences.
\item Format dataset in input form 
\item \texttt{pyrouge} setup for evaluation.
\end{compactitem}

\noindent
\textbf{Week 2: Model development}
\begin{compactitem}
\item Summary statistics for the dataset
\item Develop model against the baseline
\end{compactitem}

\noindent
\textbf{Week 3: Evaluation and reading}
\begin{compactitem}
\item Evaluate model quantitatively.
\item Evaluate model qualitatively.
\item Related work reading.
\end{compactitem}

\noindent
\textbf{Week 4 (till the end): Writing}
\begin{compactitem}
\item Report writing
\end{compactitem}

%\newpage
\bibliography{acl2018}
\bibliographystyle{acl_natbib}
