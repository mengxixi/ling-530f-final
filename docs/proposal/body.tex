\section{Motivation}
Automatic text summarization is an important task in natural language processing. With the ever-growing amount of textual material emerging, the ability to efficiently and accurately generate summaries becomes crucial \cite{gambhir2017recent}. Summaries can allow us to quickly index documents and make selections in a much shorter period of time. It is also less prone to personal bias when performed by a machine. Therefore, in this project, we will explore deep learning methods to hopefully provide a general approach to this problem.

\section{Problem Statement}
We will implement a deep neural network model which given a short article (e.g., short news stories), it will generate a summary (around 1 to 3 sentences) that is fluent and concise. The process is essentially a sequence-to-sequence learning problem. If time and computational resource allows, we will extend the problem to a larger setting, for example, generating abstracts from academic publications. 


\section{Methods}
We will be experimenting with recent advancements in sequence-to-sequence models, which are encoder-decoder recurrent neural networks (RNNs). We will feed in the input article as a sequence of word embeddings from pre-trained language models (GloVe, fastText or Word2Vec) \todo{Add citations?}. We will also explore options that may enhance our performance, such as attention mechanism, rich sentence features \cite{nallapati2016abstractive}, convolution neural networks (CNNs) and other architecture based on further research. \todo{How does your project compare to NLP and deep learning published research?} 

To ensure a productive progress of our project, we will take the following steps as a breakdown of the larger task. First, we will start with a small dataset of news articles, and we will treat the title as the target summary that we try to generate. This way, we can ensure our overall seq2seq framework is working and able to generate somewhat sensible one-line summaries. Then we will add in enhancement techniques to further improve our model on the ROUGE metric system \cite{lin2004rouge}. Finally, we will scale up the problem by using a more comprehensive dataset in which the summaries are a bit more informative. 

\section{Dataset}
TODO

\section{Evaluation}
For quantitative evaluation, we will use the Recall-Oriented Understudy for Gisting Evaluation (ROUGE) metrics that are often used for evaluating automatic summarization and machine translation systems. Specifically, we wil use the ROUGE-1 (overlap of 1-gram), ROUGE-2 (overlap of bigrams) and ROUGE-L (longest common subsequence) measures to compare our model against the baseline model. To avoid reinventing the wheel, we will use the pyrouge \todo{cite?} Python package for ROUGE evaluation scripts and configuration files. \todo{which baseline are we comparing to?} Furthermore, we will evaluate our model qualitatively by randomly selecting a few summaries generated by our model and manually compare them with the corresponding human written summaries. We will then examine the fluency, accuracy and comprehensiveness of the machine-generated results.

\section{Tasks and Assignment}

\begin{compactitem}
\item Task 1 (Fan, Meng)
\item Task 2 (Yin, Meng)
\item Task 3 (Yin)
\item Task 4 (Fan)
\end{compactitem}


\section{Timeline}
\begin{compactitem}
\item Week 1: do task 1
\item Week 2: do task 2
\item Week 3: do task 3
\item Week 4: do task 3 and task 4
\end{compactitem}

\newpage
\bibliography{acl2018}
\bibliographystyle{acl_natbib}
